{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d6803a",
   "metadata": {},
   "source": [
    "# Extract daily flows from HYDAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f3bb785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "import python_cs_functions as cs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73793dff",
   "metadata": {},
   "source": [
    "### Config handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd4a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where the config file can be found\n",
    "config_file = '../0_config/config.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4855e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the required info from the config file\n",
    "data_path = cs.read_from_config(config_file,'data_path')\n",
    "\n",
    "# CAMELS-spat metadata\n",
    "cs_meta_path = cs.read_from_config(config_file,'cs_basin_path')\n",
    "cs_meta_name = cs.read_from_config(config_file,'cs_meta_name')\n",
    "cs_unusable_name = cs.read_from_config(config_file,'cs_unusable_name')\n",
    "\n",
    "# Basin folder\n",
    "cs_basin_folder = cs.read_from_config(config_file, 'cs_basin_path')\n",
    "basins_path = Path(data_path) / cs_basin_folder\n",
    "\n",
    "# Data period\n",
    "time_s = datetime.strptime( cs.read_from_config(config_file, 'hydat_start_t'), '%Y-%m-%d')\n",
    "time_e = datetime.strptime( cs.read_from_config(config_file, 'hydat_start_e'), '%Y-%m-%d')\n",
    "\n",
    "# Hydat folder\n",
    "ref_path = cs.read_from_config(config_file,'ref_shps_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f6f7d7",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6e1dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAMELS-spat metadata file\n",
    "cs_meta_path = Path(data_path) / cs_meta_path\n",
    "cs_meta = pd.read_csv(cs_meta_path / cs_meta_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df1aac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open list of unusable stations; Enforce reading IDs as string to keep leading 0's\n",
    "cs_unusable = pd.read_csv(cs_meta_path / cs_unusable_name, dtype={'Station_id': object}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51e3ac2",
   "metadata": {},
   "source": [
    "### Define data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa8ede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the download location\n",
    "hydat_path = Path(data_path) / ref_path / 'RHBN-CAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3830949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the HYDAT database. Assumes only 1 sqlite3 database exists\n",
    "hydat_name = sorted(hydat_path.glob('*.sqlite3'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65580ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open database\n",
    "db = cs.connect_to_sqlite_database(hydat_path/hydat_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7186b863",
   "metadata": {},
   "source": [
    "### Loop over sites and extract the flow record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2ea66ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the metadata file\n",
    "meta_column_start = 'dv_flow_obs_availability_start'\n",
    "meta_column_end   = 'dv_flow_obs_availability_end'\n",
    "meta_column_miss  = 'flow_obs_missing_daily'\n",
    "c_start = np.where(cs_meta.columns == meta_column_start)[0][0]\n",
    "c_end   = np.where(cs_meta.columns == meta_column_end)[0][0]\n",
    "c_miss  = np.where(cs_meta.columns == meta_column_miss)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deb8c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries\n",
    "sql_table = 'DLY_FLOWS'\n",
    "sql_field = 'STATION_NUMBER'\n",
    "sbs = [] # We'll store QC symbols in here and check them later\n",
    "dnf = [] # We'll store a check here to see if we have any data at all available here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4dc94d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found for site 05RE002\n",
      "No data found for site 06DA001\n",
      "No data found for site 07BJ006\n",
      "No data found for site 07QD002\n",
      "No data found for site 07SB017\n",
      "No data found for site 08AA007\n",
      "No data found for site 08KH011\n",
      "No data found for site 08LD003\n",
      "No data found for site 08LF023\n",
      "No data found for site 08MG020\n",
      "No data found for site 08MG022\n",
      "No data found for site 09AA004\n",
      "No data found for site 09AE002\n"
     ]
    }
   ],
   "source": [
    "# Loop over the Canada stations only\n",
    "dnf = [] # List of incomplete stations, retaining these for easier printout and checking later\n",
    "for ix,row in cs_meta.iterrows():\n",
    "    if row.Country == 'CAN':\n",
    "        \n",
    "        # Get paths, etc\n",
    "        site, _, _, raw_path_dv, _,_ = cs.prepare_flow_download_outputs(cs_meta, ix, basins_path, time='daily')\n",
    "        \n",
    "        # Resume after interupts\n",
    "        if os.path.isfile(raw_path_dv): # If raw file exists it must have been created already \n",
    "            continue\n",
    "                \n",
    "        # Construct the SQL query we need\n",
    "        query = cs.construct_query_from_dataframe_column(sql_table, sql_field, [site])\n",
    "        \n",
    "        # Get the daily flow data from the HYDAT database as a dataframe\n",
    "        station_table = cs.sql_query_to_dataframe(db, query)\n",
    "        \n",
    "        # Restructure the daily flows into a mangeable time series\n",
    "        df = cs.restructure_daily_flows_in_hydat_table(station_table, time_s, time_e, site)\n",
    "        \n",
    "        # Abort and track if we didn't find any data\n",
    "        if len(df) == 0:\n",
    "            dnf.append(site)\n",
    "            print(f'No data found for site {site}')\n",
    "            continue\n",
    "        \n",
    "        # Update meta data\n",
    "        cs_meta.iat[ix,c_start] = df.index[0].strftime('%Y-%m-%d %X')\n",
    "        cs_meta.iat[ix,c_end]   = df.index[-1].strftime('%Y-%m-%d %X')\n",
    "        cs_meta.iat[ix,c_miss]  = df['FLOW'].isna().sum()\n",
    "        \n",
    "        # Save to file\n",
    "        df.to_csv(raw_path_dv)\n",
    "        \n",
    "        # Track the symbols for (possible) later use\n",
    "        sbs.append(df['SYMBOL'].unique())\n",
    "        print(f'Finished site {site}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdab1bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the metadata file\n",
    "cs_meta.to_csv(cs_meta_path / cs_meta_name, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f00fee",
   "metadata": {},
   "source": [
    "## Check sites for which we could not obtain flow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f58cf43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No daily data extracted for gauge 05RE002\n",
      "No daily data extracted for gauge 06DA001\n",
      "No daily data extracted for gauge 07BJ006\n",
      "No daily data extracted for gauge 07QD002\n",
      "No daily data extracted for gauge 07SB017\n",
      "No daily data extracted for gauge 08AA007\n",
      "No daily data extracted for gauge 08KH011\n",
      "No daily data extracted for gauge 08LD003\n",
      "No daily data extracted for gauge 08LF023\n",
      "No daily data extracted for gauge 08MG020\n",
      "No daily data extracted for gauge 08MG022\n",
      "No daily data extracted for gauge 09AA004\n",
      "No daily data extracted for gauge 09AE002\n",
      "End of list\n"
     ]
    }
   ],
   "source": [
    "# Print which basins we need to check\n",
    "for entry in dnf:\n",
    "    print(f'No daily data extracted for gauge {entry}')\n",
    "print('End of list')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a33d96a",
   "metadata": {},
   "source": [
    "Manual checks using WSC website indicate that for these stations only water levels, but not flows, are available: https://wateroffice.ec.gc.ca/search/historical_e.html\n",
    "- 05RE002: `WEAVER LAKE AT OUTLET` No flows, but level data available for 1967-2021\n",
    "- 06DA001: `WOLLASTON LAKE AT ROSS CHANNEL` No flows, but level data available for 1971-2022\n",
    "- 07BJ006: `LESSER SLAVE LAKE AT SLAVE LAKE` No flows, but level data available for 1979-2021\n",
    "- 07QD002: `NONACHO LAKE NEAR LUTSELK'E (SNOWDRIFT)` No flows, but level data available for 1962-2022\n",
    "- 07SB017: `PRELUDE LAKE NEAR YELLOWKNIFE` No flows, but level data available for 1995-2022\n",
    "- 08AA007: `SEKULMUN LAKE NEAR WHITEHORSE` No flows, but level data available for 1980-2021\n",
    "- 08KH011: `QUESNEL LAKE NEAR LIKELY` No flows, but level data available for 1956-2022\n",
    "- 08LD003: `ADAMS LAKE NEAR SQUILAX` No flows, but level data available for 1949-2021\n",
    "- 08LF023: `THOMPSON RIVER AT KAMLOOPS` Flow data for 1911-1913 only, level data available for 1911-2022\n",
    "- 08MG020: `LILLOOET LAKE NEAR PEMBERTON` No flows, but level data available for 1971-2021\n",
    "- 08MG022: `HARRISON RIVER BELOW MORRIS CREEK` No flows, but level data available for 1973-2021\n",
    "- 09AA004: `BENNETT LAKE AT CARCROSS` No flows, but level data available for 1947-2022\n",
    "- 09AE002: `TESLIN LAKE AT TESLIN` No flows, but level data available for 1944-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ab557b",
   "metadata": {},
   "source": [
    "### Update the metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05857738",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'CAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e447f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = 'dv'\n",
    "reason = 'No discharge values available (only water levels)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f010aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_unusable = pd.concat([cs_unusable, pd.DataFrame({'Country': country,\n",
    "                                                    'Station_id': dnf,\n",
    "                                                    'Missing': missing,\n",
    "                                                    'Reason': reason})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a8540bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_unusable = cs_unusable.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "682ef6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_unusable.to_csv(cs_meta_path / cs_unusable_name, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4444c488",
   "metadata": {},
   "source": [
    "## Check which streamflow codes we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fb24bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print which data codes we have in the data\n",
    "sbs_flat = []\n",
    "for arr in sbs:\n",
    "    for item in arr:\n",
    "        if str(item) != 'nan':\n",
    "            sbs_flat.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "61b6e203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ', 'A', 'B', 'E', None}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(sbs_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f7ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For posterity\n",
    "sbs_flat = [' ', 'A', 'B', 'E', None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2d70f85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A', 'Partial Day', 'Journée incomplète')\n",
      "('B', 'Ice Conditions', 'Conditions à glace')\n",
      "('D', 'Dry', 'Sec')\n",
      "('E', 'Estimated', 'Estimé')\n",
      "('S', 'Sample(s) collected this day', 'échantillons prélevés ce jour-là')\n"
     ]
    }
   ],
   "source": [
    "cs.find_table_contents(db,'DATA_SYMBOLS',to_screen=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c9b66",
   "metadata": {},
   "source": [
    "### Database investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "258f57d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('STATIONS',)\n",
      "('CONCENTRATION_SYMBOLS',)\n",
      "('SED_SAMPLES_PSD',)\n",
      "('ANNUAL_INSTANT_PEAKS',)\n",
      "('STN_DATUM_UNRELATED',)\n",
      "('DATA_SYMBOLS',)\n",
      "('SED_VERTICAL_LOCATION',)\n",
      "('STN_DATA_COLLECTION',)\n",
      "('PEAK_CODES',)\n",
      "('SED_DATA_TYPES',)\n",
      "('MEASUREMENT_CODES',)\n",
      "('SED_VERTICAL_SYMBOLS',)\n",
      "('DATA_TYPES',)\n",
      "('DLY_FLOWS',)\n",
      "('STN_REMARKS',)\n",
      "('STN_DATUM_CONVERSION',)\n",
      "('AGENCY_LIST',)\n",
      "('SED_DLY_SUSCON',)\n",
      "('STN_OPERATION_SCHEDULE',)\n",
      "('STN_DATA_RANGE',)\n",
      "('PRECISION_CODES',)\n",
      "('SED_DLY_LOADS',)\n",
      "('DLY_LEVELS',)\n",
      "('OPERATION_CODES',)\n",
      "('STN_REGULATION',)\n",
      "('DATUM_LIST',)\n",
      "('ANNUAL_STATISTICS',)\n",
      "('VERSION',)\n",
      "('REGIONAL_OFFICE_LIST',)\n",
      "('SAMPLE_REMARK_CODES',)\n",
      "('STN_REMARK_CODES',)\n",
      "('SED_SAMPLES',)\n",
      "('STN_STATUS_CODES',)\n"
     ]
    }
   ],
   "source": [
    "# list the available tables\n",
    "tables = cs.find_all_table_names(db) # -> we want 'STATIONS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bfbc1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STATION_NUMBER', 'YEAR', 'MONTH', 'FULL_MONTH', 'NO_DAYS', 'MONTHLY_MEAN', 'MONTHLY_TOTAL', 'FIRST_DAY_MIN', 'MIN', 'FIRST_DAY_MAX', 'MAX', 'FLOW1', 'FLOW_SYMBOL1', 'FLOW2', 'FLOW_SYMBOL2', 'FLOW3', 'FLOW_SYMBOL3', 'FLOW4', 'FLOW_SYMBOL4', 'FLOW5', 'FLOW_SYMBOL5', 'FLOW6', 'FLOW_SYMBOL6', 'FLOW7', 'FLOW_SYMBOL7', 'FLOW8', 'FLOW_SYMBOL8', 'FLOW9', 'FLOW_SYMBOL9', 'FLOW10', 'FLOW_SYMBOL10', 'FLOW11', 'FLOW_SYMBOL11', 'FLOW12', 'FLOW_SYMBOL12', 'FLOW13', 'FLOW_SYMBOL13', 'FLOW14', 'FLOW_SYMBOL14', 'FLOW15', 'FLOW_SYMBOL15', 'FLOW16', 'FLOW_SYMBOL16', 'FLOW17', 'FLOW_SYMBOL17', 'FLOW18', 'FLOW_SYMBOL18', 'FLOW19', 'FLOW_SYMBOL19', 'FLOW20', 'FLOW_SYMBOL20', 'FLOW21', 'FLOW_SYMBOL21', 'FLOW22', 'FLOW_SYMBOL22', 'FLOW23', 'FLOW_SYMBOL23', 'FLOW24', 'FLOW_SYMBOL24', 'FLOW25', 'FLOW_SYMBOL25', 'FLOW26', 'FLOW_SYMBOL26', 'FLOW27', 'FLOW_SYMBOL27', 'FLOW28', 'FLOW_SYMBOL28', 'FLOW29', 'FLOW_SYMBOL29', 'FLOW30', 'FLOW_SYMBOL30', 'FLOW31', 'FLOW_SYMBOL31']\n"
     ]
    }
   ],
   "source": [
    "# Find the headers in the DLY_FLOWS table\n",
    "table_name = 'DLY_FLOWS'\n",
    "headers = cs.find_table_headers(db,table_name) # -> we need to subset on 'STATION_NUMBER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79f24d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('01AD001',\n",
       " 1928,\n",
       " 10,\n",
       " 1,\n",
       " 31,\n",
       " 19.799999237060547,\n",
       " 613.2000122070312,\n",
       " 6,\n",
       " 15.300000190734863,\n",
       " 26,\n",
       " 29.700000762939453,\n",
       " 16.600000381469727,\n",
       " 'E',\n",
       " 16.600000381469727,\n",
       " 'E',\n",
       " 16.600000381469727,\n",
       " 'E',\n",
       " 16.600000381469727,\n",
       " 'E',\n",
       " 16.600000381469727,\n",
       " 'E',\n",
       " 15.300000190734863,\n",
       " 'E',\n",
       " 15.300000190734863,\n",
       " 'E',\n",
       " 15.300000190734863,\n",
       " 'E',\n",
       " 15.300000190734863,\n",
       " 'E',\n",
       " 15.300000190734863,\n",
       " 'E',\n",
       " 15.899999618530272,\n",
       " 'E',\n",
       " 15.899999618530272,\n",
       " 'E',\n",
       " 15.899999618530272,\n",
       " 'E',\n",
       " 15.899999618530272,\n",
       " 'E',\n",
       " 15.899999618530272,\n",
       " 'E',\n",
       " 15.300000190734863,\n",
       " 'E',\n",
       " 15.300000190734863,\n",
       " 'E',\n",
       " 15.300000190734863,\n",
       " 'E',\n",
       " 15.300000190734863,\n",
       " 'E',\n",
       " 15.300000190734863,\n",
       " 'E',\n",
       " 23.899999618530277,\n",
       " 'E',\n",
       " 23.899999618530277,\n",
       " 'E',\n",
       " 23.899999618530277,\n",
       " 'E',\n",
       " 23.899999618530277,\n",
       " 'E',\n",
       " 23.899999618530277,\n",
       " 'E',\n",
       " 29.700000762939453,\n",
       " 'E',\n",
       " 29.700000762939453,\n",
       " 'E',\n",
       " 29.700000762939453,\n",
       " 'E',\n",
       " 29.700000762939453,\n",
       " 'E',\n",
       " 29.700000762939453,\n",
       " 'E',\n",
       " 29.700000762939453,\n",
       " 'E')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the contents of a single row to find which format we need to specify STATION_NUMBER in\n",
    "contents = cs.find_table_contents(db,table_name,to_screen=False)\n",
    "contents[0] # -> we need to specify station IDs as '01AA002' INCLUDING the apostrophes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1446fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels-spat-env",
   "language": "python",
   "name": "camels-spat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
