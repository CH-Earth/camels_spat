{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db3e732-426b-4b22-b055-b9395f29fa09",
   "metadata": {},
   "source": [
    "## Calculate attributes\n",
    "Takes prepared geospatial data and computes various attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac7b17d-07d9-4985-bb9c-8508fb2c7554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "from python_cs_functions import config as cs, attributes as csa\n",
    "from python_cs_functions.delineate import prepare_delineation_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bcba8f-d99c-4058-9de9-23ae255c19e9",
   "metadata": {},
   "source": [
    "### Config handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dfdcc03-f734-4df6-8bee-25ae59f5560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where the config file can be found\n",
    "config_file = '../0_config/config.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa42aa7-cf4d-4c51-8555-05638e2ffc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the required info from the config file\n",
    "data_path            = cs.read_from_config(config_file,'data_path')\n",
    "\n",
    "# CAMELS-spat metadata\n",
    "cs_meta_path = cs.read_from_config(config_file,'cs_basin_path')\n",
    "cs_meta_name = cs.read_from_config(config_file,'cs_meta_name')\n",
    "cs_unusable_name = cs.read_from_config(config_file,'cs_unusable_name')\n",
    "\n",
    "# Basin folder\n",
    "cs_basin_folder = cs.read_from_config(config_file, 'cs_basin_path')\n",
    "basins_path = Path(data_path) / cs_basin_folder\n",
    "\n",
    "# Get the temporary data folder\n",
    "cs_temp_folder = cs.read_from_config(config_file, 'temp_path')\n",
    "temp_path = Path(cs_temp_folder)\n",
    "temp_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933dae9c-b5fe-42d2-9cc9-2ee54661e2c7",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "333bd63a-b0a1-417d-b9c6-d28306a0120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAMELS-spat metadata file\n",
    "cs_meta_path = Path(data_path) / cs_meta_path\n",
    "cs_meta = pd.read_csv(cs_meta_path / cs_meta_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a497a0-a9a2-404c-9b2a-8173cbb1436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open list of unusable stations; Enforce reading IDs as string to keep leading 0's\n",
    "cs_unusable = pd.read_csv(cs_meta_path / cs_unusable_name, dtype={'Station_id': object})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87113cc-bfb3-48dd-bae1-4c7f19fd39b3",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3185dc90-2a46-46b2-bdd1-c0032c2c452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_message = f'\\n!!! CHECK DEBUGGING STATUS: \\n- Testing 1 file \\n- Testing 1 basin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c395a8-7e5d-4140-b61d-65581fc05a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subfolders = ['era5', 'worldclim', 'hydrology', 'lai', 'forest_height', 'glclu2019', 'modis_land', 'lgrip30', 'merit', 'hydrolakes', 'pelletier', 'glhymps'] # \n",
    "#, 'soilgrids'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11e6b92c-35d3-4358-a3fe-0daec6cd96f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every attribute needs a list, so that we can efficiently construct a dataframe later\n",
    "l_gauges = [] # station ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b44331c-4cd6-4b2a-ba99-db3a767f9ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!!! CHECK DEBUGGING STATUS: \n",
      "- Testing 1 file \n",
      "- Testing 1 basin\n",
      "Processing geospatial data into attributes for CAN_01AD002\n",
      " - processing era5\n",
      " - processing worldclim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wmk934/data/CAMELS_spat/camels-spat-env-TRIAL/lib/python3.11/site-packages/rasterstats/io.py:328: NodataWarning: Setting nodata to -999; specify nodata explicitly\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - processing hydrology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - processing lai\n",
      " - processing forest_height\n",
      " - processing glclu2019\n",
      " - processing modis_land\n",
      " - processing lgrip30\n",
      " - processing merit\n",
      " - processing hydrolakes\n",
      " - processing pelletier\n",
      " - processing glhymps\n",
      "\n",
      "!!! CHECK DEBUGGING STATUS: \n",
      "- Testing 1 file \n",
      "- Testing 1 basin\n"
     ]
    }
   ],
   "source": [
    "print(debug_message)\n",
    "for ix,row in cs_meta.iterrows():\n",
    "\n",
    "    # DEBUGGING\n",
    "    if ix != 0: continue\n",
    "\n",
    "    # Get the paths\n",
    "    basin_id, shp_lump_path, shp_dist_path, _, _ = prepare_delineation_outputs(cs_meta, ix, basins_path)\n",
    "    geo_folder = basins_path / 'basin_data' / basin_id / 'geospatial'\n",
    "    met_folder = basins_path / 'basin_data' / basin_id / 'forcing'\n",
    "    hyd_folder = basins_path / 'basin_data' / basin_id / 'observations'\n",
    "\n",
    "    # Data storage\n",
    "    l_gauges.append(basin_id) # Update the Station list\n",
    "    l_values = [] # Initialize an empty list where we'll store this basin's attributes\n",
    "    l_index = [] # Initialize an empty list where we'll store the attribute descriptions\n",
    "\n",
    "    # Define the shapefiles\n",
    "    shp = str(shp_lump_path) # because zonalstats wants a file path, not a geodataframe\n",
    "    riv = str(shp_dist_path).format('river') # For topographic attributes\n",
    "    \n",
    "    # Data-specific processing\n",
    "    print(f'Processing geospatial data into attributes for {basin_id}')\n",
    "    for dataset in data_subfolders:\n",
    "        print(f' - processing {dataset}')\n",
    "\n",
    "        ## CLIMATE\n",
    "        if dataset == 'era5':\n",
    "            l_values, l_index, ds_precip, ds_era5 = csa.attributes_from_era5(met_folder, shp, 'era5', l_values, l_index)                                \n",
    "        if dataset == 'worldclim':\n",
    "            csa.oudin_pet_from_worldclim(geo_folder, dataset) # Get an extra PET estimate to sanity check ERA5 outcomes\n",
    "            csa.aridity_and_fraction_snow_from_worldclim(geo_folder, dataset) # Get monthly aridity and fraction snow maps\n",
    "            l_values, l_index = csa.attributes_from_worldclim(geo_folder, dataset, shp, l_values, l_index)\n",
    "\n",
    "        ## LAND COVER\n",
    "        if dataset == 'forest_height':\n",
    "            l_values, l_index = csa.attributes_from_forest_height(geo_folder, dataset, shp, l_values, l_index)\n",
    "        if dataset == 'lai':\n",
    "            l_values, l_index = csa.attributes_from_lai(geo_folder, dataset, temp_path, shp, l_values, l_index)\n",
    "        if dataset == 'glclu2019':\n",
    "            l_values, l_index = csa.attributes_from_glclu2019(geo_folder, dataset, shp, l_values, l_index)\n",
    "        if dataset == 'modis_land':\n",
    "            l_values, l_index = csa.attributes_from_modis_land(geo_folder, dataset, shp, l_values, l_index)\n",
    "        if dataset == 'lgrip30':\n",
    "            l_values, l_index = csa.attributes_from_lgrip30(geo_folder, dataset, shp, l_values, l_index)\n",
    "\n",
    "        ## TOPOGRAPHY\n",
    "        if dataset == 'merit':\n",
    "            l_values, l_index = csa.attributes_from_merit(geo_folder, dataset, shp, riv, row, l_values, l_index)\n",
    "\n",
    "        ## OPENWATER\n",
    "        if dataset == 'hydrolakes':\n",
    "            l_values, l_index = csa.attributes_from_hydrolakes(geo_folder, dataset, l_values, l_index)\n",
    "        if dataset == 'hydrology':\n",
    "            l_values, l_index = csa.attributes_from_streamflow(hyd_folder, dataset, basin_id, ds_precip, row, l_values, l_index)\n",
    "\n",
    "        ## SOIL\n",
    "        if dataset == 'pelletier':\n",
    "            l_values, l_index = csa.attributes_from_pelletier(geo_folder, dataset, shp, l_values, l_index)\n",
    "        if dataset == 'soilgrids':\n",
    "            l_values, l_index = csa.attributes_from_soilgrids(geo_folder, dataset, shp, l_values, l_index)\n",
    "\n",
    "        ## GEOLOGY\n",
    "        if dataset == 'glhymps':\n",
    "            l_values, l_index = csa.attributes_from_glhymps(geo_folder, dataset, l_values, l_index)\n",
    "            \n",
    "print(debug_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d197b84-b519-487b-aaf4-b14929671829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(840, 840)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l_values),len(l_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f740e80-faf5-4c56-a379-48b3fb2b6b5f",
   "metadata": {},
   "source": [
    "#### Make the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f038ae6-5dff-43cf-a462-8d31eea3d39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>CAN_01AD002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Climate</th>\n",
       "      <th>num_years_era5</th>\n",
       "      <th>years</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mtpr_mean</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>1137.1210937712333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mtpr_std</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>136.74820016873835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mper_mean</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>209.9189803019856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mper_std</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>12.083002663320935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Geology</th>\n",
       "      <th>porosity_std</th>\n",
       "      <th>-</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>0.07108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_permeability_min</th>\n",
       "      <th>m^2</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>-16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_permeability_mean</th>\n",
       "      <th>m^2</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>-14.377311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_permeability_max</th>\n",
       "      <th>m^2</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>-12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_permeability_std</th>\n",
       "      <th>m^2</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>1.234355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>840 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     CAN_01AD002\n",
       "Category Attribute             Unit  Source                     \n",
       "Climate  num_years_era5        years ERA5                     70\n",
       "         mtpr_mean             mm    ERA5     1137.1210937712333\n",
       "         mtpr_std              mm    ERA5     136.74820016873835\n",
       "         mper_mean             mm    ERA5      209.9189803019856\n",
       "         mper_std              mm    ERA5     12.083002663320935\n",
       "...                                                          ...\n",
       "Geology  porosity_std          -     GLHYMPS             0.07108\n",
       "         log_permeability_min  m^2   GLHYMPS               -16.5\n",
       "         log_permeability_mean m^2   GLHYMPS          -14.377311\n",
       "         log_permeability_max  m^2   GLHYMPS               -12.5\n",
       "         log_permeability_std  m^2   GLHYMPS            1.234355\n",
       "\n",
       "[840 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with a fake second station\n",
    "l_gauges = ['CAN_01AD002','CAN_01AD003']\n",
    "\n",
    "# Make the dataframe\n",
    "input_dict = dict(zip(l_gauges, [l_values,l_values]))\n",
    "df = pd.DataFrame(input_dict)\n",
    "\n",
    "# Set the index\n",
    "multi_index = pd.MultiIndex.from_tuples(l_index, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "df.index = multi_index\n",
    "\n",
    "# Drop the fake extra column\n",
    "df = df.drop(columns=['CAN_01AD003'], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "309ae298-c51c-4fff-b74f-201e02ba5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test_new_function5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b27b6090-0a20-48aa-8c85-0a0c5e3ceff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>CAN_01AD002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">Climate</th>\n",
       "      <th>mper_mean_month_01</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>5.603236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mper_mean_month_02</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>6.93889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mper_mean_month_03</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>11.45397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mper_mean_month_04</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>17.082485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mper_mean_month_05</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>29.253726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fracsnow2_std_month_10</th>\n",
       "      <th>-</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fracsnow2_mean_month_11</th>\n",
       "      <th>-</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>0.999119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fracsnow2_std_month_11</th>\n",
       "      <th>-</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>0.029672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fracsnow2_mean_month_12</th>\n",
       "      <th>-</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fracsnow2_std_month_12</th>\n",
       "      <th>-</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                CAN_01AD002\n",
       "Category Attribute               Unit Source               \n",
       "Climate  mper_mean_month_01      mm   ERA5         5.603236\n",
       "         mper_mean_month_02      mm   ERA5          6.93889\n",
       "         mper_mean_month_03      mm   ERA5         11.45397\n",
       "         mper_mean_month_04      mm   ERA5        17.082485\n",
       "         mper_mean_month_05      mm   ERA5        29.253726\n",
       "...                                                     ...\n",
       "         fracsnow2_std_month_10  -    WorldClim         0.0\n",
       "         fracsnow2_mean_month_11 -    WorldClim    0.999119\n",
       "         fracsnow2_std_month_11  -    WorldClim    0.029672\n",
       "         fracsnow2_mean_month_12 -    WorldClim         1.0\n",
       "         fracsnow2_std_month_12  -    WorldClim         0.0\n",
       "\n",
       "[600 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of selection code\n",
    "tmp = df.loc[df.index.get_level_values('Category').str.contains('Climate')]# & \n",
    "             #df.index.get_level_values('Attribute').str.contains('mean')].copy()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127feb5-ed86-424b-8367-d4ec38177968",
   "metadata": {},
   "source": [
    "## DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fe183cc-8879-4189-9e21-014a1a76204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import baseflow\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "import numpy as np\n",
    "from rasterstats import zonal_stats\n",
    "import rasterio\n",
    "from scipy.stats import circmean, circstd, skew, kurtosis\n",
    "from scipy.optimize import curve_fit\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd30bf-7f36-4a08-89bb-cc86515a05ab",
   "metadata": {},
   "source": [
    "### High-level collection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad527f-f424-4899-b781-370c7515218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_soilgrids(geo_folder, dataset, shp_str, l_values, l_index):\n",
    "\n",
    "    '''Calculates attributes from SOILGRIDS maps'''\n",
    "\n",
    "    # File specifiction\n",
    "    sub_folders = ['bdod',     'cfvo',       'clay',    'sand',    'silt',    'soc']\n",
    "    units       = ['cg cm^-3', 'cm^3 dm^-3', 'g kg^-1', 'g kg^-1', 'g kg^-1', 'dg kg^-1']\n",
    "    depths = ['0-5cm', '5-15cm', '15-30cm', '30-60cm', '60-100cm', '100-200cm']\n",
    "    fields = ['mean', 'uncertainty']\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    \n",
    "    # Loop over the files and calculate stats\n",
    "    for sub_folder, unit in zip(sub_folders, units):\n",
    "        for depth in depths:\n",
    "            for field in fields:\n",
    "                tif = str(geo_folder / dataset / 'raw' / f'{sub_folder}' / f'{sub_folder}_{depth}_{field}.tif')\n",
    "                zonal_out = zonal_stats(shp_str, tif, stats=stats)\n",
    "                scale,offset = csa.read_scale_and_offset(tif)\n",
    "                l_values = csa.update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "                l_index += [('Soil', f'{sub_folder}_{depth}_{stat}_min',  f'{unit}', 'SOILGRIDS'),\n",
    "                            ('Soil', f'{sub_folder}_{depth}_{stat}_mean', f'{unit}', 'SOILGRIDS'),\n",
    "                            ('Soil', f'{sub_folder}_{depth}_{stat}_max',  f'{unit}', 'SOILGRIDS'),\n",
    "                            ('Soil', f'{sub_folder}_{depth}_{stat}_std',  f'{unit}', 'SOILGRIDS')]\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79319e1e-4f35-447a-8733-2652714f68f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_glhymps(geo_folder, dataset, l_values, l_index):\n",
    "\n",
    "    '''Calculates attributes from GLHYMPS'''\n",
    "\n",
    "    # Load the file\n",
    "    geol_str = str(geo_folder / dataset / 'raw' / 'glhymps.shp')\n",
    "    geol = gpd.read_file(geol_str)\n",
    "\n",
    "    # Rename the columns, because the shortened ones are not very helpful\n",
    "    if ('Porosity' in geol.columns) and ('Permeabili' in geol.columns) \\\n",
    "    and ('Permeabi_1' in geol.columns) and ('Permeabi_2' in geol.columns):\n",
    "        geol.rename(columns={'Porosity': 'porosity',   'Permeabili': 'logK_Ferr',\n",
    "                             'Permeabi_1': 'logK_Ice', 'Permeabi_2': 'logK_std'}, inplace=True)\n",
    "        geol.to_file(geol_str)\n",
    "\n",
    "    # Porosity\n",
    "    l_values.append( geol['porosity'].min() )\n",
    "    l_index.append(('Geology', 'porosity_min',  '-', 'GLHYMPS'))\n",
    "    l_values.append( geol['porosity'].mean() )\n",
    "    l_index.append(('Geology', 'porosity_mean',  '-', 'GLHYMPS'))\n",
    "    l_values.append( geol['porosity'].max() )\n",
    "    l_index.append(('Geology', 'porosity_max',  '-', 'GLHYMPS'))\n",
    "    l_values.append( geol['porosity'].std() )\n",
    "    l_index.append(('Geology', 'porosity_std',  '-', 'GLHYMPS'))\n",
    "\n",
    "    # Permeability\n",
    "    l_values.append( geol['logK_Ice'].min() )\n",
    "    l_index.append(('Geology', 'log_permeability_min',  'm^2', 'GLHYMPS'))\n",
    "    l_values.append( geol['logK_Ice'].mean() )\n",
    "    l_index.append(('Geology', 'log_permeability_mean',  'm^2', 'GLHYMPS'))\n",
    "    l_values.append( geol['logK_Ice'].max() )\n",
    "    l_index.append(('Geology', 'log_permeability_max',  'm^2', 'GLHYMPS'))\n",
    "    l_values.append( geol['logK_Ice'].std() )\n",
    "    l_index.append(('Geology', 'log_permeability_std',  'm^2', 'GLHYMPS'))\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7757733b-4eeb-479b-a9ea-c3c0d68f8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_pelletier(geo_folder, dataset, shp, l_values, l_index):\n",
    "    \n",
    "    '''Calculates statistics for Pelletier maps'''\n",
    "    # See: https://daac.ornl.gov/SOILS/guides/Global_Soil_Regolith_Sediment.html\n",
    "\n",
    "    # General\n",
    "    files = ['upland_hill-slope_regolith_thickness.tif',\n",
    "             'upland_hill-slope_soil_thickness.tif',\n",
    "             'upland_valley-bottom_and_lowland_sedimentary_deposit_thickness.tif',\n",
    "             'average_soil_and_sedimentary-deposit_thickness.tif']\n",
    "    attrs = ['regolith_thickness',    'soil_thickness',\n",
    "             'sedimentary_thickness', 'average_thickness']\n",
    "    units = ['m', 'm', 'm', 'm']\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "\n",
    "    for file,att,unit in zip(files,attrs,units):\n",
    "        tif = str( geo_folder / dataset / 'raw' / file )\n",
    "        zonal_out = zonal_stats(shp, tif, stats=stats)\n",
    "        scale,offset = read_scale_and_offset(tif)\n",
    "        l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "        l_index += [('Soil', f'{att}_min',  f'{unit}', 'Pelletier'),\n",
    "                    ('Soil', f'{att}_mean', f'{unit}', 'Pelletier'),\n",
    "                    ('Soil', f'{att}_max',  f'{unit}', 'Pelletier'),\n",
    "                    ('Soil', f'{att}_std',  f'{unit}', 'Pelletier')]\n",
    "        \n",
    "    return l_values,l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed902534-4015-47a3-a37f-0a1639ea9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_streamflow(hyd_folder, dataset, basin_id, pre, row, l_values, l_index):\n",
    "\n",
    "    '''Calculates various streamflow signatures'''\n",
    "\n",
    "    # Constants\n",
    "    seconds_per_minute = 60 # s min-1\n",
    "    seconds_per_hour = 60 * seconds_per_minute # s hr-1\n",
    "    seconds_per_day = 24 * seconds_per_hour # s d-1\n",
    "    water_density = 1000 # kg m-3\n",
    "    mm_per_m = 1000 # mm m-1\n",
    "    m_per_km = 1000 # m km-1\n",
    "\n",
    "    # Find the data source\n",
    "    if row['Country'] == 'CAN':\n",
    "        source = 'WSC'\n",
    "    elif row['Country'] == 'USA':\n",
    "        source = 'USGS'\n",
    "\n",
    "    # Load observations\n",
    "    hyd_file = hyd_folder / f'{basin_id}_daily_flow_observations.nc'\n",
    "    hyd = xr.open_dataset(hyd_file)\n",
    "    hyd = csa.subset_dataset_to_max_full_years(hyd, res='day', water_year=True)\n",
    "\n",
    "    # Convert observations to mm d-1\n",
    "    area_km2 = row['Basin_area_km2']\n",
    "    area_m2 = area_km2 * m_per_km**2 # [km2] * ([m km-1]^2 = [m2 km-2]) = [m2]\n",
    "    hyd['q_obs'] = hyd['q_obs'] * seconds_per_day / area_m2 * mm_per_m # [m3 s-1] * [s d-1] / [m2] * [mm m-1] = [m d-1]\n",
    "\n",
    "    # Convert precipitation into mm d-1\n",
    "    pre = (pre * seconds_per_hour).resample(time='1D').sum() / water_density * mm_per_m # ([kg m-2 s-1] * [s hr-1]).resample(time='1D').sum() / [kg m-3] * [mm m-1] = [mm d-1]\n",
    "    \n",
    "    # Match times between hydrologic data and precipitation\n",
    "    pre = pre.sel(time=slice(hyd['time'][0].values, hyd['time'][-1].values))\n",
    "    assert hyd['time'][0].values == pre['time'][0].values, 'attributes_from_streamflow(): mismatch between precipitation and streamflow start timestamp'\n",
    "    assert hyd['time'][-1].values == pre['time'][-1].values, 'attributes_from_streamflow(): mismatch between precipitation and streamflow final timestamp'\n",
    "\n",
    "    # Create a water-year time variable\n",
    "    hyd['water_year'] = hyd['time'].dt.year.where(hyd['time'].dt.month < 10, hyd['time'].dt.year + 1)\n",
    "    pre['water_year'] = pre['time'].dt.year.where(pre['time'].dt.month < 10, pre['time'].dt.year + 1)\n",
    "    \n",
    "    # Track the data years used\n",
    "    num_years = len(hyd.groupby('time.year'))\n",
    "    l_values.append(num_years)\n",
    "    l_index.append( ('Hydrology', 'num_years_hyd ', 'years', '-') )\n",
    "    \n",
    "    # Signatures\n",
    "    calculate_signatures(hyd, pre, source, l_values, l_index)\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740b45f-f227-41bb-8371-42abda75c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_hydrolakes(geo_folder, dataset, l_values, l_index):\n",
    "    \n",
    "    '''Calculates open water attributes from HydroLAKES'''\n",
    "\n",
    "    # Load the file\n",
    "    lake_str = str(geo_folder / dataset / 'raw'    / 'HydroLAKES_polys_v10_NorthAmerica.shp')\n",
    "    lakes = gpd.read_file(lake_str)\n",
    "\n",
    "    # General stats\n",
    "    res_mask  = lakes['Lake_type'] == 2 # Lake Type 2 == reservoir; see docs (https://data.hydrosheds.org/file/technical-documentation/HydroLAKES_TechDoc_v10.pdf)\n",
    "    num_lakes = len(lakes)\n",
    "    num_resvr = res_mask.sum() \n",
    "    l_values.append(num_lakes)\n",
    "    l_index.append(('Open water', 'open_water_number',  '-', 'HydroLAKES'))\n",
    "    l_values.append(num_resvr)\n",
    "    l_index.append(('Open water', 'known_reservoirs',  '-', 'HydroLAKES'))\n",
    "\n",
    "    # Summary stats\n",
    "    l_values, l_index = get_open_water_stats(lakes, 'Lake_area', 'all', l_values, l_index) # All open water\n",
    "    l_values, l_index = get_open_water_stats(lakes, 'Vol_total', 'all', l_values, l_index)\n",
    "    l_values, l_index = get_open_water_stats(lakes, 'Lake_area', 'reservoir', l_values, l_index) # Reservoirs only\n",
    "    l_values, l_index = get_open_water_stats(lakes, 'Vol_total', 'reservoir', l_values, l_index)\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543601ad-64ad-4d95-8609-4f7cbdb0a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_merit(geo_folder, dataset, shp_str, riv_str, row, l_values, l_index):\n",
    "    \n",
    "    '''Calculates topographic attributes from MERIT data'''\n",
    "\n",
    "    ## Known values\n",
    "    lat = row['Station_lat']\n",
    "    lon = row['Station_lon']\n",
    "    area= row['Basin_area_km2']\n",
    "    src = row['Station_source']\n",
    "    l_values.append(lat)\n",
    "    l_index.append(('Topography', 'gauge_lat',  'degrees', f'{src}'))\n",
    "    l_values.append(lon)\n",
    "    l_index.append(('Topography', 'gauge_lon',  'degrees', f'{src}'))\n",
    "    l_values.append(area)\n",
    "    l_index.append(('Topography', 'basin_area', 'km^2', 'MERIT Hydro'))\n",
    "\n",
    "    ## RASTERS\n",
    "    # Slope and elevation can use zonal stats\n",
    "    files = [str(geo_folder / dataset / 'raw'    / 'merit_hydro_elv.tif'),\n",
    "             str(geo_folder / dataset / 'slope'  / 'merit_hydro_slope.tif')]\n",
    "    attrs = ['merit_hydro_elev', 'merit_hydro_slope']\n",
    "    units = ['m.a.s.l.',         'm m-1']\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    for tif,att,unit in zip(files,attrs,units):\n",
    "        zonal_out = zonal_stats(shp_str, tif, stats=stats)\n",
    "        scale,offset = csa.read_scale_and_offset(tif)\n",
    "        l_values = csa.update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "        l_index += [('Topography', f'{att}_min',  f'{unit}', 'MERIT Hydro'),\n",
    "                    ('Topography', f'{att}_mean', f'{unit}', 'MERIT Hydro'),\n",
    "                    ('Topography', f'{att}_max',  f'{unit}', 'MERIT Hydro'),\n",
    "                    ('Topography', f'{att}_std',  f'{unit}', 'MERIT Hydro')]\n",
    "\n",
    "    # Aspect needs circular stats\n",
    "    tif = str(geo_folder / dataset / 'aspect' / 'merit_hydro_aspect.tif')\n",
    "    l_values, l_index = get_aspect_attributes(tif,l_values,l_index) \n",
    "\n",
    "    ## VECTOR\n",
    "    l_values, l_index = get_river_attributes(riv_str, l_values, l_index, area)\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4033e983-99ba-4070-9a05-ecf0048719e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_lgrip30(geo_folder, dataset, shp_str, l_values, l_index):\n",
    "\n",
    "    '''Calculates percentage occurrence of all classes in LGRIP30 map'''\n",
    "\n",
    "    tif = geo_folder / dataset / 'raw' / 'lgrip30_agriculture.tif'\n",
    "    zonal_out = zonal_stats(shp_str, tif, categorical=True)\n",
    "    check_scale_and_offset(tif)\n",
    "    l_values,l_index = update_values_list_with_categorical(l_values, l_index, zonal_out, 'LGRIP30', prefix='lc3_')\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4579b4a9-d3ae-4665-a16d-e9bead393f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_modis_land(geo_folder, dataset, shp_str, l_values, l_index):\n",
    "\n",
    "    '''Calculates percentage occurrence of all classes in MODIS IGBP map'''\n",
    "\n",
    "    tif = geo_folder / dataset / 'raw' / '2001_2022_mode_MCD12Q1_LC_Type1.tif'\n",
    "    zonal_out = zonal_stats(shp_str, tif, categorical=True)\n",
    "    check_scale_and_offset(tif)\n",
    "    l_values,l_index = update_values_list_with_categorical(l_values, l_index, zonal_out, 'MCD12Q1.061', prefix='lc2_')\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00eb7bfe-a77d-49bf-8468-510437a42aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_glclu2019(geo_folder, dataset, shp_str, l_values, l_index):\n",
    "\n",
    "    '''Calculates percentage occurrence of all classes in GLCLU2019 map'''\n",
    "\n",
    "    tif = geo_folder / dataset / 'raw' / 'glclu2019_map.tif'\n",
    "    zonal_out = zonal_stats(shp_str, tif, categorical=True)\n",
    "    check_scale_and_offset(tif)\n",
    "    l_values,l_index = update_values_list_with_categorical(l_values, l_index, zonal_out, 'GLCLU 2019', prefix='lc1_')\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "101eb959-c4a2-41dc-8e60-7694dca0de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_era5(met_folder, shp_path, dataset, l_values, l_index, use_mfdataset=False):\n",
    "\n",
    "    '''Calculates a variety of metrics from ERA5 data'''\n",
    "\n",
    "    # Define various conversion constants\n",
    "    water_density = 1000 # kg m-3\n",
    "    mm_per_m = 1000 # mm m-1\n",
    "    seconds_per_hour = 60*60 # s h-1\n",
    "    seconds_per_day = seconds_per_hour*24 # s d-1\n",
    "    days_per_month = np.array([31, 28.25, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]).reshape(-1, 1) # d month-1\n",
    "    days_per_year = days_per_month.sum()\n",
    "    flip_sign = -1 # -; used to convert PET from negative (by convention this indicates an upward flux) to positive\n",
    "    kelvin_to_celsius = -273.15\n",
    "    pa_per_kpa = 1000 # Pa kPa-1\n",
    "\n",
    "    # Define file locations, depending on if we are dealing with lumped or distributed cases\n",
    "    if 'lumped' in shp_path:\n",
    "        era_folder = met_folder / 'lumped'\n",
    "    elif 'distributed' in shp_path:\n",
    "        era_folder = met_folder / 'distributed'\n",
    "    era_files = sorted( glob.glob( str(era_folder / 'ERA5_*.nc') ) )\n",
    "\n",
    "    # Open the data\n",
    "    if use_mfdataset:\n",
    "        ds = xr.open_mfdataset(era_files, engine='netcdf4')\n",
    "        ds = ds.load() # load the whole thing into memory instead of lazy-loading\n",
    "    else:\n",
    "        ds = xr.merge([xr.open_dataset(f) for f in era_files])\n",
    "        ds = ds.load()\n",
    "    \n",
    "    # Select whole years only\n",
    "    #   This avoids issues in cases where we have incomplete whole data years\n",
    "    #   (e.g. 2000-06-01 to 2007-12-31) in basins with very seasonal weather\n",
    "    #   (e.g. all precip occurs in Jan, Feb, Mar). By using only full years\n",
    "    #   we avoid accidentally biasing the attributes.\n",
    "    ds = subset_dataset_to_max_full_years(ds)\n",
    "\n",
    "    # --- Annual statistics (P, PET, T, aridity, seasonality, temperature, snow)\n",
    "    # P\n",
    "    yearly_mtpr = ds['mtpr'].resample(time='1YE').mean() * seconds_per_day * days_per_year * mm_per_m / water_density # kg m-2 s-1 > mm yr-1\n",
    "    l_values.append(yearly_mtpr.mean().values)\n",
    "    l_index.append(('Climate', 'mtpr_mean', 'mm', 'ERA5'))\n",
    "    l_values.append(yearly_mtpr.std().values)\n",
    "    l_index.append(('Climate', 'mtpr_std', 'mm', 'ERA5'))\n",
    "    \n",
    "    # PET\n",
    "    yearly_mper = ds['mper'].resample(time='1YE').mean() * flip_sign * seconds_per_day * days_per_year * mm_per_m / water_density # kg m-2 s-1 > mm yr-1\n",
    "    l_values.append(yearly_mper.mean().values)\n",
    "    l_index.append(('Climate', 'mper_mean', 'mm', 'ERA5'))\n",
    "    l_values.append(yearly_mper.std().values)\n",
    "    l_index.append(('Climate', 'mper_std', 'mm', 'ERA5'))\n",
    "    \n",
    "    # T\n",
    "    yearly_tavg = ds['t'].resample(time='1YE').mean() + kelvin_to_celsius # K > C\n",
    "    l_values.append(yearly_tavg.mean().values)\n",
    "    l_index.append(('Climate', 'tdavg_mean', 'C', 'ERA5'))\n",
    "    l_values.append(yearly_tavg.std().values)\n",
    "    l_index.append(('Climate', 'tdavg_std', 'C', 'ERA5'))\n",
    "    \n",
    "    # Aridity\n",
    "    yearly_ari  = yearly_mper / yearly_mtpr\n",
    "    l_values.append(yearly_ari.mean().values)\n",
    "    l_index.append(('Climate', 'aridity1_mean', '-', 'ERA5'))\n",
    "    l_values.append(yearly_ari.std().values)\n",
    "    l_index.append(('Climate', 'aridity1_std', '-', 'ERA5'))\n",
    "    \n",
    "    # Snow\n",
    "    ds['snow'] = xr.where(ds['t'] < 273.15, ds['mtpr'],0) # rates: kg m-2 s-1\n",
    "    yearly_snow = ds['snow'].resample(time='1YE').mean() * seconds_per_day * days_per_year * mm_per_m / water_density # kg m-2 s-1 > mm yr-1\n",
    "    yearly_fs   = yearly_snow / yearly_mtpr\n",
    "    l_values.append(yearly_fs.mean().values)\n",
    "    l_index.append(('Climate', 'fracsnow1_mean', '-', 'ERA5'))\n",
    "    l_values.append(yearly_fs.std().values)\n",
    "    l_index.append(('Climate', 'fracsnow1_std', '-', 'ERA5'))\n",
    "    \n",
    "    # Seasonality\n",
    "    seasonality = find_climate_seasonality_era5(ds,use_typical_cycle=False)\n",
    "    l_values.append(seasonality.mean())\n",
    "    l_index.append(('Climate', 'seasonality1_mean', '-', 'ERA5'))\n",
    "    l_values.append(seasonality.std())\n",
    "    l_index.append(('Climate', 'seasonality1_std', '-', 'ERA5'))\n",
    "    \n",
    "    # --- Monthly attributes\n",
    "    # Calculate monthly PET in mm\n",
    "    #      kg m-2 s-1 / kg m-3\n",
    "    # mm month-1 = kg m-2 s-1 * kg-1 m3 * s d-1 * d month-1 * mm m-1 * -\n",
    "    monthly_mper = ds['mper'].resample(time='1ME').mean().groupby('time.month') \n",
    "    mper_m = monthly_mper.mean() / water_density * seconds_per_day * days_per_month * mm_per_m * flip_sign  # [kg m-2 s-1] to [mm month-1]; negative to indicate upward flux\n",
    "    mper_s = monthly_mper.std() / water_density * seconds_per_day * days_per_month * mm_per_m  # [kg m-2 s-1] to [mm month-1]; negative to indicate upward flux\n",
    "    l_values, l_index = process_era5_means_to_lists(mper_m, 'mean', l_values, l_index, 'mper', 'mm')\n",
    "    l_values, l_index = process_era5_means_to_lists(mper_s, 'std', l_values, l_index, 'mper', 'mm')\n",
    "        \n",
    "    # Same for precipitation: [mm month-1]\n",
    "    monthly_mtpr = ds['mtpr'].resample(time='1ME').mean().groupby('time.month')\n",
    "    mtpr_m = monthly_mtpr.mean() / water_density * seconds_per_day * days_per_month * mm_per_m # [kg m-2 s-1] to [mm month-1]\n",
    "    mtpr_s = monthly_mtpr.std() / water_density * seconds_per_day * days_per_month * mm_per_m # [kg m-2 s-1] to [mm month-1]\n",
    "    l_values, l_index = process_era5_means_to_lists(mtpr_m, 'mean', l_values, l_index, 'mtpr', 'mm')\n",
    "    l_values, l_index = process_era5_means_to_lists(mtpr_s, 'std', l_values, l_index, 'mtpr', 'mm')\n",
    "    \n",
    "    # Monthly temperature statistics [C]\n",
    "    monthly_tavg = (ds['t'].resample(time='1D').mean().resample(time='1ME').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tavg_m = monthly_tavg.mean()\n",
    "    tavg_s = monthly_tavg.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(tavg_m, 'mean', l_values, l_index, 'tdavg', 'C')\n",
    "    l_values, l_index = process_era5_means_to_lists(tavg_s, 'std', l_values, l_index, 'tdavg', 'C')\n",
    "    \n",
    "    monthly_tmin = (ds['t'].resample(time='1D').min().resample(time='1ME').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tmin_m = monthly_tmin.mean()\n",
    "    tmin_s = monthly_tmin.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(tmin_m, 'mean', l_values, l_index, 'tdmin', 'C')\n",
    "    l_values, l_index = process_era5_means_to_lists(tmin_m, 'std', l_values, l_index, 'tdmin', 'C')\n",
    "    \n",
    "    monthly_tmax = (ds['t'].resample(time='1D').max().resample(time='1ME').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tmax_m = monthly_tmax.mean()\n",
    "    tmax_s = monthly_tmax.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(tmax_m, 'mean', l_values, l_index, 'tdmax', 'C')\n",
    "    l_values, l_index = process_era5_means_to_lists(tmax_s, 'std', l_values, l_index, 'tdmax', 'C')\n",
    "    \n",
    "    # Monthly shortwave and longwave [W m-2]\n",
    "    monthly_sw = ds['msdwswrf'].resample(time='1ME').mean().groupby('time.month')\n",
    "    sw_m = monthly_sw.mean()\n",
    "    sw_s = monthly_sw.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(sw_m, 'mean', l_values, l_index, 'msdwswrf', 'W m^-2')\n",
    "    l_values, l_index = process_era5_means_to_lists(sw_s, 'std', l_values, l_index, 'msdwswrf', 'W m^-2')\n",
    "    \n",
    "    monthly_lw = ds['msdwlwrf'].resample(time='1ME').mean().groupby('time.month')\n",
    "    lw_m = monthly_lw.mean(dim='time')\n",
    "    lw_s = monthly_lw.std(dim='time')\n",
    "    l_values, l_index = process_era5_means_to_lists(lw_m, 'mean', l_values, l_index, 'msdwlwrf', 'W m^-2')\n",
    "    l_values, l_index = process_era5_means_to_lists(lw_s, 'std', l_values, l_index, 'msdwlwrf', 'W m^-2')\n",
    "\n",
    "    # Surface pressure [Pa]\n",
    "    monthly_sp = ds['sp'].resample(time='1ME').mean().groupby('time.month')\n",
    "    sp_m = monthly_sp.mean() / pa_per_kpa # [Pa] > [kPa]\n",
    "    sp_s = monthly_sp.std() / pa_per_kpa\n",
    "    l_values, l_index = process_era5_means_to_lists(sp_m, 'mean', l_values, l_index, 'sp', 'kPa')\n",
    "    l_values, l_index = process_era5_means_to_lists(sp_s, 'std', l_values, l_index, 'sp', 'kPa')\n",
    "    \n",
    "    # Humidity [-]\n",
    "    monthly_q = ds['q'].resample(time='1ME').mean().groupby('time.month') # specific\n",
    "    q_m = monthly_q.mean()\n",
    "    q_s = monthly_q.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(q_m, 'mean', l_values, l_index, 'q', 'kg kg^-1')\n",
    "    l_values, l_index = process_era5_means_to_lists(q_s, 'std', l_values, l_index, 'q', 'kg kg^-1')\n",
    "    \n",
    "    monthly_rh = ds['rh'].resample(time='1ME').mean().groupby('time.month') # relative\n",
    "    rh_m = monthly_rh.mean()\n",
    "    rh_s = monthly_rh.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(rh_m, 'mean', l_values, l_index, 'rh', 'kPa kPa^-1')\n",
    "    l_values, l_index = process_era5_means_to_lists(rh_s, 'std', l_values, l_index, 'rh', 'kPa kPa^-1')\n",
    "    \n",
    "    # Wind speed [m s-1]\n",
    "    monthly_w = ds['w'].resample(time='1ME').mean().groupby('time.month')\n",
    "    w_m = monthly_w.mean()\n",
    "    w_s = monthly_w.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(w_m, 'mean', l_values, l_index, 'w', 'm s^-1')\n",
    "    l_values, l_index = process_era5_means_to_lists(w_s, 'std', l_values, l_index, 'w', 'm s^-1')\n",
    "    \n",
    "    # Wind direction\n",
    "    monthly_phi = ds['phi'].resample(time='1ME').apply(circmean_group).groupby('time.month')\n",
    "    phi_m = monthly_phi.apply(circmean_group)\n",
    "    phi_s = monthly_phi.apply(circstd_group)\n",
    "    l_values, l_index = process_era5_means_to_lists(phi_m, 'mean', l_values, l_index, 'phi', 'degrees')\n",
    "    l_values, l_index = process_era5_means_to_lists(phi_s, 'std', l_values, l_index, 'phi', 'degrees')\n",
    "    \n",
    "    # --- Long-term statistics (aridity, seasonality, snow)\n",
    "    monthly_ari = ((ds['mper'].resample(time='1ME').mean() * flip_sign) / ds['mtpr'].resample(time='1ME').mean()).groupby('time.month')\n",
    "    ari_m = monthly_ari.mean()\n",
    "    ari_s = monthly_ari.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(ari_m, 'mean', l_values, l_index, 'aridity', '-')\n",
    "    l_values, l_index = process_era5_means_to_lists(ari_s, 'std', l_values, l_index, 'aridity', '-')\n",
    "\n",
    "    ds['snow'] = xr.where(ds['t'] < 273.15, ds['mtpr'],0)\n",
    "    monthly_snow = (ds['snow'].resample(time='1ME').mean() / ds['mtpr'].resample(time='1ME').mean()).groupby('time.month')\n",
    "    snow_m = monthly_snow.mean()\n",
    "    snow_s = monthly_snow.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(snow_m, 'mean', l_values, l_index, 'fracsnow', '-')\n",
    "    l_values, l_index = process_era5_means_to_lists(snow_s, 'std', l_values, l_index, 'fracsnow', '-')\n",
    "\n",
    "    # --- High-frequency statistics (high/low duration/timing/magnitude)\n",
    "    #  Everyone does precip. We'll add temperature too as a drought/frost indicator\n",
    "    #  ERA5 only\n",
    "    \n",
    "    # -- LOW TEMPERATURE\n",
    "    variable  = 't'\n",
    "    low_threshold = 273.15 # K, freezing point\n",
    "    low_condition = ds[variable] < low_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('temp',low_condition,'low',l_values,l_index)\n",
    "    \n",
    "    # -- HIGH TEMPERATURE\n",
    "    # WMO defines a heat wave as a 5-day or longer period with maximum daily temperatures 5C above \n",
    "    # \"standard\" daily max temperature (1961-1990; source:\n",
    "    # https://www.ifrc.org/sites/default/files/2021-06/10-HEAT-WAVE-HR.pdf).\n",
    "    # We define a \"hot day\" therefore as a day with a maximum temperature 5 degrees over the \n",
    "    # the long-term mean maximum temperature.\n",
    "    #   Note: we don't have 1961-1990 data for some stations, so we stick with long-term mean.\n",
    "    #   Note: this will in most cases slightly underestimate heat waves compared to WMO definition\n",
    "    \n",
    "    # First, we identify the long-term mean daily maximum temperature in a dedicated function\n",
    "    high_threshold = create_mean_daily_max_series(ds,var='t')\n",
    "    \n",
    "    # Next, we check if which 't' values are 5 degrees above the long-term mean daily max \n",
    "    #  (\"(ds['t'] > result_array + 5)\"), and resample this to a daily time series \n",
    "    #  (\"resample(time='1D')\") filled with \"True\" if any value in that day was True.\n",
    "    daily_flags = (ds['t'] > high_threshold + 5).resample(time='1D').any()\n",
    "    \n",
    "    # Finally, we reindex these daily flags back onto the hourly time series by filling values\n",
    "    high_condition = daily_flags.reindex_like(ds['t'], method='ffill')\n",
    "    \n",
    "    # Now calculate stats like before\n",
    "    l_values,l_index = calculate_temp_prcp_stats('temp',high_condition,'high',l_values,l_index)\n",
    "    \n",
    "    # -- LOW PRECIPITATION\n",
    "    variable = 'mtpr'\n",
    "    # We'll stick with the original CAMELS definition of low precipitation: < 1 mm day-1\n",
    "    # It may not make too much sense to look at \"dry hours\" so we'll do this analysis at daily step\n",
    "    low_threshold = 1 # [mm d-1]\n",
    "    # Create daily precipitation sum (divided by density, times mm m-1 cancels out)\n",
    "    # [kg m-2 s-1] * [s h-1] / [kg m-3] * [mm m-1] = [mm h-1]\n",
    "    low_condition = (ds[variable] * seconds_per_hour).resample(time='1D').sum() < low_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('prec',low_condition,'low',l_values,l_index,\n",
    "                                             units='days') # this 'units' argument prevents conversion to days inside the functiom\n",
    "    \n",
    "    # -- HIGH PRECIPITATION\n",
    "    # CAMELS: > 5 times mean daily precip\n",
    "    high_threshold = 5 * (ds[variable] * seconds_per_hour).resample(time='1D').sum().mean()\n",
    "    high_condition = (ds[variable] * seconds_per_hour).resample(time='1D').sum() >= high_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('prec',high_condition,'high',l_values,l_index,\n",
    "                                                 units='days')\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3686d712-b932-4cb7-b153-0bfaab2feb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_worldclim(geo_folder, dataset, shp_str, l_values, index):\n",
    "\n",
    "    '''Calculates mean and stdv for tifs of monthly WorldClim values'''\n",
    "\n",
    "    # Define file locations\n",
    "    # Units source: https://www.worldclim.org/data/worldclim21.html\n",
    "    clim_folder = geo_folder / dataset / 'raw'\n",
    "    sub_folders =      ['prec', 'srad',         'tavg', 'tmax', 'tmin', 'vapr', 'wind',   'pet']\n",
    "    sub_folder_units = ['mm',   'kJ m^-2 d^-1', 'C',    'C',    'C',    'kPa',  'm s^-1', 'mm']\n",
    "\n",
    "    # Loop over the files and calculate the stats\n",
    "    for sub_folder, sub_folder_unit in zip(sub_folders, sub_folder_units):\n",
    "        month_files = sorted( glob.glob(str(clim_folder / sub_folder / '*.tif')) )\n",
    "        for month_file in month_files:\n",
    "            month_file = clim_folder / sub_folder / month_file # Construct the full path, because listdir() gives only files\n",
    "            stats = ['mean', 'std']\n",
    "            zonal_out = zonal_stats(shp_str, month_file, stats=stats)\n",
    "\n",
    "            scale, offset = csa.read_scale_and_offset(month_file)\n",
    "            scale, offset = read_scale_and_offset(month_file)\n",
    "            if sub_folder == 'srad':\n",
    "                zonal_out = zonal_stats_unit_conversion(zonal_out,stats,'srad', scale, offset)\n",
    "            l_values = csa.update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "            \n",
    "            month = os.path.basename(month_file).split('_')[3].split('.')[0]\n",
    "            var = os.path.basename(month_file).split('_')[2]\n",
    "            source = 'WorldClim'\n",
    "            if var == 'pet': source = 'WorldClim (derived, Oudin et al., 2005)'\n",
    "            index += [('Climate', f'{var}_mean_month_{month}', f'{sub_folder_unit}',  source),\n",
    "                      ('Climate', f'{var}_stdev_month_{month}', f'{sub_folder_unit}', source)]\n",
    "\n",
    "    return l_values, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1af9044e-e75b-48a7-815a-c8869f6212b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_forest_height(geo_folder, dataset, shp_str, l_values):\n",
    "\n",
    "    '''Calculates mean, min, max and stdv for forest height 2000 and 2020 tifs'''\n",
    "\n",
    "    # Year 2000 min, mean, max, stdev\n",
    "    tif = str( geo_folder / dataset / 'raw' / 'forest_height_2000.tif' )\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    zonal_out = zonal_stats(shp, tif, stats=stats)\n",
    "    scale,offset = read_scale_and_offset(tif)\n",
    "    l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "\n",
    "    # Year 2020 mean, stdev\n",
    "    tif = geo_folder / dataset / 'raw' / 'forest_height_2020.tif'\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    zonal_out = zonal_stats(shp, tif, stats=stats)\n",
    "    scale,offset = read_scale_and_offset(tif)\n",
    "    l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "\n",
    "    return l_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b4204-865e-45ad-85f5-aff233536d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_lai(geo_folder, dataset, temp_path, shp_str, l_values):\n",
    "\n",
    "    '''Calculates mean and stdv for tifs of monthly LAI values'''\n",
    "\n",
    "    # Calculate monthly mean maps (e.g. mean Jan, Feb, etc.)\n",
    "    lai_folder = geo_folder / dataset / 'raw' \n",
    "    lai_files = sorted( glob.glob(str(lai_folder / '*.tif')) ) # Find LAI files\n",
    "    month_files = calculate_monthly_lai_maps(lai_files, temp_path) # Create 12 monthly maps\n",
    "\n",
    "    # Monthly mean, stdev LAI; monthly mean, stdev GVF\n",
    "    for month_file in month_files:\n",
    "        stats = ['mean', 'std']\n",
    "        zonal_out = zonal_stats(str(shp_lump_path), month_file, stats=stats)\n",
    "        scale, offset = read_scale_and_offset(month_file)\n",
    "        scale,offset = read_scale_and_offset(month_file)\n",
    "        l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "    \n",
    "    # Clear temp folder\n",
    "    files_to_remove = os.listdir(temp_path)\n",
    "    for file_to_remove in files_to_remove:\n",
    "        file_remove_path = os.path.join(temp_path, file_to_remove)\n",
    "        if os.path.isfile(file_remove_path):\n",
    "            os.remove(file_remove_path)\n",
    "    \n",
    "    return l_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d63472-1778-4ab1-9eee-11c597265944",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ecf09c0-e431-4dd9-8e93-522ddbc9b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import glob\n",
    "import numpy as np\n",
    "from osgeo import gdal, osr\n",
    "import os\n",
    "import pandas as pd\n",
    "from rasterstats import zonal_stats\n",
    "from scipy.stats import circmean, circstd, skew, kurtosis\n",
    "from scipy.optimize import curve_fit\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4152e7a-4f4d-4d73-9f45-5b82d7ff4654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_annual_worldclim_aridity(annual_prec, annual_pet, output_folder, output_name):\n",
    "    '''Creates an annual aridity map from annual P and PET maps'''\n",
    "\n",
    "    # Load the data and mask the no-data values\n",
    "    p_data = csa.get_geotif_data_as_array(annual_prec)\n",
    "    no_data = csa.get_geotif_nodata_value(annual_prec)\n",
    "    masked_p = np.ma.masked_array(p_data, mask=p_data==no_data)\n",
    "    \n",
    "    pet_data = csa.get_geotif_data_as_array(annual_pet)\n",
    "    no_data = csa.get_geotif_nodata_value(annual_pet)\n",
    "    masked_pet = np.ma.masked_array(pet_data, mask=pet_data==no_data)\n",
    "\n",
    "    # Build in a failsafe for zero P\n",
    "    masked_p = np.where(masked_p == 0, 1, masked_p)\n",
    "    \n",
    "    # Calculate aridity\n",
    "    ari = np.ma.divide(masked_pet,masked_p)\n",
    "\n",
    "    # Apply no-data value and write to file\n",
    "    ari = ari.filled(no_data)\n",
    "    output_path = str(output_folder/output_name)\n",
    "    csa.write_geotif_sameDomain(annual_prec, output_path, ari, nodata_value=no_data)\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b0c5e-b77d-4cc7-b276-8388d193d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annual_worldclim_map(files, output_folder, output_name, var):\n",
    "    '''Creates an annual map from monthly WorldClim data'''\n",
    "\n",
    "    # Load the data and mask the no-data values\n",
    "    datasets = [csa.get_geotif_data_as_array(file) for file in files]\n",
    "    no_datas = [csa.get_geotif_nodata_value(file) for file in files]\n",
    "    masked_data = [np.ma.masked_array(data, mask=data==no_data) for data,no_data in zip(datasets,no_datas)]\n",
    "\n",
    "    # Create stacked array for computations along the third dimension\n",
    "    stacked_array = np.ma.dstack(masked_data)\n",
    "\n",
    "    # Create the annual value depending on what we're after\n",
    "    if var in ['prec','pet','snow']:\n",
    "        res_array = np.ma.sum(stacked_array, axis=2) # Sum along the third dimension\n",
    "    if var in ['tavg']:\n",
    "        res_array = np.ma.mean(stacked_array, axis=2) # Sum along the third dimension\n",
    "\n",
    "    # Apply no-data value and write to file\n",
    "    res_array = res_array.filled(no_datas[0])\n",
    "    output_path = str(output_folder/output_name)\n",
    "    csa.write_geotif_sameDomain(files[0], output_path, res_array, nodata_value=no_datas[0])\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f3a6d-8a1b-44c8-8a36-7335e1e152e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annual_worldclim_attributes(clim_folder, shp_str, prec_folder, tavg_folder, pet_folder, snow_folder):\n",
    "\n",
    "    '''Calculates annual WorldClim statistics'''\n",
    "\n",
    "    # Create the output folder\n",
    "    ann_folder = clim_folder / 'annual'\n",
    "    ann_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # General settings\n",
    "    stats = ['mean', 'std']\n",
    "    \n",
    "    # --- P\n",
    "    prec_files = sorted( glob.glob(str(clim_folder / prec_folder / '*.tif')) )\n",
    "    annual_prec = create_annual_worldclim_map(prec_files, ann_folder, 'prec_sum.tif', 'prec')\n",
    "    zonal_out = zonal_stats(shp_str, annual_prec, stats=stats)\n",
    "    for stat in stats:\n",
    "        l_values.append(zonal_out[0][stat])\n",
    "        l_index.append(('Climate',f'prec_{stat}','mm', 'WorldClim'))\n",
    "\n",
    "    # --- PET\n",
    "    pet_files = sorted( glob.glob(str(clim_folder / pet_folder / '*.tif')) )\n",
    "    annual_pet = create_annual_worldclim_map(pet_files, ann_folder, 'pet_sum.tif', 'pet')\n",
    "    zonal_out = zonal_stats(shp_str, annual_pet, stats=stats)\n",
    "    for stat in stats:\n",
    "        l_values.append(zonal_out[0][stat])\n",
    "        l_index.append(('Climate',f'pet_{stat}','mm', 'WorldClim'))\n",
    "        \n",
    "    # --- T\n",
    "    tavg_files = sorted( glob.glob(str(clim_folder / tavg_folder / '*.tif')) )\n",
    "    annual_tavg = create_annual_worldclim_map(tavg_files, ann_folder, 't_avg.tif', 'tavg')\n",
    "    zonal_out = zonal_stats(shp_str, annual_tavg, stats=stats)\n",
    "    for stat in stats:\n",
    "        l_values.append(zonal_out[0][stat])\n",
    "        l_index.append(('Climate',f'tavg_{stat}','C', 'WorldClim'))\n",
    "\n",
    "    # --- Snow\n",
    "    snow_files = sorted( glob.glob(str(clim_folder / snow_folder / '*.tif')) )\n",
    "    annual_snow = create_annual_worldclim_map(snow_files, ann_folder, 'snow_sum.tif', 'snow')\n",
    "\n",
    "    # --- Aridity\n",
    "    annual_ari = derive_annual_worldclim_aridity(annual_prec, annual_pet, ann_folder, 'aridity.tif')\n",
    "    zonal_out = zonal_stats(shp_str, annual_ari, stats=stats)\n",
    "    for stat in stats:\n",
    "        l_values.append(zonal_out[0][stat])\n",
    "        l_index.append(('Climate',f'aridity2_{stat}','-', 'WorldClim'))\n",
    "\n",
    "    # --- Seasonality\n",
    "    annual_seas = derive_annual_worldclim_seasonality(prec_files, tavg_files, ann_folder, 'seasonality.tif')\n",
    "    zonal_out = zonal_stats(shp_str, annual_seas, stats=stats)\n",
    "    for stat in stats:\n",
    "        l_values.append(zonal_out[0][stat])\n",
    "        l_index.append(('Climate',f'seasonality2_{stat}','-', 'WorldClim'))\n",
    "    \n",
    "    # --- Snow fraction\n",
    "    annual_fs = derive_annual_worldclim_fracsnow(annual_prec, annual_snow, ann_folder, 'fracsnow.tif')\n",
    "    zonal_out = zonal_stats(shp_str, annual_fs, stats=stats)\n",
    "    for stat in stats:\n",
    "        l_values.append(zonal_out[0][stat])\n",
    "        l_index.append(('Climate',f'fracsnow2_{stat}','-', 'WorldClim'))\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7212173-0108-4c3b-bdc3-390e11be7af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_annual_worldclim_seasonality(prec_files, tavg_files, output_folder, output_name):\n",
    "\n",
    "    '''Calculates a map of seasonality values using WorldClim precipitation and temperature'''\n",
    "\n",
    "    # Get the precip files\n",
    "    datasets = [csa.get_geotif_data_as_array(file) for file in prec_files]\n",
    "    no_datas = [csa.get_geotif_nodata_value(file) for file in prec_files]\n",
    "    masked_data = [np.ma.masked_array(data, mask=data==no_data) for data,no_data in zip(datasets,no_datas)]\n",
    "    stacked_prec = np.ma.dstack(masked_data)\n",
    "\n",
    "    # Get the tavg files\n",
    "    datasets = [csa.get_geotif_data_as_array(file) for file in tavg_files]\n",
    "    no_datas = [csa.get_geotif_nodata_value(file) for file in tavg_files]\n",
    "    masked_data = [np.ma.masked_array(data, mask=data==no_data) for data,no_data in zip(datasets,no_datas)]\n",
    "    stacked_tavg = np.ma.dstack(masked_data)\n",
    "\n",
    "    # Fit the sine curves in the third dimension\n",
    "    t_pars = np.apply_along_axis(fit_temp_sines, axis=2, arr=stacked_tavg)\n",
    "    p_pars = np.apply_along_axis(fit_prec_sines, axis=2, arr=stacked_prec)\n",
    "\n",
    "    # Calculate the dimensionless seasonality index in space as per Eq. 14 in Woods (2009)\n",
    "    seasonality = p_pars[:,:,1]*np.sign(t_pars[:,:,1])*np.cos(2*np.pi*(p_pars[:,:,2]-t_pars[:,:,2])/1)\n",
    "\n",
    "    # Apply no-data value and write to file\n",
    "    output_path = str(output_folder/output_name)\n",
    "    csa.write_geotif_sameDomain(prec_files[0], output_path, seasonality, nodata_value=np.nan)\n",
    "     \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851e647c-ffe5-4e60-8d23-f1b93d59e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_temp_sines(t):\n",
    "\n",
    "    # Short-circuit the computation if we have only masked values\n",
    "    if np.ma.getmaskarray(t).all():\n",
    "        return [np.nan, np.nan, np.nan]\n",
    "    \n",
    "    # Define the x-coordinate as daily steps as fractions of a year\n",
    "    x  = np.arange(0,len(t))/len(t) # days as fraction of a year\n",
    "    \n",
    "    # Fit the temperature sine\n",
    "    t_mean   = float(t.mean())\n",
    "    t_delta  = float(t.max()-t.min())\n",
    "    t_phase  = 0.5\n",
    "    initial_guess = [t_mean, t_delta, t_phase]\n",
    "    t_pars, _ = curve_fit(sine_function_temp, x, t, p0=initial_guess)\n",
    "\n",
    "    return t_pars\n",
    "\n",
    "def fit_prec_sines(p):\n",
    "\n",
    "    # Short-circuit the computation if we have only masked values\n",
    "    if np.ma.getmaskarray(p).all():\n",
    "        return [np.nan, np.nan, np.nan]\n",
    "    \n",
    "    # Define the x-coordinate as daily steps as fractions of a year\n",
    "    x  = np.arange(0,len(p))/len(p) # days as fraction of a year\n",
    "\n",
    "    # Fit the precipitation sine\n",
    "    p_mean   = float(p.mean())\n",
    "    p_delta  = float(p.max()-p.min()) / float(p.mean())\n",
    "    p_phase  = 0.5\n",
    "    initial_guess = [p_mean, p_delta, p_phase]\n",
    "    p_pars, _ = curve_fit(sine_function_prec, x, p, p0=initial_guess)\n",
    "\n",
    "    return p_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9878c4-5272-4335-89ea-bc91aece934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sine functions for P and T as per Woods (2009), Eq. 2 and 3. Period = 1 year\n",
    "def sine_function_temp(x, mean, delta, phase, period=1):\n",
    "    result = mean + delta * np.sin(2*np.pi*(x-phase)/period)\n",
    "    return result\n",
    "def sine_function_prec(x, mean, delta, phase, period=1):\n",
    "    result = mean * (1+ delta* np.sin(2*np.pi*(x-phase)/period))\n",
    "    results = np.where(result < 0, 0, result)\n",
    "    return result\n",
    "\n",
    "def fit_climate_sines(t,p):\n",
    "\n",
    "    # Define the x-coordinate as daily steps as fractions of a year\n",
    "    x  = np.arange(0,len(p))/len(p) # days as fraction of a year\n",
    "    \n",
    "    # Fit the temperature sine\n",
    "    t_mean   = float(daily_t.mean())\n",
    "    t_delta  = float(daily_t.max()-daily_t.min())\n",
    "    t_phase  = 0.5\n",
    "    initial_guess = [t_mean, t_delta, t_phase]\n",
    "    t_pars, _ = curve_fit(sine_function_temp, x, t, p0=initial_guess)\n",
    "\n",
    "    # Fit the precipitation sine\n",
    "    p_mean   = float(daily_p.mean())\n",
    "    p_delta  = float(daily_p.max()-daily_p.min()) / float(daily_p.mean())\n",
    "    p_phase  = 0.5\n",
    "    initial_guess = [p_mean, p_delta, p_phase]\n",
    "    p_pars, _ = curve_fit(sine_function_prec, x, p, p0=initial_guess)\n",
    "\n",
    "    # Compute seasonality as per Eq. 14 in Woods (2009)\n",
    "    return p_pars[1]*np.sign(t_pars[1])*np.cos(2*np.pi*(p_pars[2]-t_pars[2])/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d401b9-a453-4186-9f1e-5d72f427fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_annual_worldclim_fracsnow(annual_prec, annual_snow, output_folder, output_name):\n",
    "    '''Creates an annual aridity map from annual P and snow maps'''\n",
    "\n",
    "    # Load the data and mask the no-data values\n",
    "    p_data = csa.get_geotif_data_as_array(annual_prec)\n",
    "    no_data = csa.get_geotif_nodata_value(annual_prec)\n",
    "    masked_p = np.ma.masked_array(p_data, mask=p_data==no_data)\n",
    "    \n",
    "    snow_data = csa.get_geotif_data_as_array(annual_snow)\n",
    "    no_data = csa.get_geotif_nodata_value(annual_snow)\n",
    "    masked_snow = np.ma.masked_array(snow_data, mask=snow_data==no_data)\n",
    "\n",
    "    # Build in a failsafe for zero P\n",
    "    masked_p = np.where(masked_p == 0, 1, masked_p)\n",
    "    \n",
    "    # Calculate aridity\n",
    "    fsnow = np.ma.divide(masked_snow,masked_p)\n",
    "\n",
    "    # Apply no-data value and write to file\n",
    "    fsnow = fsnow.filled(no_data)\n",
    "    output_path = str(output_folder/output_name)\n",
    "    csa.write_geotif_sameDomain(annual_prec, output_path, fsnow, nodata_value=no_data)\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea402a-5821-4955-816b-c19b4621eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_climate_seasonality_era5(ds, use_typical_cycle=False):\n",
    "\n",
    "    if not use_typical_cycle:\n",
    "        # Resample the observations to daily, retain individual years\n",
    "        daily_p_groups = ds['mtpr'].resample(time='1D').mean().groupby('time.year')\n",
    "        daily_t_groups = ds['t'].resample(time='1D').mean().groupby('time.year')\n",
    "\n",
    "        seasonalities = []\n",
    "        for zip_p, zip_t in zip(daily_p_groups,daily_t_groups):\n",
    "            year = zip_p[0]\n",
    "            yp = zip_p[1].values.flatten()\n",
    "            yt = zip_t[1].values.flatten()\n",
    "            seasonalities.append(fit_climate_sines(yt,yp))\n",
    "        return np.array(seasonalities)\n",
    "    \n",
    "    else:\n",
    "        # Resample to typical seasonal cycles at daily resolution\n",
    "        daily_p = ds['mtpr'].resample(time='1D').mean().groupby('time.dayofyear').mean()\n",
    "        daily_t = ds['t'].resample(time='1D').mean().groupby('time.dayofyear').mean()\n",
    "\n",
    "        # Time series of input and output\n",
    "        yp = daily_p.values.flatten()\n",
    "        yt = daily_t.values.flatten()\n",
    "\n",
    "        # Fit the sine curves and return the seasonality coefficient\n",
    "        return fit_climate_sines(yt,yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd224c0-96a1-4916-8393-61e0450addaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_signatures(hyd, pre, source, l_values, l_index):\n",
    "    '''Calculates various signatures'''\n",
    "\n",
    "    ## LONG-TERM STATISTICS\n",
    "    # Mean daily discharge\n",
    "    daily_mean_q = hyd['q_obs'].groupby(hyd['water_year']).mean() # .mean() of daily values gives us [mm d-1]\n",
    "    mean_q_m = daily_mean_q.mean()\n",
    "    mean_q_s = daily_mean_q.std()\n",
    "    l_values.append(float(mean_q_m.values))\n",
    "    l_index.append( ('Hydrology', 'daily_discharge_mean', 'mm d^-1', f'{source}') )\n",
    "    l_values.append(float(mean_q_s.values))\n",
    "    l_index.append( ('Hydrology', 'daily_discharge_std', 'mm d^-1', f'{source}') )\n",
    "\n",
    "    # Mean monthly flows\n",
    "    monthly_q = hyd['q_obs'].resample(time='1ME').mean().groupby('time.month')\n",
    "    monthly_m = monthly_q.mean()\n",
    "    monthly_s = monthly_q.std()\n",
    "    l_values, l_index = csa.process_monthly_means_to_lists(monthly_m, 'mean', l_values, l_index, 'daily_streamflow', 'mm day^-1', 'Hydrology', source)\n",
    "    l_values, l_index = csa.process_monthly_means_to_lists(monthly_s, 'std',  l_values, l_index, 'daily_streamflow', 'mm day^-1', 'Hydrology', source)\n",
    "    \n",
    "    # Runoff ratio\n",
    "    daily_mean_p = pre.groupby(hyd['water_year']).mean()\n",
    "    yearly_rr = daily_mean_q/daily_mean_p\n",
    "    rr_m = yearly_rr.mean()\n",
    "    rr_s = yearly_rr.std()\n",
    "    l_values.append(float(rr_m.values))\n",
    "    l_index.append( ('Hydrology', 'runoff_ratio_mean', 'mm d^-1 month-1', f'{source}, ERA5') )\n",
    "    l_values.append(float(rr_s.values))\n",
    "    l_index.append( ('Hydrology', 'runoff_ratio_std', 'mm d^-1 month-1', f'{source}, ERA5') )\n",
    "\n",
    "    # Streamflow elasticity\n",
    "    q_elas = np.median(((daily_mean_q - daily_mean_q.mean())/(daily_mean_p - daily_mean_p.mean()))*(daily_mean_p.mean()/daily_mean_q.mean()))\n",
    "    l_values.append(q_elas)\n",
    "    l_index.append( ('Hydrology', 'streamflow_elasticity', '-', f'{source}, ERA5') )\n",
    "\n",
    "    # Slope of FDC\n",
    "    groups = hyd['q_obs'].groupby(hyd['water_year'])\n",
    "    slopes = []\n",
    "    for year,group in groups:\n",
    "        flows = group.values.copy()\n",
    "        flows.sort()\n",
    "        flows = np.log(flows)\n",
    "        slope = (np.percentile(flows,66) - np.percentile(flows,33)) / (.66*len(flows) - .33*len(flows))\n",
    "        slopes.append(slope)\n",
    "    slopes = np.array(slopes)\n",
    "    slope_m = slopes.mean()\n",
    "    slope_s = slopes.std()\n",
    "    \n",
    "    l_values.append(slope_m)\n",
    "    l_index.append( ('Hydrology', 'fdc_slope_mean', '-', f'{source}') )\n",
    "    l_values.append(slope_s)\n",
    "    l_index.append( ('Hydrology', 'fdc_slope_std', '-', f'{source}') )\n",
    "\n",
    "    # Baseflow index\n",
    "    rec,_ = baseflow.separation(hyd['q_obs'].values, method='Eckhardt') # Find baseflow with Eckhardt filter\n",
    "    tmp = xr.DataArray(rec['Eckhardt'], dims='time', coords={'time': hyd['time']}) # Prepare a new variable to put in 'hyd'\n",
    "    hyd['q_bas'] = tmp\n",
    "    daily_mean_qbase = hyd['q_bas'].groupby(hyd['water_year']).mean()\n",
    "    bfi_m = (daily_mean_qbase / daily_mean_q).mean()\n",
    "    bfi_s = (daily_mean_qbase / daily_mean_q).std()\n",
    "    l_values.append(float(bfi_m.values))\n",
    "    l_index.append( ('Hydrology', 'bfi_mean', '-', f'{source}') )\n",
    "    l_values.append(float(bfi_s.values))\n",
    "    l_index.append( ('Hydrology', 'bfi_std', '-', f'{source}') )\n",
    "\n",
    "    # Half-flow date\n",
    "    tmp_cum_flow = hyd['q_obs'].groupby(hyd['water_year']).cumsum() # Cumulative flow per water year\n",
    "    tmp_sum_flow = hyd['q_obs'].groupby(hyd['water_year']).sum() # Total flow per water year\n",
    "    tmp_frc_flow = tmp_cum_flow.groupby(hyd['water_year']) / tmp_sum_flow # Fractional flow per water year\n",
    "    groups = tmp_frc_flow.groupby(hyd['water_year'])\n",
    "    dates = []\n",
    "    for year,group in groups:\n",
    "        hdf = group[group > 0.5][0]['time'].values\n",
    "        dates.append(pd.Timestamp(hdf).dayofyear)\n",
    "    dates = np.array(dates)\n",
    "    hdf_m = circmean(dates, high=366)\n",
    "    hdf_s = circstd(dates, high=366)\n",
    "    l_values.append(hdf_m)\n",
    "    l_index.append( ('Hydrology', 'hfd_mean', 'day of year', f'{source}') )\n",
    "    l_values.append(hdf_s)\n",
    "    l_index.append( ('Hydrology', 'hfd_std', 'day of year', f'{source}') )\n",
    "\n",
    "    # Quantiles\n",
    "    groups = hyd['q_obs'].groupby(hyd['water_year'])\n",
    "    for quantile in [0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]:\n",
    "        q_m = groups.quantile(quantile).mean()\n",
    "        q_s = groups.quantile(quantile).std()\n",
    "        l_values.append(float(q_m.values))\n",
    "        l_index.append(('Hydrology', f'q{int(quantile*100)}_mean', 'mm day^-1', f'{source}'))\n",
    "        l_values.append(float(q_s.values))\n",
    "        l_index.append(('Hydrology', f'q{int(quantile*100)}_std', 'mm day^-1', f'{source}'))\n",
    "\n",
    "    # Durations\n",
    "    variable  = 'q_obs'\n",
    "    no_flow_threshold = 0 # mm d-1\n",
    "    no_flow_condition = hyd[variable] < no_flow_threshold\n",
    "    l_values,l_index  = calculate_flow_period_stats('flow',no_flow_condition,'no',l_values,l_index,dataset='source',units='days',category='Hydrology')\n",
    "    \n",
    "    low_flow_threshold = 0.2 * hyd['q_obs'].mean() # mm d-1\n",
    "    low_flow_condition = hyd[variable] < low_flow_threshold\n",
    "    l_values,l_index   = calculate_flow_period_stats('flow',low_flow_condition,'low',l_values,l_index,dataset='source',units='days',category='Hydrology')\n",
    "    \n",
    "    high_flow_threshold = 9 * hyd['q_obs'].median() # mm d-1\n",
    "    high_flow_condition = hyd[variable] > high_flow_threshold\n",
    "    l_values,l_index    = calculate_flow_period_stats('flow',high_flow_condition,'high',l_values,l_index,dataset='source',units='days',category='Hydrology')\n",
    "\n",
    "    return l_values,l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa077d-a9b8-46ca-836b-4c1a93491c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_flow_period_stats(var, condition, hilo, l_values, l_index,\n",
    "                                dataset='ERA5', units='hours', category='Climate'):\n",
    "    \n",
    "    '''Calculates frequency (mean), duration (mean, median, skew, kurtosis) and\n",
    "        timing of periods identified with a certain condition'''\n",
    "\n",
    "    # Constants. We want everything in [days] for consistency with original CAMELS\n",
    "    hours_per_day = 24 # [hours day-1]\n",
    "    days_per_year = 365.25 # [days year-1]\n",
    "\n",
    "    # Calculate frequencies\n",
    "    freq = condition.mean(dim='time') * days_per_year # [-] * [days year-1]\n",
    "    l_values.append(float(freq.values))\n",
    "    l_index.append( (f'{category}', f'{hilo}_{var}_freq', 'days year^-1', dataset) )\n",
    "    \n",
    "    # Calculate duration statistics\n",
    "    durations = csa.find_durations(condition) # [time steps]\n",
    "    if len(durations) == 0:\n",
    "        dur_mean = 0\n",
    "        dur_med = 0\n",
    "        dur_skew = 0\n",
    "        dur_kur = 0\n",
    "    else:\n",
    "        dur_mean = np.mean(durations)\n",
    "        dur_med = np.median(durations)\n",
    "        dur_skew = skew(durations)\n",
    "        dur_kur = kurtosis(durations)\n",
    "    l_values.append(dur_mean) # [days]\n",
    "    l_index.append((f'{category}', f'{hilo}_{var}_dur_mean', 'days', dataset) ) # Consistency with\n",
    "    l_values.append(dur_med) # [days]\n",
    "    l_index.append((f'{category}', f'{hilo}_{var}_dur_median', 'days', dataset) )\n",
    "    l_values.append(dur_skew) # [-]\n",
    "    l_index.append((f'{category}', f'{hilo}_{var}_dur_skew', '-', dataset) )\n",
    "    l_values.append(dur_kur) # [-]\n",
    "    l_index.append((f'{category}', f'{hilo}_{var}_dur_kurtosis', '-', dataset) )\n",
    "\n",
    "    # Calculate timing statistic\n",
    "    condition['season'] = ('time', \n",
    "        [csa.get_season(month) for month in condition['time.month'].values]) # add seasons\n",
    "    season_groups = condition.groupby('season')\n",
    "    season_list   = list(season_groups.groups.keys())\n",
    "    max_season_id = int(season_groups.sum().argmax(dim='season').values) # find season with most True values\n",
    "    l_values.append(season_list[max_season_id]) # add season abbrev\n",
    "    l_index.append( (f'{category}', f'{hilo}_{var}_timing', 'season', dataset) )\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180ad99-60a7-43ee-aab4-78f7f1d6b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_open_water_stats(gdf, att, mask, l_values, l_index):\n",
    "    '''Calculates min, mean, max, std and total att ('Lake_area', 'Vol_total'), optionally using a reservoir mask'''\n",
    "\n",
    "    # Setup\n",
    "    if mask == 'all':\n",
    "        mask = gdf.index >= 0 # Selects everything if no mask is provided\n",
    "        type = 'open_water' # no reservoir mask, hence we're working with lakes\n",
    "    elif mask == 'reservoir':\n",
    "        mask = gdf['Lake_type'] == 2\n",
    "        type = 'reservoir'\n",
    "    if att == 'Lake_area':\n",
    "        units = 'km^2' # https://data.hydrosheds.org/file/technical-documentation/HydroLAKES_TechDoc_v10.pdf\n",
    "        att_d = 'area'\n",
    "    elif att == 'Vol_total':\n",
    "        units = 'million~m^3' # https://data.hydrosheds.org/file/technical-documentation/HydroLAKES_TechDoc_v10.pdf\n",
    "        att_d = 'volume'\n",
    "\n",
    "    # Stats\n",
    "    l_values.append(gdf[mask][att].min())\n",
    "    l_index.append(('Open water', f'{type}_{att_d}_min',  f'{units}', 'HydroLAKES'))\n",
    "    l_values.append(gdf[mask][att].mean())\n",
    "    l_index.append(('Open water', f'{type}_{att_d}_mean',  f'{units}', 'HydroLAKES'))\n",
    "    l_values.append(gdf[mask][att].max())\n",
    "    l_index.append(('Open water', f'{type}_{att_d}_max',  f'{units}', 'HydroLAKES'))\n",
    "    l_values.append(gdf[mask][att].std())\n",
    "    l_index.append(('Open water', f'{type}_{att_d}_std',  f'{units}', 'HydroLAKES'))\n",
    "    l_values.append(gdf[mask][att].sum()) # total\n",
    "    l_index.append(('Open water', f'{type}_{att_d}_total',  f'{units}', 'HydroLAKES'))\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c8d1a1-cf4d-4965-9569-ccca01c136d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_attributes(tif,l_values,l_index):\n",
    "    '''Calculates circular statistics for MERIT Hydro aspect'''\n",
    "\n",
    "    # Get data as a masked array - we know there are no-data values outside the catchment boundaries\n",
    "    aspect = csa.get_geotif_data_as_array(tif)\n",
    "    no_data = get_geotif_nodata_value(tif)\n",
    "    masked_aspect = np.ma.masked_array(aspect, aspect == no_data)\n",
    "\n",
    "    ## Calculate the statistics\n",
    "    l_values.append(masked_aspect.min())\n",
    "    l_index.append(('Topography', 'merit_hydro_aspect_min',  'degrees', 'MERIT Hydro'))\n",
    "    \n",
    "    l_values.append(circmean(masked_aspect,high=360))\n",
    "    l_index.append(('Topography', 'merit_hydro_aspect_mean',  'degrees', 'MERIT Hydro'))\n",
    "\n",
    "    l_values.append(circstd(masked_aspect,high=360))\n",
    "    l_index.append(('Topography', 'merit_hydro_aspect_std',  'degrees', 'MERIT Hydro'))\n",
    "\n",
    "    l_values.append(masked_aspect.max())\n",
    "    l_index.append(('Topography', 'merit_hydro_aspect_max',  'degrees', 'MERIT Hydro'))\n",
    "    \n",
    "    return l_values,l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e848e1-6858-4a5f-aebd-56a13af6d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_river_attributes(riv_str, l_values, l_index, area):\n",
    "    \n",
    "    '''Calculates topographic attributes from a MERIT Hydro Basins river polygon'''\n",
    "\n",
    "    # Load shapefiles\n",
    "    river = gpd.read_file(riv_str)\n",
    "    river = river.set_index('COMID')\n",
    "    \n",
    "    # Raw data\n",
    "    stream_lengths = []\n",
    "    headwaters = river[river['maxup'] == 0] # identify reaches with no upstream\n",
    "    for COMID in headwaters.index:\n",
    "        stream_length = 0\n",
    "        while COMID in river.index:\n",
    "            stream_length += river.loc[COMID]['lengthkm'] # Add the length of the current segment\n",
    "            COMID = river.loc[COMID]['NextDownID'] # Get the downstream reach\n",
    "        stream_lengths.append(stream_length) # If we get here we ran out of downstream IDs\n",
    "    \n",
    "    # Stats\n",
    "    stream_total = river['lengthkm'].sum()\n",
    "    stream_lengths = np.array(stream_lengths)\n",
    "    l_values.append(stream_lengths.min())\n",
    "    l_index.append(('Topography', 'stream_length_min',  'km', 'MERIT Hydro Basins'))\n",
    "    l_values.append(stream_lengths.mean())\n",
    "    l_index.append(('Topography', 'stream_length_mean',  'km', 'MERIT Hydro Basins'))\n",
    "    l_values.append(stream_lengths.max())\n",
    "    l_index.append(('Topography', 'stream_length_max',  'km', 'MERIT Hydro Basins'))\n",
    "    l_values.append(stream_lengths.std())\n",
    "    l_index.append(('Topography', 'stream_length_std',  'km', 'MERIT Hydro Basins'))\n",
    "    l_values.append(stream_total)\n",
    "    l_index.append(('Topography', 'stream_length_total',  'km', 'MERIT Hydro Basins'))\n",
    "    \n",
    "    # Order\n",
    "    l_values.append(river['order'].max())\n",
    "    l_index.append(('Topography', 'steam_order_max',  '-', 'MERIT Hydro Basins'))\n",
    "\n",
    "    # Derived\n",
    "    density = stream_total/area\n",
    "    elongation = 2*np.sqrt(area/np.pi)/stream_lengths.max()\n",
    "    l_values.append(density)\n",
    "    l_index.append(('Topography', 'stream_density',  'km^-1', 'Derived'))\n",
    "    l_values.append(elongation)\n",
    "    l_index.append(('Topography', 'elongation_ratio','-', 'Derived'))\n",
    "    \n",
    "    return l_values,l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4992b34-8064-4536-8be8-89bab8eac544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geotif_nodata_value(tif):\n",
    "    with rasterio.open(tif) as src:\n",
    "        nodata_value = src.nodata\n",
    "    return nodata_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e664b557-93a8-4a10-9151-03441bc91fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_scale_and_offset(tif):\n",
    "    scale,offset = read_scale_and_offset(tif) # just to check we don't have any scale/offset going on\n",
    "    if scale is None: scale = 1 # If scale is undefined that means we simply multiply by 1\n",
    "    if offset is None: offset = 0 # Undefined offset > add 0\n",
    "    if not (scale == 1) and not (offset == 0):\n",
    "        print(f'--- WARNING: check_scale_and_offset(): scale or offset not 1 or 0 respectively.')\n",
    "    return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060773f4-11fd-4e8c-ab53-344abec5d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_dict(source):\n",
    "    '''Contains dictionaries for categorical variables'''\n",
    "    \n",
    "    if source == 'GLCLU 2019':\n",
    "        cat_dict = {1: 'true_desert',\n",
    "                    2: 'semi_arid',\n",
    "                    3: 'dense_short_vegetation',\n",
    "                    4: 'open_tree_cover',\n",
    "                    5: 'dense_tree_cover',\n",
    "                    6: 'tree_cover_gain',\n",
    "                    7: 'tree_cover_loss',\n",
    "                    8: 'salt_pan',\n",
    "                    9: 'wetland_sparse_vegetation',\n",
    "                   10: 'wetland_dense_short_vegetation',\n",
    "                   11: 'wetland_open_tree_cover',\n",
    "                   12: 'wetland_dense_tree_cover',\n",
    "                   13: 'wetland_tree_cover_gain',\n",
    "                   14: 'wetland_tree_cover_loss',\n",
    "                   15: 'ice',\n",
    "                   16: 'water',\n",
    "                   17: 'cropland',\n",
    "                   18: 'built_up',\n",
    "                   19: 'ocean',\n",
    "                   20: 'no_data'}\n",
    "\n",
    "    if source == 'MCD12Q1.061':\n",
    "        cat_dict = {1: 'evergreen_needleleaf_forest',\n",
    "                    2: 'evergreen_broadleaf_forest',\n",
    "                    3: 'deciduous_needleleaf_forest',\n",
    "                    4: 'deciduous_broadleaf_forest',\n",
    "                    5: 'mixed_forest',\n",
    "                    6: 'closed_shrubland',\n",
    "                    7: 'open_shrubland',\n",
    "                    8: 'woody_savanna',\n",
    "                    9: 'savanna',\n",
    "                   10: 'grassland',\n",
    "                   11: 'permanent_wetland',\n",
    "                   12: 'cropland',\n",
    "                   13: 'urban_and_built_up',\n",
    "                   14: 'cropland_natural_mosaic',\n",
    "                   15: 'permanent_snow_ice',\n",
    "                   16: 'barren',\n",
    "                   17: 'water',\n",
    "                  255: 'unclassified'}\n",
    "\n",
    "    if source == 'LGRIP30':\n",
    "        cat_dict = {0: 'water',\n",
    "                    1: 'non_cropland',\n",
    "                    2: 'irrigated_cropland',\n",
    "                    3: 'rainfed_cropland'}\n",
    "    \n",
    "    return cat_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7597647-5d0c-475b-bc59-8e1882e5c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_values_list_with_categorical(l_values, l_index, zonal_out, source, prefix=''):\n",
    "    '''Maps a zonal histogram of categorical classes onto descriptions and adds to lists'''\n",
    "\n",
    "    # Get the category definitions\n",
    "    cat_dict = get_categorical_dict(source)    \n",
    "\n",
    "    # Find the total number of classified pixels\n",
    "    total_pixels = 0\n",
    "    for land_id,count in zonal_out[0].items():\n",
    "        total_pixels += count\n",
    "    \n",
    "    # Loop over all categories and see what we have in this catchment\n",
    "    for land_id,text in cat_dict.items():\n",
    "        land_prct = 0\n",
    "        if land_id in zonal_out[0].keys():\n",
    "            land_prct = zonal_out[0][land_id] / total_pixels\n",
    "        l_values.append(land_prct)\n",
    "        l_index.append(('Land cover', f'{prefix}{text}_fraction', '-', f'{source}'))\n",
    "\n",
    "    return l_values,l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681110fc-128c-4599-b82b-34d42c53dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aridity_and_fraction_snow_from_worldclim(geo_folder, dataset):\n",
    "    \n",
    "    '''Calculates aridity and fraction snow maps from WorldClim data'''\n",
    "\n",
    "    # Find files\n",
    "    clim_folder = geo_folder / dataset / 'raw'\n",
    "    prc_files = sorted( glob.glob(str(clim_folder / 'prec' / '*.tif')) ) # [mm]\n",
    "    pet_files = sorted( glob.glob(str(clim_folder / 'pet' / '*.tif')) ) # [mm]\n",
    "    tmp_files = sorted( glob.glob(str(clim_folder / 'tavg' / '*.tif')) ) # [C]\n",
    "    \n",
    "    # Make the output locations\n",
    "    ari_folder = clim_folder / 'aridity'\n",
    "    ari_folder.mkdir(parents=True, exist_ok=True)\n",
    "    snow_folder = clim_folder / 'fraction_snow'\n",
    "    snow_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Loop over files and calculate aridity\n",
    "    for prc_file, pet_file, tmp_file in zip(prc_files, pet_files, tmp_files):\n",
    "\n",
    "        # Define month\n",
    "        month = prc_file.split('_')[-1].split('.')[0] # 'wc2.1_30s_prec_01.tif' > '01', ..., '12'\n",
    "        month_ix = int(month)-1 # -1 to account for zero-based indexing: Jan value is at index 0, not 1\n",
    "\n",
    "        # Load data\n",
    "        prc_path = clim_folder / 'prec' / prc_file\n",
    "        pet_path = clim_folder / 'pet'  / pet_file\n",
    "        tmp_path = clim_folder / 'tavg' / tmp_file      \n",
    "        prc = get_geotif_data_as_array(prc_path) # [mm]\n",
    "        pet = get_geotif_data_as_array(pet_path) # [mm]\n",
    "        tmp = get_geotif_data_as_array(tmp_path) # [C]\n",
    "\n",
    "        # Calculate variables\n",
    "        snow = np.where(tmp < 0, prc, 0) # get snow first, because this needs precip and we'll (possibly) be updating the precip value below\n",
    "        if (prc == 0).any():\n",
    "            prc[prc == 0] = 1 # add 1 mm to avoid divide by zero errors\n",
    "        ari = pet/prc # [-]\n",
    "        frac_snow = snow/prc # [-]\n",
    "\n",
    "        # Define output file name and write to disk\n",
    "        ari_name = prc_file.replace('prec','aridity')\n",
    "        ari_file = str(ari_folder / ari_name)\n",
    "        write_geotif_sameDomain(prc_path, ari_file, ari)\n",
    "        snow_name = prc_file.replace('prec','fraction_snow')\n",
    "        snow_file = str(snow_folder / snow_name)\n",
    "        write_geotif_sameDomain(prc_path, snow_file, frac_snow)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22da84d-7099-4744-9b09-1ec1dcf270f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonal_stats_unit_conversion(zonal_out, stat_to_convert, variable, scale, offset):\n",
    "    '''Takes a zonal_stats output and converts the units of any variable listed in stat_to_convert'''\n",
    "\n",
    "    # Constants\n",
    "    j_per_kj = 1000 # [J kJ-1]\n",
    "    seconds_per_day = 24*60*60 # [s day-1]\n",
    "\n",
    "    # Keep track of scale and offset\n",
    "    # Update scale and offset to usable values - we get None if scale and offset are 1 and 0 in the GeoTIFF\n",
    "    if scale is None: scale = 1 # If scale is undefined that means we simply multiply by 1\n",
    "    if offset is None: offset = 0 # Undefined offset > add 0\n",
    "\n",
    "    #  We'll need code to handle this if these aren't 1 and 0 respectively\n",
    "    if (scale != 1) or (offset !=0):\n",
    "        print(f'--- ERROR: zonal_stats_unit_conversion(): code needed to deal with scale {scale} and offset {offset}')\n",
    "        return -1\n",
    "\n",
    "    # Select conversion factor\n",
    "    if variable == 'srad':\n",
    "        # From [kJ m-2 day-1] to [W m-2]:\n",
    "        # [kJ m-2 day-1] * 1/[s day-1] * [J kJ-1] = [J m-2 s-1] = [W m-2]\n",
    "        c_factor = 1/seconds_per_day * j_per_kj\n",
    "\n",
    "    # loop over all list elements\n",
    "    for list_id in range(0,len(zonal_out)):\n",
    "        zonal_dict = zonal_out[list_id]\n",
    "\n",
    "        # Loop over dictionary entries\n",
    "        for key,val in zonal_dict.items():\n",
    "            if k in stat_to_convert:\n",
    "                zonal_out[list_id][key] = zonal_out[list_id][key] * c_factor\n",
    "\n",
    "    return zonal_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9b9c7de4-1e98-4626-a041-241145c73a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oudin_pet_from_worldclim(geo_folder, dataset, debug=False):\n",
    "\n",
    "    '''Calculates PET estimates from WorldClim data, using the Oudin (2005; 10.1016/j.jhydrol.2004.08.026) formulation'''\n",
    "\n",
    "    # Constants\n",
    "    lh = 2.45 # latent heat flux, MJ kg-1\n",
    "    rw = 1000 # rho water, kg m-3\n",
    "    days_per_month = [31, 28.25, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31] # days month-1\n",
    "    mm_per_m = 1000 # mm m-1\n",
    "    \n",
    "    # Find files\n",
    "    clim_folder = geo_folder / dataset / 'raw'\n",
    "    srad_files = sorted( glob.glob(str(clim_folder / 'srad' / '*.tif')) ) # website says [kJ m-2 day-1], but paper says [MJ m-2 day-1]\n",
    "    tavg_files = sorted( glob.glob(str(clim_folder / 'tavg' / '*.tif')) ) # C\n",
    "\n",
    "    # Make the output location\n",
    "    pet_folder = clim_folder / 'pet'\n",
    "    pet_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Loop over files and calculate PET\n",
    "    for srad_file, tavg_file in zip(srad_files, tavg_files):\n",
    "\n",
    "        # Define month\n",
    "        month = srad_file.split('_')[-1].split('.')[0] # 'wc2.1_30s_srad_01.tif' > '01', ..., '12'\n",
    "        month_ix = int(month)-1 # -1 to account for zero-based indexing: Jan value is at index 0, not 1\n",
    "        \n",
    "        # Load data\n",
    "        srad_path = clim_folder / 'srad' / srad_file\n",
    "        tavg_path = clim_folder / 'tavg' / tavg_file      \n",
    "        srad = get_geotif_data_as_array(srad_path) / 1000 # [kJ m-2 day-1] / 1000 = [MJ m-2 day-1]\n",
    "        tavg = get_geotif_data_as_array(tavg_path)\n",
    "        \n",
    "        # Oudin et al, 2005, Eq. 3\n",
    "        pet = np.where(tavg+5 > 0, (srad / (lh*rw)) * ((tavg+5)/100) * mm_per_m, 0) # mm day-1\n",
    "        pet_month = pet * days_per_month[month_ix] # mm month-1\n",
    "        if debug: print(f'Calculating monthly PET for month {month} at day-index {month_ix}')\n",
    "\n",
    "        # Define output file name and write to disk\n",
    "        pet_name = srad_file.replace('srad','pet')\n",
    "        pet_file = str(pet_folder / pet_name)\n",
    "        write_geotif_sameDomain(srad_path, pet_file, pet_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e7499d25-ef22-40e4-a506-6bcc351348f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_values_list(l_values, stats, zonal_out, scale, offset):\n",
    "\n",
    "    # Update scale and offset to usable values\n",
    "    if scale is None: scale = 1 # If scale is undefined that means we simply multiply by 1\n",
    "    if offset is None: offset = 0 # Undefined offset > add 0\n",
    "\n",
    "    # We loop through the calculated stats in a pre-determined order:\n",
    "    # 1. min\n",
    "    # 2. mean\n",
    "    # 3. max\n",
    "    # 4. stdev\n",
    "    # 5. ..\n",
    "    if 'min' in stats:  l_values.append(zonal_out[0]['min']  * scale + offset)\n",
    "    if 'mean' in stats: l_values.append(zonal_out[0]['mean'] * scale + offset)\n",
    "    if 'max' in stats:  l_values.append(zonal_out[0]['max']  * scale + offset)\n",
    "    if 'std' in stats:  l_values.append(zonal_out[0]['std']  * scale + offset)\n",
    "\n",
    "    return l_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "382324e2-789c-4e3a-ac08-18d777e689c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_scale_and_offset(geotiff_path):\n",
    "    # Open the GeoTIFF file\n",
    "    dataset = gdal.Open(geotiff_path)\n",
    "\n",
    "    if dataset is None:\n",
    "        raise FileNotFoundError(f\"File not found: {geotiff_path}\")\n",
    "\n",
    "    # Get the scale and offset values\n",
    "    scale = dataset.GetRasterBand(1).GetScale()\n",
    "    offset = dataset.GetRasterBand(1).GetOffset()\n",
    "\n",
    "    # Close the dataset\n",
    "    dataset = None\n",
    "\n",
    "    return scale, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ff0619b3-e43b-418d-adab-2e946c3f08d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lai_files_by_date(files, last_n_years=[], last_n_months=[], last_n_days=[],\n",
    "                                    years=[], months=[], days=[]):\n",
    "\n",
    "    '''Filters list of LAI file names by last n years/months/days and/or by year/month/day x.\n",
    "       Assumes date is given as 'yyyymmdd_*.tif', as part of the filename.\n",
    "       Use years/months/days (input as list) to subset further.'''\n",
    "\n",
    "    # Check inputs\n",
    "    if (last_n_years and last_n_months) or \\\n",
    "       (last_n_years and last_n_days) or \\\n",
    "       (last_n_months and last_n_days):\n",
    "        print('WARNING: filter_lai_files_by_date(): specify only one of last_n_years, last_n_months, last_n_days')\n",
    "        return\n",
    "\n",
    "    # Create a DatetimeIndex from filenames\n",
    "    dates = []\n",
    "    for file in files:\n",
    "        file_name = os.path.basename(file)\n",
    "        yyyymmdd = file_name[0:8]\n",
    "        dates.append(yyyymmdd)\n",
    "    dti = pd.to_datetime(dates,format='%Y%m%d')\n",
    "\n",
    "    # Set the first entry\n",
    "    start_date = dti[0]\n",
    "    \n",
    "    # Find the last entry\n",
    "    last_year  = dti[-1].year\n",
    "    last_month = dti[-1].month\n",
    "    last_day   = dti[-1].day\n",
    "    \n",
    "    # Select the last n entries\n",
    "    if last_n_years:    start_date = dti[-1] - relativedelta(years = last_n_years)\n",
    "    elif last_n_months: start_date = dti[-1] - relativedelta(months = last_n_months)\n",
    "    elif last_n_days:   start_date = dti[-1] - relativedelta(days = last_n_days)\n",
    "    last_n = (dti >= start_date) & (dti <= dti[-1])\n",
    "\n",
    "    # Specify filters to include all if no specific years/months/days were requested\n",
    "    if not years:  years  = list(set(dti.year))  # i.e. filter to include all unique years in dti, \\\n",
    "    if not months: months = list(set(dti.month)) #    else use user input\n",
    "    if not days:   days   = list(set(dti.day))\n",
    "    mask = dti.year.isin(years) & dti.month.isin(months) & dti.day.isin(days)\n",
    "\n",
    "    # Return the filtered list\n",
    "    return [file for file, bool1, bool2 in zip(files,last_n,mask) if bool1 and bool2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e05e737-0a94-4c5d-9ac5-2fb1f06af17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geotif_data_as_array(file, band=1):\n",
    "    ds = gdal.Open(file) # open the file\n",
    "    band = ds.GetRasterBand(band) # get the data band\n",
    "    data = band.ReadAsArray() # convert to numpy array for further manipulation   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a4c52d08-25f2-4771-8632-db225075f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_data_range(data,min,max,replace_with='limit'):\n",
    "\n",
    "    '''Clamps data at min and max values'''\n",
    "\n",
    "    if replace_with =='limit':\n",
    "        data[data<min] = min\n",
    "        data[data>max] = max\n",
    "    else:\n",
    "        data[data<min] = replace_with\n",
    "        data[data>max] = replace_with\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e2028938-66f1-4887-ae77-075606c7dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_geotif_sameDomain(src_file,des_file,des_data):\n",
    "    \n",
    "    # load the source file to get the appropriate attributes\n",
    "    src_ds = gdal.Open(src_file)\n",
    "    \n",
    "    # get the geotransform\n",
    "    des_transform = src_ds.GetGeoTransform()\n",
    "\n",
    "    # Get the scale factor from the source metadata\n",
    "    scale_factor = src_ds.GetRasterBand(1).GetScale()\n",
    "    offset = src_ds.GetRasterBand(1).GetOffset()\n",
    "    \n",
    "    # get the data dimensions\n",
    "    ncols = des_data.shape[1]\n",
    "    nrows = des_data.shape[0]\n",
    "    \n",
    "    # make the file\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    dst_ds = driver.Create(des_file,ncols,nrows,1,gdal.GDT_Float32, options = [ 'COMPRESS=DEFLATE' ])\n",
    "    dst_ds.GetRasterBand(1).WriteArray( des_data )\n",
    "\n",
    "    # Set the scale factor in the destination band, if they were specified in the source\n",
    "    if scale_factor: dst_ds.GetRasterBand(1).SetScale(scale_factor)\n",
    "    if offset: dst_ds.GetRasterBand(1).SetOffset(offset)\n",
    "    \n",
    "    # Set the geotransform\n",
    "    dst_ds.SetGeoTransform(des_transform)\n",
    "\n",
    "    # Set the projection\n",
    "    wkt = src_ds.GetProjection()\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromWkt(wkt)\n",
    "    dst_ds.SetProjection( srs.ExportToWkt() )\n",
    "    \n",
    "    # close files\n",
    "    src_ds = None\n",
    "    des_ds = None\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3c78f63f-6a8b-4c4a-ba36-f783d4afc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monthly_lai_maps(lai_files, des_path):\n",
    "    des_files = []\n",
    "    for month in range(1,13):\n",
    "\n",
    "        # Define valid data range\n",
    "        # See docs, Table 4: https://lpdaac.usgs.gov/documents/926/MOD15_User_Guide_V61.pdf\n",
    "        modis_min = 0\n",
    "        modis_max = 100\n",
    "    \n",
    "        # Get the files we have for this month, for the last n years\n",
    "        #print(f'Processing month {month:02d}')\n",
    "        month_files = filter_lai_files_by_date(lai_files, months=[month])\n",
    "    \n",
    "        # Remove the one file we know is incomplete, 2022-10-16\n",
    "        month_files = [file for file in month_files if '20221016' not in file]\n",
    "        \n",
    "        # Load the data as numpy arrays, stack vertically, and find the mean value (ignoring nan)\n",
    "        data = [get_geotif_data_as_array(file) for file in month_files] # Get data as uint8\n",
    "        stacked = np.dstack(data) # Create a 3D stack\n",
    "        stacked_msk = np.ma.masked_array(stacked, mask=(stacked<modis_min) | (stacked>modis_max)) # Retain valid values only\n",
    "        mean_lai = np.ma.mean(stacked_msk, axis=2)\n",
    "    \n",
    "        # Define the no-data locations\n",
    "        #mean_all = np.nanmean(stacked, axis=2) # Any pixel that consistently has no-data in the source files (>= 249) should have a >= 249 mean\n",
    "        #mean_lai[mean_all >= 249] = mean_all[mean_all >= 249] # Place the no-data values in the new monthly-mean-lai file\n",
    "        \n",
    "        # Define output file name and write to disk\n",
    "        src_file = month_files[0] # We use this to copy over domain, projection, data scaling, etc\n",
    "        des_file = str( des_path / f'month_mean_{month:02d}_MOD_Grid_MOD15A2H_Lai_500m.tif' )\n",
    "        write_geotif_sameDomain(src_file, des_file, mean_lai)\n",
    "        des_files.append(des_file)\n",
    "    return des_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ad7eba19-9572-454f-81d9-6971d032ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to apply circmean to each group\n",
    "# Without this, xarray chokes on dimensions when converting the circmean output back into something with the right month indices\n",
    "def circmean_group(group):\n",
    "    return xr.DataArray(circmean(group, high=360, low=0), name='phi')\n",
    "\n",
    "def circstd_group(group):\n",
    "    return xr.DataArray(circstd(group, high=360, low=0), name='phi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa7fb4-9c67-4ee1-8ff3-046385c4f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing function to update the two main lists we're populating\n",
    "def process_era5_means_to_lists(da, l_values, l_index, var, unit):\n",
    "    '''Takes an xarray data array with monthly means and processes into l_values and l_index lists'''\n",
    "    for month in range(1,13):\n",
    "        val = da.sel(month=month).values.flatten()[0]\n",
    "        txt = (f'Climate', f'{var}_mean_month_{month:02}', f'{unit}', 'ERA5')\n",
    "        l_values += [val] # Needs to be this way because we're appending to a list\n",
    "        l_index  += [txt]\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a250e820-13ad-46bd-a8d9-34d0b8a52039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_monthly_means_to_lists(da, stat, l_values, l_index, var, unit, category='Climate', source='ERA5'):\n",
    "    '''Takes an xarray data array with monthly statistics and processes into l_values and l_index lists'''\n",
    "    for month in range(1,13):\n",
    "        val = da.sel(month=month).values.flatten()[0]\n",
    "        txt = (f'{category}', f'{var}_{stat}_month_{month:02}', f'{unit}', f'{source}')\n",
    "        l_values += [val] # Needs to be this way because we're appending to a list\n",
    "        l_index  += [txt]\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad326f-a555-4e94-a0dd-7195037869c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to map months to seasons\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'djf'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'mam'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'jja'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'son'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a678b51-a610-4f2b-a4de-ee6ad82eb4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds duration counts in a vector of True/False values\n",
    "def find_durations(condition):\n",
    "    '''Counts the duration(s) of True values in an xarray dataseries'''\n",
    "\n",
    "    previous = False\n",
    "    duration = 0\n",
    "    durations = []\n",
    "    for flag in condition.values:\n",
    "        \n",
    "        # Time step where we reset the count\n",
    "        if not previous and flag:\n",
    "            duration = 0 # New first timestep where condition = True, so duration = 0\n",
    "            previous = True\n",
    "    \n",
    "        # Time step where we're in a sequence of condition = True\n",
    "        if previous and flag:\n",
    "            duration += 1 # Update duration, implicitly retain previous = True by not changing it\n",
    "        \n",
    "        # Time step where we reach the end of a condition = True duration\n",
    "        if previous and not flag:\n",
    "            durations.append(duration) # Save the maximum duration length to list\n",
    "            previous = False # Update previous; duration will be reset next time we encounter a condition = True\n",
    "    \n",
    "        # Time step where we're in a continuation of condition = False\n",
    "        if not previous and not flag:\n",
    "            continue # do nothing\n",
    "    \n",
    "    return np.array(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bce3f5-d19e-4cf1-baf7-7c00ce1381d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds a long-term mean daily maximum temperature in a way that doesn't rely on \n",
    "#  time.dt.dayofyear, because the latter will have Dec-31 as day 365 in non-leap-years,\n",
    "#  and as 366 in leap years. Hence the day-of-year means do not use the same dates for \n",
    "#  a given DoY in leap years as they do in regular years.\n",
    "def create_mean_daily_max_series(ds,var='t'):\n",
    "    '''Finds the long-term mean daily maximum value of a variable'''\n",
    "    \n",
    "    # Create an array of all the month-days we have (e.g. 1949-12-31 00:00 becomes 1231)\n",
    "    month_days_all = ds.time.dt.month * 100 + ds.time.dt.day\n",
    "\n",
    "    # Loop over the unique month-days we have, and find the mean daily maximum value for each\n",
    "    month_days_unique = np.unique(month_days_all)\n",
    "    mean_daily_max = []\n",
    "    for month_day in month_days_unique:\n",
    "        val = ds[var].sel(time=(month_days_all==month_day)).groupby('time.year').max().mean().values\n",
    "        mean_daily_max.append(val)\n",
    "\n",
    "    # Convert the list to an array for further processing\n",
    "    mean_daily_max = np.array(mean_daily_max)\n",
    "\n",
    "    # Extract month_day values from the long xarray DataArray\n",
    "    month_day_values = month_days_all.values\n",
    "    \n",
    "    # Find the indices of each month_day in the unique_month_days array\n",
    "    indices = np.searchsorted(month_days_unique, month_day_values)\n",
    "    \n",
    "    # Use the indices to extract the corresponding data values\n",
    "    corresponding_data_values = mean_daily_max[indices]\n",
    "    \n",
    "    # Create a new DataArray with the corresponding data values\n",
    "    result_array = xr.DataArray(corresponding_data_values, \n",
    "                                coords=month_days_all.coords, \n",
    "                                dims=month_days_all.dims)\n",
    "\n",
    "    return result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42539dd6-391f-4ede-b38a-4ccf695e8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General processing of high/low temperature/precipitation frequency/duration/timing stats\n",
    "def calculate_temp_prcp_stats(var, condition, hilo, l_values,l_index,\n",
    "                              dataset='ERA5', units='hours'):\n",
    "    \n",
    "    '''Calculates frequency (mean) and duration (mean, median, skew, kurtosis) \n",
    "        of temperature/precipitation periods'''\n",
    "\n",
    "    # Constants. We want everything in [days] for consistency with original CAMELS\n",
    "    hours_per_day = 24 # [hours day-1]\n",
    "    days_per_year = 365.25 # [days year-1]\n",
    "\n",
    "    # Calculate frequencies\n",
    "    freq = condition.mean(dim='time') * days_per_year # [-] * [days year-1]\n",
    "    l_values.append(freq.values[0])\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_freq', 'days year^-1', dataset) )\n",
    "    \n",
    "    # Calculate duration statistics\n",
    "    durations = find_durations(condition) # [time steps]\n",
    "    if units == 'hours':\n",
    "        durations = durations / hours_per_day # [days] = [hours] / [hours day-1]\n",
    "    l_values.append(np.mean(durations)) # [days]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_mean', 'days', dataset) ) # Consistency with\n",
    "    l_values.append(np.median(durations)) # [days]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_median', 'days', dataset) )\n",
    "    l_values.append(skew(durations)) # [-]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_skew', '-', dataset) )\n",
    "    l_values.append(kurtosis(durations)) # [-]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_kurtosis', '-', dataset) )\n",
    "\n",
    "    # Calculate timing statistic\n",
    "    condition['season'] = ('time', \n",
    "        [get_season(month) for month in condition['time.month'].values]) # add seasons\n",
    "    max_season_id = condition.groupby('season').sum().argmax(dim='season') # find season with most True values\n",
    "    l_values.append(condition.season[max_season_id].values[0]) # add season abbrev\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_timing', 'season', dataset) )\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8cccc-dc64-4df5-af73-7659280451cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_dataset_to_max_full_years(ds, time='time', res='hour', water_year=False, debug=False) -> xr.Dataset:\n",
    "\n",
    "    # Find final and end years\n",
    "    start_year = ds[time][0].dt.year.values\n",
    "    final_year = ds[time][-1].dt.year.values\n",
    "    final_timestamp = ds[time][-1]\n",
    "\n",
    "    # Set how a year is defined\n",
    "    if water_year: # Oct-1 to Sep-30\n",
    "        start_month = 10\n",
    "        start_day   = 1\n",
    "        final_month = 9\n",
    "        final_day   = 30\n",
    "    else: # Jan-1 to Dec-31\n",
    "        start_month = 1\n",
    "        start_day   = 1\n",
    "        final_month = 12\n",
    "        final_day   = 31\n",
    "\n",
    "    # Find the first occurrence of Jan-01 00:00 as start of the subset\n",
    "    for year in range(start_year,final_year):\n",
    "        \n",
    "        # Define the initial timestamp to check\n",
    "        if res == 'hour':\n",
    "            start_timestamp = pd.Timestamp(year,start_month,start_day,0,0,0)\n",
    "        elif res == 'day':\n",
    "            start_timestamp = pd.Timestamp(year,start_month,start_day)\n",
    "        if debug: print(f'checking {start_timestamp}')\n",
    "\n",
    "        # Select the subset of the dataset for the current duration\n",
    "        # Note: if either start or final are not part of the time series,\n",
    "        #  this will silently just use whatever is available\n",
    "        subset_ds = ds.sel(time=slice(start_timestamp, final_timestamp))\n",
    "\n",
    "        # Check if we actually selected the duration we requested\n",
    "        subset_start = pd.Timestamp(subset_ds[time][0].values)\n",
    "        if subset_start == start_timestamp:\n",
    "            subset_start_year = year # Keep track of where we start\n",
    "            break # stop searching. We've found the first occurrence of Jan-01\n",
    "\n",
    "    # Find the last occurrence of Dec-31 23:00 as end of the subset\n",
    "    for year in range(final_year,subset_start_year,-1):\n",
    "\n",
    "        # Define the initial timestamp to check\n",
    "        if res == 'hour':\n",
    "            end_timestamp = pd.Timestamp(year,final_month,final_day,23,0,0)\n",
    "        elif res == 'day':\n",
    "            end_timestamp = pd.Timestamp(year,final_month,final_day)\n",
    "        if debug: print(f'checking {end_timestamp}')\n",
    "\n",
    "        # Select the subset of the dataset for the current duration\n",
    "        subset_ds = ds.sel(time=slice(start_timestamp, end_timestamp))\n",
    "\n",
    "        # Check if we actually selected the duration we requested\n",
    "        subset_end = pd.Timestamp(subset_ds[time][-1].values)\n",
    "        if subset_end == end_timestamp:\n",
    "            subset_end_year = year # Keep track of where we start\n",
    "            break # stop searching. We've found the first occurrence of Jan-01\n",
    "\n",
    "    # Now check if we have selected a zero-year period\n",
    "    # This would imply we have less than a full year of data\n",
    "    # In this case, just return the original data set with a warning\n",
    "    if len(subset_ds) == 0:\n",
    "        print(f'--- WARNING: subset_dataset_to_max_full_years(): Found no full data years. Returning original DataArray')\n",
    "        return ds\n",
    "    else:   \n",
    "        return subset_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ba6ff-0399-44b5-bc39-6f0087556e1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c03abfe-2021-4a17-a995-62ea44df6aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Vegetation</th>\n",
       "      <th>forest_height_2020_mean</th>\n",
       "      <th>m</th>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forest_height_2020_stdev</th>\n",
       "      <th>m</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Topography</th>\n",
       "      <th>dem_mean</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "      <td>100</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dem_stdev</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               01   02   03   04\n",
       "Category   Attribute                Unit                        \n",
       "Vegetation forest_height_2020_mean  m          25   30   35   40\n",
       "           forest_height_2020_stdev m           5    3    3    4\n",
       "Topography dem_mean                 m.a.s.l.  300  400  100  600\n",
       "           dem_stdev                m.a.s.l.   30    1   10    7"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample data\n",
    "l_station_id = ['01', '02', '03', '04']\n",
    "l_values = []\n",
    "l_values.append([25, 5, 300, 30])\n",
    "l_values.append([30, 3, 400, 1])\n",
    "l_values.append([35, 3, 100, 10])\n",
    "l_values.append([40, 4, 600, 7])\n",
    "\n",
    "# Make the dataframe\n",
    "input_dict = dict(zip(l_station_id, l_values))\n",
    "df = pd.DataFrame(input_dict)\n",
    "\n",
    "# Set the index\n",
    "multi_index = pd.MultiIndex.from_tuples([\n",
    "    ('Vegetation', 'forest_height_2020_mean', 'm'),\n",
    "    ('Vegetation', 'forest_height_2020_stdev', 'm'),\n",
    "    ('Topography', 'dem_mean', 'm.a.s.l.'),\n",
    "    ('Topography', 'dem_stdev', 'm.a.s.l.')\n",
    "], names=['Category', 'Attribute', 'Unit'])\n",
    "df.index = multi_index\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f4a4ab7d-5c44-463c-8c31-859f3bdf5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test_attributes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f90a83-2162-4d9d-ac9e-79fc89641910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels-spat-env-TRIAL",
   "language": "python",
   "name": "camels-spat-env-trial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
