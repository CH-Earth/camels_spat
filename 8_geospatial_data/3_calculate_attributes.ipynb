{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db3e732-426b-4b22-b055-b9395f29fa09",
   "metadata": {},
   "source": [
    "## Calculate attributes\n",
    "Takes prepared geospatial data and computes various attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac7b17d-07d9-4985-bb9c-8508fb2c7554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w4/hh0hkdr92cg8llf8s0l5rzj80000gq/T/ipykernel_10307/3056185177.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "from python_cs_functions import config as cs, attributes as csa\n",
    "from python_cs_functions.delineate import prepare_delineation_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bcba8f-d99c-4058-9de9-23ae255c19e9",
   "metadata": {},
   "source": [
    "### Config handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dfdcc03-f734-4df6-8bee-25ae59f5560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where the config file can be found\n",
    "config_file = '../0_config/config.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa42aa7-cf4d-4c51-8555-05638e2ffc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the required info from the config file\n",
    "data_path            = cs.read_from_config(config_file,'data_path')\n",
    "\n",
    "# CAMELS-spat metadata\n",
    "cs_meta_path = cs.read_from_config(config_file,'cs_basin_path')\n",
    "cs_meta_name = cs.read_from_config(config_file,'cs_meta_name')\n",
    "cs_unusable_name = cs.read_from_config(config_file,'cs_unusable_name')\n",
    "\n",
    "# Basin folder\n",
    "cs_basin_folder = cs.read_from_config(config_file, 'cs_basin_path')\n",
    "basins_path = Path(data_path) / cs_basin_folder\n",
    "\n",
    "# Get the temporary data folder\n",
    "cs_temp_folder = cs.read_from_config(config_file, 'temp_path')\n",
    "temp_path = Path(cs_temp_folder)\n",
    "temp_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933dae9c-b5fe-42d2-9cc9-2ee54661e2c7",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "333bd63a-b0a1-417d-b9c6-d28306a0120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAMELS-spat metadata file\n",
    "cs_meta_path = Path(data_path) / cs_meta_path\n",
    "cs_meta = pd.read_csv(cs_meta_path / cs_meta_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a497a0-a9a2-404c-9b2a-8173cbb1436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open list of unusable stations; Enforce reading IDs as string to keep leading 0's\n",
    "cs_unusable = pd.read_csv(cs_meta_path / cs_unusable_name, dtype={'Station_id': object})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87113cc-bfb3-48dd-bae1-4c7f19fd39b3",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3185dc90-2a46-46b2-bdd1-c0032c2c452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_message = f'\\n!!! CHECK DEBUGGING STATUS: \\n- Testing 1 file \\n- Testing 1 basin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c395a8-7e5d-4140-b61d-65581fc05a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_subfolders = ['worldclim', 'lai', 'forest_height', 'glclu2019', 'modis_land','lgrip30','merit'] #'era5'\n",
    "# 'glhymps', 'hydrolakes','pelletier','soilgrids', 'hydrology'\n",
    "geo_subfolders = ['merit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11e6b92c-35d3-4358-a3fe-0daec6cd96f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every attribute needs a list, so that we can efficiently construct a dataframe later\n",
    "l_gauges = [] # station ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b44331c-4cd6-4b2a-ba99-db3a767f9ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!!! CHECK DEBUGGING STATUS: \n",
      "- Testing 1 file \n",
      "- Testing 1 basin\n",
      "Processing geospatial data into attributes for CAN_01AD002\n",
      " - processing merit\n",
      "\n",
      "!!! CHECK DEBUGGING STATUS: \n",
      "- Testing 1 file \n",
      "- Testing 1 basin\n"
     ]
    }
   ],
   "source": [
    "print(debug_message)\n",
    "for ix,row in cs_meta.iterrows():\n",
    "\n",
    "    # DEBUGGING\n",
    "    if ix != 0: continue\n",
    "\n",
    "    # Get the paths\n",
    "    basin_id, shp_lump_path, shp_dist_path, _, _ = prepare_delineation_outputs(cs_meta, ix, basins_path)\n",
    "    geo_folder = basins_path / 'basin_data' / basin_id / 'geospatial'\n",
    "    met_folder = basins_path / 'basin_data' / basin_id / 'forcing'\n",
    "\n",
    "    # Data storage\n",
    "    l_gauges.append(basin_id) # Update the Station list\n",
    "    l_values = [] # Initialize an empty list where we'll store this basin's attributes\n",
    "    l_index = [] # Initialize an empty list where we'll store the attribute descriptions\n",
    "\n",
    "    # Define the shapefiles\n",
    "    shp = str(shp_lump_path) # because zonalstats wants a file path, not a geodataframe\n",
    "    riv = str(shp_dist_path).format('river') # For topographic attributes\n",
    "    \n",
    "    # Data-specific processing\n",
    "    print(f'Processing geospatial data into attributes for {basin_id}')\n",
    "    for dataset in geo_subfolders:\n",
    "        print(f' - processing {dataset}')\n",
    "\n",
    "        ## CLIMATE\n",
    "        if dataset == 'era5':\n",
    "            l_values, l_index = csa.attributes_from_era5(met_folder, shp, 'era5', l_values, l_index)                                \n",
    "        if dataset == 'worldclim':\n",
    "            csa.oudin_pet_from_worldclim(geo_folder, dataset) # Get an extra PET estimate to sanity check ERA5 outcomes\n",
    "            csa.aridity_and_fraction_snow_from_worldclim(geo_folder, dataset) # Get monthly aridity and fraction snow maps\n",
    "            l_values, l_index = csa.attributes_from_worldclim(geo_folder, dataset, shp, l_values, l_index)\n",
    "\n",
    "        ## LAND COVER\n",
    "        if dataset == 'forest_height':\n",
    "            l_values, l_index = csa.attributes_from_forest_height(geo_folder, dataset, shp, l_values, l_index)\n",
    "        if dataset == 'lai':\n",
    "            l_values, l_index = csa.attributes_from_lai(geo_folder, dataset, temp_path, shp, l_values, l_index)\n",
    "        if dataset == 'glclu2019':\n",
    "            l_values, l_index = csa.attributes_from_glclu2019(geo_folder, dataset, shp, l_values, l_index)\n",
    "        if dataset == 'modis_land':\n",
    "            l_values, l_index = csa.attributes_from_modis_land(geo_folder, dataset, shp, l_values, l_index)\n",
    "        if dataset == 'lgrip30':\n",
    "            l_values, l_index = csa.attributes_from_lgrip30(geo_folder, dataset, shp, l_values, l_index)\n",
    "\n",
    "        ## TOPOGRAPHY\n",
    "        if dataset == 'merit':\n",
    "            l_values, l_index = csa.attributes_from_merit(geo_folder, dataset, shp, riv, row, l_values, l_index)\n",
    "        \n",
    "print(debug_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d197b84-b519-487b-aaf4-b14929671829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 23)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l_values),len(l_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f740e80-faf5-4c56-a379-48b3fb2b6b5f",
   "metadata": {},
   "source": [
    "#### Make the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f038ae6-5dff-43cf-a462-8d31eea3d39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>CAN_01AD002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"23\" valign=\"top\">Topography</th>\n",
       "      <th>gauge_lat</th>\n",
       "      <th>degrees</th>\n",
       "      <th>WSC 20222 data set</th>\n",
       "      <td>47.258060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gauge_lon</th>\n",
       "      <th>degrees</th>\n",
       "      <th>WSC 20222 data set</th>\n",
       "      <td>-68.595830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basin_area</th>\n",
       "      <th>km^2</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>14691.618940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merit_hydro_elev_min</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>152.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merit_hydro_elev_mean</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>362.035068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merit_hydro_elev_max</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>858.200012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merit_hydro_elev_std</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>86.043743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merit_hydro_slope_min</th>\n",
       "      <th>m m-1</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merit_hydro_slope_mean</th>\n",
       "      <th>m m-1</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>4.029331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merit_hydro_slope_max</th>\n",
       "      <th>m m-1</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>51.633347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merit_hydro_slope_std</th>\n",
       "      <th>m m-1</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>3.502045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merit_hydro_aspect_min</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merit_hydro_aspect_mean</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>142.194444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merit_hydro_aspect_std</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>148.736972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merit_hydro_aspect_max</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>359.999939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stream_length_min</th>\n",
       "      <th>km</th>\n",
       "      <th>MERIT Hydro Basins</th>\n",
       "      <td>12.469541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stream_length_mean</th>\n",
       "      <th>km</th>\n",
       "      <th>MERIT Hydro Basins</th>\n",
       "      <td>144.991369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stream_length_max</th>\n",
       "      <th>km</th>\n",
       "      <th>MERIT Hydro Basins</th>\n",
       "      <td>286.189869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stream_length_std</th>\n",
       "      <th>km</th>\n",
       "      <th>MERIT Hydro Basins</th>\n",
       "      <td>66.377624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stream_length_total</th>\n",
       "      <th>km</th>\n",
       "      <th>MERIT Hydro Basins</th>\n",
       "      <td>2502.025189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>steam_order_max</th>\n",
       "      <th>-</th>\n",
       "      <th>MERIT Hydro Basins</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stream_density</th>\n",
       "      <th>km^-1</th>\n",
       "      <th>Derived</th>\n",
       "      <td>0.170303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elongation_ratio</th>\n",
       "      <th>-</th>\n",
       "      <th>Derived</th>\n",
       "      <td>0.477898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 CAN_01AD002\n",
       "Category   Attribute               Unit     Source                          \n",
       "Topography gauge_lat               degrees  WSC 20222 data set     47.258060\n",
       "           gauge_lon               degrees  WSC 20222 data set    -68.595830\n",
       "           basin_area              km^2     MERIT Hydro         14691.618940\n",
       "           merit_hydro_elev_min    m.a.s.l. MERIT Hydro           152.000000\n",
       "           merit_hydro_elev_mean   m.a.s.l. MERIT Hydro           362.035068\n",
       "           merit_hydro_elev_max    m.a.s.l. MERIT Hydro           858.200012\n",
       "           merit_hydro_elev_std    m.a.s.l. MERIT Hydro            86.043743\n",
       "           merit_hydro_slope_min   m m-1    MERIT Hydro             0.000000\n",
       "           merit_hydro_slope_mean  m m-1    MERIT Hydro             4.029331\n",
       "           merit_hydro_slope_max   m m-1    MERIT Hydro            51.633347\n",
       "           merit_hydro_slope_std   m m-1    MERIT Hydro             3.502045\n",
       "           merit_hydro_aspect_min  degrees  MERIT Hydro             0.000000\n",
       "           merit_hydro_aspect_mean degrees  MERIT Hydro           142.194444\n",
       "           merit_hydro_aspect_std  degrees  MERIT Hydro           148.736972\n",
       "           merit_hydro_aspect_max  degrees  MERIT Hydro           359.999939\n",
       "           stream_length_min       km       MERIT Hydro Basins     12.469541\n",
       "           stream_length_mean      km       MERIT Hydro Basins    144.991369\n",
       "           stream_length_max       km       MERIT Hydro Basins    286.189869\n",
       "           stream_length_std       km       MERIT Hydro Basins     66.377624\n",
       "           stream_length_total     km       MERIT Hydro Basins   2502.025189\n",
       "           steam_order_max         -        MERIT Hydro Basins      5.000000\n",
       "           stream_density          km^-1    Derived                 0.170303\n",
       "           elongation_ratio        -        Derived                 0.477898"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with a fake second station\n",
    "l_gauges = ['CAN_01AD002','CAN_01AD003']\n",
    "\n",
    "# Make the dataframe\n",
    "input_dict = dict(zip(l_gauges, [l_values,l_values]))\n",
    "df = pd.DataFrame(input_dict)\n",
    "\n",
    "# Set the index\n",
    "multi_index = pd.MultiIndex.from_tuples(l_index, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "df.index = multi_index\n",
    "\n",
    "# Drop the fake extra column\n",
    "df = df.drop(columns=['CAN_01AD003'], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b27b6090-0a20-48aa-8c85-0a0c5e3ceff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>CAN_01AD002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">Land cover</th>\n",
       "      <th>lai_mean_month_01</th>\n",
       "      <th>m^2 m^-2</th>\n",
       "      <th>MCD15A2H.061</th>\n",
       "      <td>0.569779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lai_std_month_01</th>\n",
       "      <th>m^2 m^-2</th>\n",
       "      <th>MCD15A2H.061</th>\n",
       "      <td>0.284748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lai_mean_month_02</th>\n",
       "      <th>m^2 m^-2</th>\n",
       "      <th>MCD15A2H.061</th>\n",
       "      <td>0.474105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lai_std_month_02</th>\n",
       "      <th>m^2 m^-2</th>\n",
       "      <th>MCD15A2H.061</th>\n",
       "      <td>0.248943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lai_mean_month_03</th>\n",
       "      <th>m^2 m^-2</th>\n",
       "      <th>MCD15A2H.061</th>\n",
       "      <td>0.418053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc2_unclassified_fraction</th>\n",
       "      <th>-</th>\n",
       "      <th>MCD12Q1.061</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc3_water_fraction</th>\n",
       "      <th>-</th>\n",
       "      <th>LGRIP30</th>\n",
       "      <td>0.026947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc3_non_cropland_fraction</th>\n",
       "      <th>-</th>\n",
       "      <th>LGRIP30</th>\n",
       "      <td>0.951509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc3_irrigated_cropland_fraction</th>\n",
       "      <th>-</th>\n",
       "      <th>LGRIP30</th>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc3_rainfed_cropland_fraction</th>\n",
       "      <th>-</th>\n",
       "      <th>LGRIP30</th>\n",
       "      <td>0.021509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  CAN_01AD002\n",
       "Category   Attribute                       Unit     Source                   \n",
       "Land cover lai_mean_month_01               m^2 m^-2 MCD15A2H.061     0.569779\n",
       "           lai_std_month_01                m^2 m^-2 MCD15A2H.061     0.284748\n",
       "           lai_mean_month_02               m^2 m^-2 MCD15A2H.061     0.474105\n",
       "           lai_std_month_02                m^2 m^-2 MCD15A2H.061     0.248943\n",
       "           lai_mean_month_03               m^2 m^-2 MCD15A2H.061     0.418053\n",
       "...                                                                       ...\n",
       "           lc2_unclassified_fraction       -        MCD12Q1.061      0.000000\n",
       "           lc3_water_fraction              -        LGRIP30          0.026947\n",
       "           lc3_non_cropland_fraction       -        LGRIP30          0.951509\n",
       "           lc3_irrigated_cropland_fraction -        LGRIP30          0.000035\n",
       "           lc3_rainfed_cropland_fraction   -        LGRIP30          0.021509\n",
       "\n",
       "[74 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of selection code\n",
    "tmp = df.loc[df.index.get_level_values('Category').str.contains('Land cover')]# & \n",
    "             #df.index.get_level_values('Attribute').str.contains('mean')].copy()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127feb5-ed86-424b-8367-d4ec38177968",
   "metadata": {},
   "source": [
    "## DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fe183cc-8879-4189-9e21-014a1a76204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from rasterstats import zonal_stats\n",
    "import rasterio\n",
    "from scipy.stats import circmean, circstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "591570a6-d8e2-401a-95b7-6162972770b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94ef3368-0266-4d65-9a4a-c240c5740037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5d4274f-7e4b-4e7f-b221-5eaeba8f716b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75e69cbc-73b4-410a-894a-1a2c68067b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ecd30bf-7f36-4a08-89bb-cc86515a05ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### High-level collection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543601ad-64ad-4d95-8609-4f7cbdb0a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_merit(geo_folder, dataset, shp_str, riv_str, row, l_values, l_index):\n",
    "    \n",
    "    '''Calculates topographic attributes from MERIT data'''\n",
    "\n",
    "    ## Known values\n",
    "    lat = row['Station_lat']\n",
    "    lon = row['Station_lon']\n",
    "    area= row['Basin_area_km2']\n",
    "    src = row['Station_source']\n",
    "    l_values.append(lat)\n",
    "    l_index.append(('Topography', 'gauge_lat',  'degrees', f'{src}'))\n",
    "    l_values.append(lon)\n",
    "    l_index.append(('Topography', 'gauge_lon',  'degrees', f'{src}'))\n",
    "    l_values.append(area)\n",
    "    l_index.append(('Topography', 'basin_area', 'km^2', 'MERIT Hydro'))\n",
    "\n",
    "    ## RASTERS\n",
    "    # Slope and elevation can use zonal stats\n",
    "    files = [str(geo_folder / dataset / 'raw'    / 'merit_hydro_elv.tif'),\n",
    "             str(geo_folder / dataset / 'slope'  / 'merit_hydro_slope.tif')]\n",
    "    attrs = ['merit_hydro_elev', 'merit_hydro_slope']\n",
    "    units = ['m.a.s.l.',         'm m-1']\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    for tif,att,unit in zip(files,attrs,units):\n",
    "        zonal_out = zonal_stats(shp_str, tif, stats=stats)\n",
    "        scale,offset = csa.read_scale_and_offset(tif)\n",
    "        l_values = csa.update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "        l_index += [('Topography', f'{att}_min',  f'{unit}', 'MERIT Hydro'),\n",
    "                    ('Topography', f'{att}_mean', f'{unit}', 'MERIT Hydro'),\n",
    "                    ('Topography', f'{att}_max',  f'{unit}', 'MERIT Hydro'),\n",
    "                    ('Topography', f'{att}_std',  f'{unit}', 'MERIT Hydro')]\n",
    "\n",
    "    # Aspect needs circular stats\n",
    "    tif = str(geo_folder / dataset / 'aspect' / 'merit_hydro_aspect.tif')\n",
    "    l_values, l_index = get_aspect_attributes(tif,l_values,l_index) \n",
    "\n",
    "    ## VECTOR\n",
    "    l_values, l_index = get_river_attributes(riv_str, l_values, l_index, area)\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4033e983-99ba-4070-9a05-ecf0048719e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_lgrip30(geo_folder, dataset, shp_str, l_values, l_index):\n",
    "\n",
    "    '''Calculates percentage occurrence of all classes in LGRIP30 map'''\n",
    "\n",
    "    tif = geo_folder / dataset / 'raw' / 'lgrip30_agriculture.tif'\n",
    "    zonal_out = zonal_stats(shp_str, tif, categorical=True)\n",
    "    check_scale_and_offset(tif)\n",
    "    l_values,l_index = update_values_list_with_categorical(l_values, l_index, zonal_out, 'LGRIP30', prefix='lc3_')\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4579b4a9-d3ae-4665-a16d-e9bead393f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_modis_land(geo_folder, dataset, shp_str, l_values, l_index):\n",
    "\n",
    "    '''Calculates percentage occurrence of all classes in MODIS IGBP map'''\n",
    "\n",
    "    tif = geo_folder / dataset / 'raw' / '2001_2022_mode_MCD12Q1_LC_Type1.tif'\n",
    "    zonal_out = zonal_stats(shp_str, tif, categorical=True)\n",
    "    check_scale_and_offset(tif)\n",
    "    l_values,l_index = update_values_list_with_categorical(l_values, l_index, zonal_out, 'MCD12Q1.061', prefix='lc2_')\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00eb7bfe-a77d-49bf-8468-510437a42aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_glclu2019(geo_folder, dataset, shp_str, l_values, l_index):\n",
    "\n",
    "    '''Calculates percentage occurrence of all classes in GLCLU2019 map'''\n",
    "\n",
    "    tif = geo_folder / dataset / 'raw' / 'glclu2019_map.tif'\n",
    "    zonal_out = zonal_stats(shp_str, tif, categorical=True)\n",
    "    check_scale_and_offset(tif)\n",
    "    l_values,l_index = update_values_list_with_categorical(l_values, l_index, zonal_out, 'GLCLU 2019', prefix='lc1_')\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "101eb959-c4a2-41dc-8e60-7694dca0de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_era5(met_folder, shp_path, dataset, l_values, l_index, use_mfdataset=False):\n",
    "\n",
    "    '''Calculates a variety of metrics from ERA5 data'''\n",
    "\n",
    "    # Define various conversion constants\n",
    "    water_density = 1000 # kg m-3\n",
    "    mm_per_m = 1000 # mm m-1\n",
    "    seconds_per_hour = 60*60 # s h-1\n",
    "    seconds_per_day = seconds_per_hour*24 # s d-1\n",
    "    days_per_month = np.array([31, 28.25, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]).reshape(-1, 1) # d month-1\n",
    "    flip_sign = -1 # -; used to convert PET from negative (by convention this indicates an upward flux) to positive\n",
    "    kelvin_to_celsius = -273.15\n",
    "    pa_per_kpa = 1000 # Pa kPa-1\n",
    "\n",
    "    # Define file locations, depending on if we are dealing with lumped or distributed cases\n",
    "    if 'lumped' in shp_path:\n",
    "        era_folder = met_folder / 'lumped'\n",
    "    elif 'distributed' in shp_path:\n",
    "        era_folder = met_folder / 'distributed'\n",
    "    era_files = sorted( glob.glob( str(era_folder / 'ERA5_*.nc') ) )\n",
    "\n",
    "    # Open the data\n",
    "    if use_mfdataset:\n",
    "        ds = xr.open_mfdataset(era_files, engine='netcdf4')\n",
    "        ds = ds.load() # load the whole thing into memory instead of lazy-loading\n",
    "    else:\n",
    "        ds = xr.merge([xr.open_dataset(f) for f in era_files])\n",
    "        ds = ds.load()\n",
    "    \n",
    "    # Select whole years only\n",
    "    #   This avoids issues in cases where we have incomplete whole data years\n",
    "    #   (e.g. 2000-06-01 to 2007-12-31) in basins with very seasonal weather\n",
    "    #   (e.g. all precip occurs in Jan, Feb, Mar). By using only full years\n",
    "    #   we avoid accidentally biasing the attributes.\n",
    "    ds = subset_dataset_to_max_full_years(ds)\n",
    "    \n",
    "    # --- Monthly attributes\n",
    "    # Calculate monthly PET in mm\n",
    "    #      kg m-2 s-1 / kg m-3\n",
    "    # mm month-1 = kg m-2 s-1 * kg-1 m3 * s d-1 * d month-1 * mm m-1 * -\n",
    "    monthly_mper = ds['mper'].resample(time='1ME').mean().groupby('time.month') \n",
    "    mper_m = monthly_mper.mean() / water_density * seconds_per_day * days_per_month * mm_per_m * flip_sign  # [kg m-2 s-1] to [mm month-1]; negative to indicate upward flux\n",
    "    mper_s = monthly_mper.std() / water_density * seconds_per_day * days_per_month * mm_per_m  # [kg m-2 s-1] to [mm month-1]; negative to indicate upward flux\n",
    "    l_values, l_index = process_era5_means_to_lists(mper_m, 'mean', l_values, l_index, 'mper', 'mm')\n",
    "    l_values, l_index = process_era5_means_to_lists(mper_s, 'std', l_values, l_index, 'mper', 'mm')\n",
    "        \n",
    "    # Same for precipitation: [mm month-1]\n",
    "    monthly_mtpr = ds['mtpr'].resample(time='1ME').mean().groupby('time.month')\n",
    "    mtpr_m = monthly_mtpr.mean() / water_density * seconds_per_day * days_per_month * mm_per_m # [kg m-2 s-1] to [mm month-1]\n",
    "    mtpr_s = monthly_mtpr.std() / water_density * seconds_per_day * days_per_month * mm_per_m # [kg m-2 s-1] to [mm month-1]\n",
    "    l_values, l_index = process_era5_means_to_lists(mtpr_m, 'mean', l_values, l_index, 'mtpr', 'mm')\n",
    "    l_values, l_index = process_era5_means_to_lists(mtpr_s, 'std', l_values, l_index, 'mtpr', 'mm')\n",
    "    \n",
    "    # Monthly temperature statistics [C]\n",
    "    monthly_tavg = (ds['t'].resample(time='1D').mean().resample(time='1ME').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tavg_m = monthly_tavg.mean()\n",
    "    tavg_s = monthly_tavg.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(tavg_m, 'mean', l_values, l_index, 'tdavg', 'C')\n",
    "    l_values, l_index = process_era5_means_to_lists(tavg_s, 'std', l_values, l_index, 'tdavg', 'C')\n",
    "    \n",
    "    monthly_tmin = (ds['t'].resample(time='1D').min().resample(time='1ME').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tmin_m = monthly_tmin.mean()\n",
    "    tmin_s = monthly_tmin.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(tmin_m, 'mean', l_values, l_index, 'tdmin', 'C')\n",
    "    l_values, l_index = process_era5_means_to_lists(tmin_m, 'std', l_values, l_index, 'tdmin', 'C')\n",
    "    \n",
    "    monthly_tmax = (ds['t'].resample(time='1D').max().resample(time='1ME').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tmax_m = monthly_tmax.mean()\n",
    "    tmax_s = monthly_tmax.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(tmax_m, 'mean', l_values, l_index, 'tdmax', 'C')\n",
    "    l_values, l_index = process_era5_means_to_lists(tmax_s, 'std', l_values, l_index, 'tdmax', 'C')\n",
    "    \n",
    "    # Monthly shortwave and longwave [W m-2]\n",
    "    monthly_sw = ds['msdwswrf'].resample(time='1ME').mean().groupby('time.month')\n",
    "    sw_m = monthly_sw.mean()\n",
    "    sw_s = monthly_sw.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(sw_m, 'mean', l_values, l_index, 'msdwswrf', 'W m^-2')\n",
    "    l_values, l_index = process_era5_means_to_lists(sw_s, 'std', l_values, l_index, 'msdwswrf', 'W m^-2')\n",
    "    \n",
    "    monthly_lw = ds['msdwlwrf'].resample(time='1ME').mean().groupby('time.month')\n",
    "    lw_m = monthly_lw.mean(dim='time')\n",
    "    lw_s = monthly_lw.std(dim='time')\n",
    "    l_values, l_index = process_era5_means_to_lists(lw_m, 'mean', l_values, l_index, 'msdwlwrf', 'W m^-2')\n",
    "    l_values, l_index = process_era5_means_to_lists(lw_s, 'std', l_values, l_index, 'msdwlwrf', 'W m^-2')\n",
    "\n",
    "    # Surface pressure [Pa]\n",
    "    monthly_sp = ds['sp'].resample(time='1ME').mean().groupby('time.month')\n",
    "    sp_m = monthly_sp.mean() / pa_per_kpa # [Pa] > [kPa]\n",
    "    sp_s = monthly_sp.std() / pa_per_kpa\n",
    "    l_values, l_index = process_era5_means_to_lists(sp_m, 'mean', l_values, l_index, 'sp', 'kPa')\n",
    "    l_values, l_index = process_era5_means_to_lists(sp_s, 'std', l_values, l_index, 'sp', 'kPa')\n",
    "    \n",
    "    # Humidity [-]\n",
    "    monthly_q = ds['q'].resample(time='1ME').mean().groupby('time.month') # specific\n",
    "    q_m = monthly_q.mean()\n",
    "    q_s = monthly_q.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(q_m, 'mean', l_values, l_index, 'q', 'kg kg^-1')\n",
    "    l_values, l_index = process_era5_means_to_lists(q_s, 'std', l_values, l_index, 'q', 'kg kg^-1')\n",
    "    \n",
    "    monthly_rh = ds['rh'].resample(time='1ME').mean().groupby('time.month') # relative\n",
    "    rh_m = monthly_rh.mean()\n",
    "    rh_s = monthly_rh.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(rh_m, 'mean', l_values, l_index, 'rh', 'kPa kPa^-1')\n",
    "    l_values, l_index = process_era5_means_to_lists(rh_s, 'std', l_values, l_index, 'rh', 'kPa kPa^-1')\n",
    "    \n",
    "    # Wind speed [m s-1]\n",
    "    monthly_w = ds['w'].resample(time='1ME').mean().groupby('time.month')\n",
    "    w_m = monthly_w.mean()\n",
    "    w_s = monthly_w.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(w_m, 'mean', l_values, l_index, 'w', 'm s^-1')\n",
    "    l_values, l_index = process_era5_means_to_lists(w_s, 'std', l_values, l_index, 'w', 'm s^-1')\n",
    "    \n",
    "    # Wind direction\n",
    "    monthly_phi = ds['phi'].resample(time='1ME').apply(circmean_group).groupby('time.month')\n",
    "    phi_m = monthly_phi.apply(circmean_group)\n",
    "    phi_s = monthly_phi.apply(circstd_group)\n",
    "    l_values, l_index = process_era5_means_to_lists(phi_m, 'mean', l_values, l_index, 'phi', 'degrees')\n",
    "    l_values, l_index = process_era5_means_to_lists(phi_s, 'std', l_values, l_index, 'phi', 'degrees')\n",
    "    \n",
    "    # --- Long-term statistics (aridity, seasonality, snow)\n",
    "    monthly_ari = ((ds['mper'].resample(time='1ME').mean() * flip_sign) / ds['mtpr'].resample(time='1ME').mean()).groupby('time.month')\n",
    "    ari_m = monthly_ari.mean()\n",
    "    ari_s = monthly_ari.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(ari_m, 'mean', l_values, l_index, 'aridity', '-')\n",
    "    l_values, l_index = process_era5_means_to_lists(ari_s, 'std', l_values, l_index, 'aridity', '-')\n",
    "\n",
    "    ds['snow'] = xr.where(ds['t'] < 273.15, ds['mtpr'],0)\n",
    "    monthly_snow = (ds['snow'].resample(time='1ME').mean() / ds['mtpr'].resample(time='1ME').mean()).groupby('time.month')\n",
    "    snow_m = monthly_snow.mean()\n",
    "    snow_s = monthly_snow.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(snow_m, 'mean', l_values, l_index, 'fracsnow', '-')\n",
    "    l_values, l_index = process_era5_means_to_lists(snow_s, 'std', l_values, l_index, 'fracsnow', '-')\n",
    "\n",
    "    # --- High-frequency statistics (high/low duration/timing/magnitude)\n",
    "    #  Everyone does precip. We'll add temperature too as a drought/frost indicator\n",
    "    #  ERA5 only\n",
    "    \n",
    "    # -- LOW TEMPERATURE\n",
    "    variable  = 't'\n",
    "    low_threshold = 273.15 # K, freezing point\n",
    "    low_condition = ds[variable] < low_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('temp',low_condition,'low',l_values,l_index)\n",
    "    \n",
    "    # -- HIGH TEMPERATURE\n",
    "    # WMO defines a heat wave as a 5-day or longer period with maximum daily temperatures 5C above \n",
    "    # \"standard\" daily max temperature (1961-1990; source:\n",
    "    # https://www.ifrc.org/sites/default/files/2021-06/10-HEAT-WAVE-HR.pdf).\n",
    "    # We define a \"hot day\" therefore as a day with a maximum temperature 5 degrees over the \n",
    "    # the long-term mean maximum temperature.\n",
    "    #   Note: we don't have 1961-1990 data for some stations, so we stick with long-term mean.\n",
    "    #   Note: this will in most cases slightly underestimate heat waves compared to WMO definition\n",
    "    \n",
    "    # First, we identify the long-term mean daily maximum temperature in a dedicated function\n",
    "    high_threshold = create_mean_daily_max_series(ds,var='t')\n",
    "    \n",
    "    # Next, we check if which 't' values are 5 degrees above the long-term mean daily max \n",
    "    #  (\"(ds['t'] > result_array + 5)\"), and resample this to a daily time series \n",
    "    #  (\"resample(time='1D')\") filled with \"True\" if any value in that day was True.\n",
    "    daily_flags = (ds['t'] > high_threshold + 5).resample(time='1D').any()\n",
    "    \n",
    "    # Finally, we reindex these daily flags back onto the hourly time series by filling values\n",
    "    high_condition = daily_flags.reindex_like(ds['t'], method='ffill')\n",
    "    \n",
    "    # Now calculate stats like before\n",
    "    l_values,l_index = calculate_temp_prcp_stats('temp',high_condition,'high',l_values,l_index)\n",
    "    \n",
    "    # -- LOW PRECIPITATION\n",
    "    variable = 'mtpr'\n",
    "    # We'll stick with the original CAMELS definition of low precipitation: < 1 mm day-1\n",
    "    # It may not make too much sense to look at \"dry hours\" so we'll do this analysis at daily step\n",
    "    low_threshold = 1 # [mm d-1]\n",
    "    # Create daily precipitation sum (divided by density, times mm m-1 cancels out)\n",
    "    # [kg m-2 s-1] * [s h-1] / [kg m-3] * [mm m-1] = [mm h-1]\n",
    "    low_condition = (ds[variable] * seconds_per_hour).resample(time='1D').sum() < low_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('prec',low_condition,'low',l_values,l_index,\n",
    "                                             units='days') # this 'units' argument prevents conversion to days inside the functiom\n",
    "    \n",
    "    # -- HIGH PRECIPITATION\n",
    "    # CAMELS: > 5 times mean daily precip\n",
    "    high_threshold = 5 * (ds[variable] * seconds_per_hour).resample(time='1D').sum().mean()\n",
    "    high_condition = (ds[variable] * seconds_per_hour).resample(time='1D').sum() >= high_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('prec',high_condition,'high',l_values,l_index,\n",
    "                                                 units='days')\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3686d712-b932-4cb7-b153-0bfaab2feb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_worldclim(geo_folder, dataset, shp_str, l_values, index):\n",
    "\n",
    "    '''Calculates mean and stdv for tifs of monthly WorldClim values'''\n",
    "\n",
    "    # Define file locations\n",
    "    # Units source: https://www.worldclim.org/data/worldclim21.html\n",
    "    clim_folder = geo_folder / dataset / 'raw'\n",
    "    sub_folders =      ['prec', 'srad',         'tavg', 'tmax', 'tmin', 'vapr', 'wind',   'pet']\n",
    "    sub_folder_units = ['mm',   'kJ m^-2 d^-1', 'C',    'C',    'C',    'kPa',  'm s^-1', 'mm']\n",
    "\n",
    "    # Loop over the files and calculate the stats\n",
    "    for sub_folder, sub_folder_unit in zip(sub_folders, sub_folder_units):\n",
    "        month_files = sorted( glob.glob(str(clim_folder / sub_folder / '*.tif')) )\n",
    "        for month_file in month_files:\n",
    "            month_file = clim_folder / sub_folder / month_file # Construct the full path, because listdir() gives only files\n",
    "            stats = ['mean', 'std']\n",
    "            zonal_out = zonal_stats(shp_str, month_file, stats=stats)\n",
    "\n",
    "            scale, offset = csa.read_scale_and_offset(month_file)\n",
    "            scale, offset = read_scale_and_offset(month_file)\n",
    "            if sub_folder == 'srad':\n",
    "                zonal_out = zonal_stats_unit_conversion(zonal_out,stats,'srad', scale, offset)\n",
    "            l_values = csa.update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "            \n",
    "            month = os.path.basename(month_file).split('_')[3].split('.')[0]\n",
    "            var = os.path.basename(month_file).split('_')[2]\n",
    "            source = 'WorldClim'\n",
    "            if var == 'pet': source = 'WorldClim (derived, Oudin et al., 2005)'\n",
    "            index += [('Climate', f'{var}_mean_month_{month}', f'{sub_folder_unit}',  source),\n",
    "                      ('Climate', f'{var}_stdev_month_{month}', f'{sub_folder_unit}', source)]\n",
    "\n",
    "    return l_values, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1af9044e-e75b-48a7-815a-c8869f6212b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_forest_height(geo_folder, dataset, shp_str, l_values):\n",
    "\n",
    "    '''Calculates mean, min, max and stdv for forest height 2000 and 2020 tifs'''\n",
    "\n",
    "    # Year 2000 min, mean, max, stdev\n",
    "    tif = str( geo_folder / dataset / 'raw' / 'forest_height_2000.tif' )\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    zonal_out = zonal_stats(shp, tif, stats=stats)\n",
    "    scale,offset = read_scale_and_offset(tif)\n",
    "    l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "\n",
    "    # Year 2020 mean, stdev\n",
    "    tif = geo_folder / dataset / 'raw' / 'forest_height_2020.tif'\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    zonal_out = zonal_stats(shp, tif, stats=stats)\n",
    "    scale,offset = read_scale_and_offset(tif)\n",
    "    l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "\n",
    "    return l_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b4204-865e-45ad-85f5-aff233536d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_lai(geo_folder, dataset, temp_path, shp_str, l_values):\n",
    "\n",
    "    '''Calculates mean and stdv for tifs of monthly LAI values'''\n",
    "\n",
    "    # Calculate monthly mean maps (e.g. mean Jan, Feb, etc.)\n",
    "    lai_folder = geo_folder / dataset / 'raw' \n",
    "    lai_files = sorted( glob.glob(str(lai_folder / '*.tif')) ) # Find LAI files\n",
    "    month_files = calculate_monthly_lai_maps(lai_files, temp_path) # Create 12 monthly maps\n",
    "\n",
    "    # Monthly mean, stdev LAI; monthly mean, stdev GVF\n",
    "    for month_file in month_files:\n",
    "        stats = ['mean', 'std']\n",
    "        zonal_out = zonal_stats(str(shp_lump_path), month_file, stats=stats)\n",
    "        scale, offset = read_scale_and_offset(month_file)\n",
    "        scale,offset = read_scale_and_offset(month_file)\n",
    "        l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "    \n",
    "    # Clear temp folder\n",
    "    files_to_remove = os.listdir(temp_path)\n",
    "    for file_to_remove in files_to_remove:\n",
    "        file_remove_path = os.path.join(temp_path, file_to_remove)\n",
    "        if os.path.isfile(file_remove_path):\n",
    "            os.remove(file_remove_path)\n",
    "    \n",
    "    return l_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d63472-1778-4ab1-9eee-11c597265944",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ecf09c0-e431-4dd9-8e93-522ddbc9b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from osgeo import gdal, osr\n",
    "import os\n",
    "import pandas as pd\n",
    "from rasterstats import zonal_stats\n",
    "from scipy.stats import circmean, circstd, skew, kurtosis\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c8d1a1-cf4d-4965-9569-ccca01c136d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_attributes(tif,l_values,l_index):\n",
    "    '''Calculates circular statistics for MERIT Hydro aspect'''\n",
    "\n",
    "    # Get data as a masked array - we know there are no-data values outside the catchment boundaries\n",
    "    aspect = csa.get_geotif_data_as_array(tif)\n",
    "    no_data = get_geotif_nodata_value(tif)\n",
    "    masked_aspect = np.ma.masked_array(aspect, aspect == no_data)\n",
    "\n",
    "    ## Calculate the statistics\n",
    "    l_values.append(masked_aspect.min())\n",
    "    l_index.append(('Topography', 'merit_hydro_aspect_min',  'degrees', 'MERIT Hydro'))\n",
    "    \n",
    "    l_values.append(circmean(masked_aspect,high=360))\n",
    "    l_index.append(('Topography', 'merit_hydro_aspect_mean',  'degrees', 'MERIT Hydro'))\n",
    "\n",
    "    l_values.append(circstd(masked_aspect,high=360))\n",
    "    l_index.append(('Topography', 'merit_hydro_aspect_std',  'degrees', 'MERIT Hydro'))\n",
    "\n",
    "    l_values.append(masked_aspect.max())\n",
    "    l_index.append(('Topography', 'merit_hydro_aspect_max',  'degrees', 'MERIT Hydro'))\n",
    "    \n",
    "    return l_values,l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e848e1-6858-4a5f-aebd-56a13af6d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_river_attributes(riv_str, l_values, l_index, area):\n",
    "    \n",
    "    '''Calculates topographic attributes from a MERIT Hydro Basins river polygon'''\n",
    "\n",
    "    # Load shapefiles\n",
    "    river = gpd.read_file(riv_str)\n",
    "    river = river.set_index('COMID')\n",
    "    \n",
    "    # Raw data\n",
    "    stream_lengths = []\n",
    "    headwaters = river[river['maxup'] == 0] # identify reaches with no upstream\n",
    "    for COMID in headwaters.index:\n",
    "        stream_length = 0\n",
    "        while COMID in river.index:\n",
    "            stream_length += river.loc[COMID]['lengthkm'] # Add the length of the current segment\n",
    "            COMID = river.loc[COMID]['NextDownID'] # Get the downstream reach\n",
    "        stream_lengths.append(stream_length) # If we get here we ran out of downstream IDs\n",
    "    \n",
    "    # Stats\n",
    "    stream_total = river['lengthkm'].sum()\n",
    "    stream_lengths = np.array(stream_lengths)\n",
    "    l_values.append(stream_lengths.min())\n",
    "    l_index.append(('Topography', 'stream_length_min',  'km', 'MERIT Hydro Basins'))\n",
    "    l_values.append(stream_lengths.mean())\n",
    "    l_index.append(('Topography', 'stream_length_mean',  'km', 'MERIT Hydro Basins'))\n",
    "    l_values.append(stream_lengths.max())\n",
    "    l_index.append(('Topography', 'stream_length_max',  'km', 'MERIT Hydro Basins'))\n",
    "    l_values.append(stream_lengths.std())\n",
    "    l_index.append(('Topography', 'stream_length_std',  'km', 'MERIT Hydro Basins'))\n",
    "    l_values.append(stream_total)\n",
    "    l_index.append(('Topography', 'stream_length_total',  'km', 'MERIT Hydro Basins'))\n",
    "    \n",
    "    # Order\n",
    "    l_values.append(river['order'].max())\n",
    "    l_index.append(('Topography', 'steam_order_max',  '-', 'MERIT Hydro Basins'))\n",
    "\n",
    "    # Derived\n",
    "    density = stream_total/area\n",
    "    elongation = 2*np.sqrt(area/np.pi)/stream_lengths.max()\n",
    "    l_values.append(density)\n",
    "    l_index.append(('Topography', 'stream_density',  'km^-1', 'Derived'))\n",
    "    l_values.append(elongation)\n",
    "    l_index.append(('Topography', 'elongation_ratio','-', 'Derived'))\n",
    "    \n",
    "    return l_values,l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4992b34-8064-4536-8be8-89bab8eac544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geotif_nodata_value(tif):\n",
    "    with rasterio.open(tif) as src:\n",
    "        nodata_value = src.nodata\n",
    "    return nodata_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e664b557-93a8-4a10-9151-03441bc91fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_scale_and_offset(tif):\n",
    "    scale,offset = read_scale_and_offset(tif) # just to check we don't have any scale/offset going on\n",
    "    if scale is None: scale = 1 # If scale is undefined that means we simply multiply by 1\n",
    "    if offset is None: offset = 0 # Undefined offset > add 0\n",
    "    if not (scale == 1) and not (offset == 0):\n",
    "        print(f'--- WARNING: check_scale_and_offset(): scale or offset not 1 or 0 respectively.')\n",
    "    return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060773f4-11fd-4e8c-ab53-344abec5d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_dict(source):\n",
    "    '''Contains dictionaries for categorical variables'''\n",
    "    \n",
    "    if source == 'GLCLU 2019':\n",
    "        cat_dict = {1: 'true_desert',\n",
    "                    2: 'semi_arid',\n",
    "                    3: 'dense_short_vegetation',\n",
    "                    4: 'open_tree_cover',\n",
    "                    5: 'dense_tree_cover',\n",
    "                    6: 'tree_cover_gain',\n",
    "                    7: 'tree_cover_loss',\n",
    "                    8: 'salt_pan',\n",
    "                    9: 'wetland_sparse_vegetation',\n",
    "                   10: 'wetland_dense_short_vegetation',\n",
    "                   11: 'wetland_open_tree_cover',\n",
    "                   12: 'wetland_dense_tree_cover',\n",
    "                   13: 'wetland_tree_cover_gain',\n",
    "                   14: 'wetland_tree_cover_loss',\n",
    "                   15: 'ice',\n",
    "                   16: 'water',\n",
    "                   17: 'cropland',\n",
    "                   18: 'built_up',\n",
    "                   19: 'ocean',\n",
    "                   20: 'no_data'}\n",
    "\n",
    "    if source == 'MCD12Q1.061':\n",
    "        cat_dict = {1: 'evergreen_needleleaf_forest',\n",
    "                    2: 'evergreen_broadleaf_forest',\n",
    "                    3: 'deciduous_needleleaf_forest',\n",
    "                    4: 'deciduous_broadleaf_forest',\n",
    "                    5: 'mixed_forest',\n",
    "                    6: 'closed_shrubland',\n",
    "                    7: 'open_shrubland',\n",
    "                    8: 'woody_savanna',\n",
    "                    9: 'savanna',\n",
    "                   10: 'grassland',\n",
    "                   11: 'permanent_wetland',\n",
    "                   12: 'cropland',\n",
    "                   13: 'urban_and_built_up',\n",
    "                   14: 'cropland_natural_mosaic',\n",
    "                   15: 'permanent_snow_ice',\n",
    "                   16: 'barren',\n",
    "                   17: 'water',\n",
    "                  255: 'unclassified'}\n",
    "\n",
    "    if source == 'LGRIP30':\n",
    "        cat_dict = {0: 'water',\n",
    "                    1: 'non_cropland',\n",
    "                    2: 'irrigated_cropland',\n",
    "                    3: 'rainfed_cropland'}\n",
    "    \n",
    "    return cat_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7597647-5d0c-475b-bc59-8e1882e5c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_values_list_with_categorical(l_values, l_index, zonal_out, source, prefix=''):\n",
    "    '''Maps a zonal histogram of categorical classes onto descriptions and adds to lists'''\n",
    "\n",
    "    # Get the category definitions\n",
    "    cat_dict = get_categorical_dict(source)    \n",
    "\n",
    "    # Find the total number of classified pixels\n",
    "    total_pixels = 0\n",
    "    for land_id,count in zonal_out[0].items():\n",
    "        total_pixels += count\n",
    "    \n",
    "    # Loop over all categories and see what we have in this catchment\n",
    "    for land_id,text in cat_dict.items():\n",
    "        land_prct = 0\n",
    "        if land_id in zonal_out[0].keys():\n",
    "            land_prct = zonal_out[0][land_id] / total_pixels\n",
    "        l_values.append(land_prct)\n",
    "        l_index.append(('Land cover', f'{prefix}{text}_fraction', '-', f'{source}'))\n",
    "\n",
    "    return l_values,l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681110fc-128c-4599-b82b-34d42c53dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aridity_and_fraction_snow_from_worldclim(geo_folder, dataset):\n",
    "    \n",
    "    '''Calculates aridity and fraction snow maps from WorldClim data'''\n",
    "\n",
    "    # Find files\n",
    "    clim_folder = geo_folder / dataset / 'raw'\n",
    "    prc_files = sorted( glob.glob(str(clim_folder / 'prec' / '*.tif')) ) # [mm]\n",
    "    pet_files = sorted( glob.glob(str(clim_folder / 'pet' / '*.tif')) ) # [mm]\n",
    "    tmp_files = sorted( glob.glob(str(clim_folder / 'tavg' / '*.tif')) ) # [C]\n",
    "    \n",
    "    # Make the output locations\n",
    "    ari_folder = clim_folder / 'aridity'\n",
    "    ari_folder.mkdir(parents=True, exist_ok=True)\n",
    "    snow_folder = clim_folder / 'fraction_snow'\n",
    "    snow_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Loop over files and calculate aridity\n",
    "    for prc_file, pet_file, tmp_file in zip(prc_files, pet_files, tmp_files):\n",
    "\n",
    "        # Define month\n",
    "        month = prc_file.split('_')[-1].split('.')[0] # 'wc2.1_30s_prec_01.tif' > '01', ..., '12'\n",
    "        month_ix = int(month)-1 # -1 to account for zero-based indexing: Jan value is at index 0, not 1\n",
    "\n",
    "        # Load data\n",
    "        prc_path = clim_folder / 'prec' / prc_file\n",
    "        pet_path = clim_folder / 'pet'  / pet_file\n",
    "        tmp_path = clim_folder / 'tavg' / tmp_file      \n",
    "        prc = get_geotif_data_as_array(prc_path) # [mm]\n",
    "        pet = get_geotif_data_as_array(pet_path) # [mm]\n",
    "        tmp = get_geotif_data_as_array(tmp_path) # [C]\n",
    "\n",
    "        # Calculate variables\n",
    "        snow = np.where(tmp < 0, prc, 0) # get snow first, because this needs precip and we'll (possibly) be updating the precip value below\n",
    "        if (prc == 0).any():\n",
    "            prc[prc == 0] = 1 # add 1 mm to avoid divide by zero errors\n",
    "        ari = pet/prc # [-]\n",
    "        frac_snow = snow/prc # [-]\n",
    "\n",
    "        # Define output file name and write to disk\n",
    "        ari_name = prc_file.replace('prec','aridity')\n",
    "        ari_file = str(ari_folder / ari_name)\n",
    "        write_geotif_sameDomain(prc_path, ari_file, ari)\n",
    "        snow_name = prc_file.replace('prec','fraction_snow')\n",
    "        snow_file = str(snow_folder / snow_name)\n",
    "        write_geotif_sameDomain(prc_path, snow_file, frac_snow)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22da84d-7099-4744-9b09-1ec1dcf270f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonal_stats_unit_conversion(zonal_out, stat_to_convert, variable, scale, offset):\n",
    "    '''Takes a zonal_stats output and converts the units of any variable listed in stat_to_convert'''\n",
    "\n",
    "    # Constants\n",
    "    j_per_kj = 1000 # [J kJ-1]\n",
    "    seconds_per_day = 24*60*60 # [s day-1]\n",
    "\n",
    "    # Keep track of scale and offset\n",
    "    # Update scale and offset to usable values - we get None if scale and offset are 1 and 0 in the GeoTIFF\n",
    "    if scale is None: scale = 1 # If scale is undefined that means we simply multiply by 1\n",
    "    if offset is None: offset = 0 # Undefined offset > add 0\n",
    "\n",
    "    #  We'll need code to handle this if these aren't 1 and 0 respectively\n",
    "    if (scale != 1) or (offset !=0):\n",
    "        print(f'--- ERROR: zonal_stats_unit_conversion(): code needed to deal with scale {scale} and offset {offset}')\n",
    "        return -1\n",
    "\n",
    "    # Select conversion factor\n",
    "    if variable == 'srad':\n",
    "        # From [kJ m-2 day-1] to [W m-2]:\n",
    "        # [kJ m-2 day-1] * 1/[s day-1] * [J kJ-1] = [J m-2 s-1] = [W m-2]\n",
    "        c_factor = 1/seconds_per_day * j_per_kj\n",
    "\n",
    "    # loop over all list elements\n",
    "    for list_id in range(0,len(zonal_out)):\n",
    "        zonal_dict = zonal_out[list_id]\n",
    "\n",
    "        # Loop over dictionary entries\n",
    "        for key,val in zonal_dict.items():\n",
    "            if k in stat_to_convert:\n",
    "                zonal_out[list_id][key] = zonal_out[list_id][key] * c_factor\n",
    "\n",
    "    return zonal_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9b9c7de4-1e98-4626-a041-241145c73a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oudin_pet_from_worldclim(geo_folder, dataset, debug=False):\n",
    "\n",
    "    '''Calculates PET estimates from WorldClim data, using the Oudin (2005; 10.1016/j.jhydrol.2004.08.026) formulation'''\n",
    "\n",
    "    # Constants\n",
    "    lh = 2.45 # latent heat flux, MJ kg-1\n",
    "    rw = 1000 # rho water, kg m-3\n",
    "    days_per_month = [31, 28.25, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31] # days month-1\n",
    "    mm_per_m = 1000 # mm m-1\n",
    "    \n",
    "    # Find files\n",
    "    clim_folder = geo_folder / dataset / 'raw'\n",
    "    srad_files = sorted( glob.glob(str(clim_folder / 'srad' / '*.tif')) ) # website says [kJ m-2 day-1], but paper says [MJ m-2 day-1]\n",
    "    tavg_files = sorted( glob.glob(str(clim_folder / 'tavg' / '*.tif')) ) # C\n",
    "\n",
    "    # Make the output location\n",
    "    pet_folder = clim_folder / 'pet'\n",
    "    pet_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Loop over files and calculate PET\n",
    "    for srad_file, tavg_file in zip(srad_files, tavg_files):\n",
    "\n",
    "        # Define month\n",
    "        month = srad_file.split('_')[-1].split('.')[0] # 'wc2.1_30s_srad_01.tif' > '01', ..., '12'\n",
    "        month_ix = int(month)-1 # -1 to account for zero-based indexing: Jan value is at index 0, not 1\n",
    "        \n",
    "        # Load data\n",
    "        srad_path = clim_folder / 'srad' / srad_file\n",
    "        tavg_path = clim_folder / 'tavg' / tavg_file      \n",
    "        srad = get_geotif_data_as_array(srad_path) / 1000 # [kJ m-2 day-1] / 1000 = [MJ m-2 day-1]\n",
    "        tavg = get_geotif_data_as_array(tavg_path)\n",
    "        \n",
    "        # Oudin et al, 2005, Eq. 3\n",
    "        pet = np.where(tavg+5 > 0, (srad / (lh*rw)) * ((tavg+5)/100) * mm_per_m, 0) # mm day-1\n",
    "        pet_month = pet * days_per_month[month_ix] # mm month-1\n",
    "        if debug: print(f'Calculating monthly PET for month {month} at day-index {month_ix}')\n",
    "\n",
    "        # Define output file name and write to disk\n",
    "        pet_name = srad_file.replace('srad','pet')\n",
    "        pet_file = str(pet_folder / pet_name)\n",
    "        write_geotif_sameDomain(srad_path, pet_file, pet_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e7499d25-ef22-40e4-a506-6bcc351348f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_values_list(l_values, stats, zonal_out, scale, offset):\n",
    "\n",
    "    # Update scale and offset to usable values\n",
    "    if scale is None: scale = 1 # If scale is undefined that means we simply multiply by 1\n",
    "    if offset is None: offset = 0 # Undefined offset > add 0\n",
    "\n",
    "    # We loop through the calculated stats in a pre-determined order:\n",
    "    # 1. min\n",
    "    # 2. mean\n",
    "    # 3. max\n",
    "    # 4. stdev\n",
    "    # 5. ..\n",
    "    if 'min' in stats:  l_values.append(zonal_out[0]['min']  * scale + offset)\n",
    "    if 'mean' in stats: l_values.append(zonal_out[0]['mean'] * scale + offset)\n",
    "    if 'max' in stats:  l_values.append(zonal_out[0]['max']  * scale + offset)\n",
    "    if 'std' in stats:  l_values.append(zonal_out[0]['std']  * scale + offset)\n",
    "\n",
    "    return l_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "382324e2-789c-4e3a-ac08-18d777e689c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_scale_and_offset(geotiff_path):\n",
    "    # Open the GeoTIFF file\n",
    "    dataset = gdal.Open(geotiff_path)\n",
    "\n",
    "    if dataset is None:\n",
    "        raise FileNotFoundError(f\"File not found: {geotiff_path}\")\n",
    "\n",
    "    # Get the scale and offset values\n",
    "    scale = dataset.GetRasterBand(1).GetScale()\n",
    "    offset = dataset.GetRasterBand(1).GetOffset()\n",
    "\n",
    "    # Close the dataset\n",
    "    dataset = None\n",
    "\n",
    "    return scale, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ff0619b3-e43b-418d-adab-2e946c3f08d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lai_files_by_date(files, last_n_years=[], last_n_months=[], last_n_days=[],\n",
    "                                    years=[], months=[], days=[]):\n",
    "\n",
    "    '''Filters list of LAI file names by last n years/months/days and/or by year/month/day x.\n",
    "       Assumes date is given as 'yyyymmdd_*.tif', as part of the filename.\n",
    "       Use years/months/days (input as list) to subset further.'''\n",
    "\n",
    "    # Check inputs\n",
    "    if (last_n_years and last_n_months) or \\\n",
    "       (last_n_years and last_n_days) or \\\n",
    "       (last_n_months and last_n_days):\n",
    "        print('WARNING: filter_lai_files_by_date(): specify only one of last_n_years, last_n_months, last_n_days')\n",
    "        return\n",
    "\n",
    "    # Create a DatetimeIndex from filenames\n",
    "    dates = []\n",
    "    for file in files:\n",
    "        file_name = os.path.basename(file)\n",
    "        yyyymmdd = file_name[0:8]\n",
    "        dates.append(yyyymmdd)\n",
    "    dti = pd.to_datetime(dates,format='%Y%m%d')\n",
    "\n",
    "    # Set the first entry\n",
    "    start_date = dti[0]\n",
    "    \n",
    "    # Find the last entry\n",
    "    last_year  = dti[-1].year\n",
    "    last_month = dti[-1].month\n",
    "    last_day   = dti[-1].day\n",
    "    \n",
    "    # Select the last n entries\n",
    "    if last_n_years:    start_date = dti[-1] - relativedelta(years = last_n_years)\n",
    "    elif last_n_months: start_date = dti[-1] - relativedelta(months = last_n_months)\n",
    "    elif last_n_days:   start_date = dti[-1] - relativedelta(days = last_n_days)\n",
    "    last_n = (dti >= start_date) & (dti <= dti[-1])\n",
    "\n",
    "    # Specify filters to include all if no specific years/months/days were requested\n",
    "    if not years:  years  = list(set(dti.year))  # i.e. filter to include all unique years in dti, \\\n",
    "    if not months: months = list(set(dti.month)) #    else use user input\n",
    "    if not days:   days   = list(set(dti.day))\n",
    "    mask = dti.year.isin(years) & dti.month.isin(months) & dti.day.isin(days)\n",
    "\n",
    "    # Return the filtered list\n",
    "    return [file for file, bool1, bool2 in zip(files,last_n,mask) if bool1 and bool2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e05e737-0a94-4c5d-9ac5-2fb1f06af17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geotif_data_as_array(file, band=1):\n",
    "    ds = gdal.Open(file) # open the file\n",
    "    band = ds.GetRasterBand(band) # get the data band\n",
    "    data = band.ReadAsArray() # convert to numpy array for further manipulation   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a4c52d08-25f2-4771-8632-db225075f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_data_range(data,min,max,replace_with='limit'):\n",
    "\n",
    "    '''Clamps data at min and max values'''\n",
    "\n",
    "    if replace_with =='limit':\n",
    "        data[data<min] = min\n",
    "        data[data>max] = max\n",
    "    else:\n",
    "        data[data<min] = replace_with\n",
    "        data[data>max] = replace_with\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e2028938-66f1-4887-ae77-075606c7dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_geotif_sameDomain(src_file,des_file,des_data):\n",
    "    \n",
    "    # load the source file to get the appropriate attributes\n",
    "    src_ds = gdal.Open(src_file)\n",
    "    \n",
    "    # get the geotransform\n",
    "    des_transform = src_ds.GetGeoTransform()\n",
    "\n",
    "    # Get the scale factor from the source metadata\n",
    "    scale_factor = src_ds.GetRasterBand(1).GetScale()\n",
    "    offset = src_ds.GetRasterBand(1).GetOffset()\n",
    "    \n",
    "    # get the data dimensions\n",
    "    ncols = des_data.shape[1]\n",
    "    nrows = des_data.shape[0]\n",
    "    \n",
    "    # make the file\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    dst_ds = driver.Create(des_file,ncols,nrows,1,gdal.GDT_Float32, options = [ 'COMPRESS=DEFLATE' ])\n",
    "    dst_ds.GetRasterBand(1).WriteArray( des_data )\n",
    "\n",
    "    # Set the scale factor in the destination band, if they were specified in the source\n",
    "    if scale_factor: dst_ds.GetRasterBand(1).SetScale(scale_factor)\n",
    "    if offset: dst_ds.GetRasterBand(1).SetOffset(offset)\n",
    "    \n",
    "    # Set the geotransform\n",
    "    dst_ds.SetGeoTransform(des_transform)\n",
    "\n",
    "    # Set the projection\n",
    "    wkt = src_ds.GetProjection()\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromWkt(wkt)\n",
    "    dst_ds.SetProjection( srs.ExportToWkt() )\n",
    "    \n",
    "    # close files\n",
    "    src_ds = None\n",
    "    des_ds = None\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3c78f63f-6a8b-4c4a-ba36-f783d4afc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monthly_lai_maps(lai_files, des_path):\n",
    "    des_files = []\n",
    "    for month in range(1,13):\n",
    "\n",
    "        # Define valid data range\n",
    "        # See docs, Table 4: https://lpdaac.usgs.gov/documents/926/MOD15_User_Guide_V61.pdf\n",
    "        modis_min = 0\n",
    "        modis_max = 100\n",
    "    \n",
    "        # Get the files we have for this month, for the last n years\n",
    "        #print(f'Processing month {month:02d}')\n",
    "        month_files = filter_lai_files_by_date(lai_files, months=[month])\n",
    "    \n",
    "        # Remove the one file we know is incomplete, 2022-10-16\n",
    "        month_files = [file for file in month_files if '20221016' not in file]\n",
    "        \n",
    "        # Load the data as numpy arrays, stack vertically, and find the mean value (ignoring nan)\n",
    "        data = [get_geotif_data_as_array(file) for file in month_files] # Get data as uint8\n",
    "        stacked = np.dstack(data) # Create a 3D stack\n",
    "        stacked_msk = np.ma.masked_array(stacked, mask=(stacked<modis_min) | (stacked>modis_max)) # Retain valid values only\n",
    "        mean_lai = np.ma.mean(stacked_msk, axis=2)\n",
    "    \n",
    "        # Define the no-data locations\n",
    "        #mean_all = np.nanmean(stacked, axis=2) # Any pixel that consistently has no-data in the source files (>= 249) should have a >= 249 mean\n",
    "        #mean_lai[mean_all >= 249] = mean_all[mean_all >= 249] # Place the no-data values in the new monthly-mean-lai file\n",
    "        \n",
    "        # Define output file name and write to disk\n",
    "        src_file = month_files[0] # We use this to copy over domain, projection, data scaling, etc\n",
    "        des_file = str( des_path / f'month_mean_{month:02d}_MOD_Grid_MOD15A2H_Lai_500m.tif' )\n",
    "        write_geotif_sameDomain(src_file, des_file, mean_lai)\n",
    "        des_files.append(des_file)\n",
    "    return des_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ad7eba19-9572-454f-81d9-6971d032ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to apply circmean to each group\n",
    "# Without this, xarray chokes on dimensions when converting the circmean output back into something with the right month indices\n",
    "def circmean_group(group):\n",
    "    return xr.DataArray(circmean(group, high=360, low=0), name='phi')\n",
    "\n",
    "def circstd_group(group):\n",
    "    return xr.DataArray(circstd(group, high=360, low=0), name='phi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa7fb4-9c67-4ee1-8ff3-046385c4f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing function to update the two main lists we're populating\n",
    "def process_era5_means_to_lists(da, l_values, l_index, var, unit):\n",
    "    '''Takes an xarray data array with monthly means and processes into l_values and l_index lists'''\n",
    "    for month in range(1,13):\n",
    "        val = da.sel(month=month).values.flatten()[0]\n",
    "        txt = (f'Climate', f'{var}_mean_month_{month:02}', f'{unit}', 'ERA5')\n",
    "        l_values += [val] # Needs to be this way because we're appending to a list\n",
    "        l_index  += [txt]\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad326f-a555-4e94-a0dd-7195037869c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to map months to seasons\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'djf'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'mam'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'jja'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'son'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a678b51-a610-4f2b-a4de-ee6ad82eb4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds duration counts in a vector of True/False values\n",
    "def find_durations(condition):\n",
    "    '''Counts the duration(s) of True values in an xarray dataseries'''\n",
    "\n",
    "    previous = False\n",
    "    duration = 0\n",
    "    durations = []\n",
    "    for flag in condition.values:\n",
    "        \n",
    "        # Time step where we reset the count\n",
    "        if not previous and flag:\n",
    "            duration = 0 # New first timestep where condition = True, so duration = 0\n",
    "            previous = True\n",
    "    \n",
    "        # Time step where we're in a sequence of condition = True\n",
    "        if previous and flag:\n",
    "            duration += 1 # Update duration, implicitly retain previous = True by not changing it\n",
    "        \n",
    "        # Time step where we reach the end of a condition = True duration\n",
    "        if previous and not flag:\n",
    "            durations.append(duration) # Save the maximum duration length to list\n",
    "            previous = False # Update previous; duration will be reset next time we encounter a condition = True\n",
    "    \n",
    "        # Time step where we're in a continuation of condition = False\n",
    "        if not previous and not flag:\n",
    "            continue # do nothing\n",
    "    \n",
    "    return np.array(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bce3f5-d19e-4cf1-baf7-7c00ce1381d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds a long-term mean daily maximum temperature in a way that doesn't rely on \n",
    "#  time.dt.dayofyear, because the latter will have Dec-31 as day 365 in non-leap-years,\n",
    "#  and as 366 in leap years. Hence the day-of-year means do not use the same dates for \n",
    "#  a given DoY in leap years as they do in regular years.\n",
    "def create_mean_daily_max_series(ds,var='t'):\n",
    "    '''Finds the long-term mean daily maximum value of a variable'''\n",
    "    \n",
    "    # Create an array of all the month-days we have (e.g. 1949-12-31 00:00 becomes 1231)\n",
    "    month_days_all = ds.time.dt.month * 100 + ds.time.dt.day\n",
    "\n",
    "    # Loop over the unique month-days we have, and find the mean daily maximum value for each\n",
    "    month_days_unique = np.unique(month_days_all)\n",
    "    mean_daily_max = []\n",
    "    for month_day in month_days_unique:\n",
    "        val = ds[var].sel(time=(month_days_all==month_day)).groupby('time.year').max().mean().values\n",
    "        mean_daily_max.append(val)\n",
    "\n",
    "    # Convert the list to an array for further processing\n",
    "    mean_daily_max = np.array(mean_daily_max)\n",
    "\n",
    "    # Extract month_day values from the long xarray DataArray\n",
    "    month_day_values = month_days_all.values\n",
    "    \n",
    "    # Find the indices of each month_day in the unique_month_days array\n",
    "    indices = np.searchsorted(month_days_unique, month_day_values)\n",
    "    \n",
    "    # Use the indices to extract the corresponding data values\n",
    "    corresponding_data_values = mean_daily_max[indices]\n",
    "    \n",
    "    # Create a new DataArray with the corresponding data values\n",
    "    result_array = xr.DataArray(corresponding_data_values, \n",
    "                                coords=month_days_all.coords, \n",
    "                                dims=month_days_all.dims)\n",
    "\n",
    "    return result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42539dd6-391f-4ede-b38a-4ccf695e8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General processing of high/low temperature/precipitation frequency/duration/timing stats\n",
    "def calculate_temp_prcp_stats(var, condition, hilo, l_values,l_index,\n",
    "                              dataset='ERA5', units='hours'):\n",
    "    \n",
    "    '''Calculates frequency (mean) and duration (mean, median, skew, kurtosis) \n",
    "        of temperature/precipitation periods'''\n",
    "\n",
    "    # Constants. We want everything in [days] for consistency with original CAMELS\n",
    "    hours_per_day = 24 # [hours day-1]\n",
    "    days_per_year = 365.25 # [days year-1]\n",
    "\n",
    "    # Calculate frequencies\n",
    "    freq = condition.mean(dim='time') * days_per_year # [-] * [days year-1]\n",
    "    l_values.append(freq.values[0])\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_freq', 'days year^-1', dataset) )\n",
    "    \n",
    "    # Calculate duration statistics\n",
    "    durations = find_durations(condition) # [time steps]\n",
    "    if units == 'hours':\n",
    "        durations = durations / hours_per_day # [days] = [hours] / [hours day-1]\n",
    "    l_values.append(np.mean(durations)) # [days]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_mean', 'days', dataset) ) # Consistency with\n",
    "    l_values.append(np.median(durations)) # [days]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_median', 'days', dataset) )\n",
    "    l_values.append(skew(durations)) # [-]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_skew', '-', dataset) )\n",
    "    l_values.append(kurtosis(durations)) # [-]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_kurtosis', '-', dataset) )\n",
    "\n",
    "    # Calculate timing statistic\n",
    "    condition['season'] = ('time', \n",
    "        [get_season(month) for month in condition['time.month'].values]) # add seasons\n",
    "    max_season_id = condition.groupby('season').sum().argmax(dim='season') # find season with most True values\n",
    "    l_values.append(condition.season[max_season_id].values[0]) # add season abbrev\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_timing', 'season', dataset) )\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8cccc-dc64-4df5-af73-7659280451cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select only whole years in the dataset, if that's possible\n",
    "def subset_dataset_to_max_full_years(ds, time='time') -> xr.Dataset:\n",
    "    '''Takes an xarray dataset and subsets this to the longest stretch of whole years, counting back from the final date'''\n",
    "\n",
    "    # Find start and end years\n",
    "    final_timestamp = pd.Timestamp(ds[time][-1].values)\n",
    "    start_year = ds[time][0].dt.year\n",
    "    final_year = ds[time][-1].dt.year\n",
    "    max_years = (final_year - start_year).values # subtraction returns DataArray, so we need to extract just the array itself\n",
    "    \n",
    "    # Iteratively try years until we have found something that works, starting at longest possible\n",
    "    for duration in range(max_years,-1,-1):\n",
    "\n",
    "        # Calculate the start datetime of the current duration\n",
    "        start_timestamp = final_timestamp - pd.DateOffset(years=duration)\n",
    "        print(f'checking {start_timestamp}')\n",
    "\n",
    "        # Select the subset of the dataset for the current duration\n",
    "        # Note: if either start or final are not part of the time series,\n",
    "        #  this will silently just use whatever is available\n",
    "        subset_ds = ds.sel(time=slice(start_timestamp, final_timestamp))\n",
    "\n",
    "        # Check if we actually selected the duration we requested\n",
    "        subset_start = pd.Timestamp(subset_ds[time][0].values)\n",
    "        if subset_start == start_timestamp:\n",
    "            break # stop searching. We're counting down the durations, so we have the longest possible one now\n",
    "\n",
    "    # Now check if we have selected a zero-year period\n",
    "    # This would imply we have less than a full year of data\n",
    "    # In this case, just return the original data set with a warning\n",
    "    if duration == 0:\n",
    "        print(f'--- WARNING: subset_dataset_to_max_full_years(): Found no full data years. Returning original DataArray')\n",
    "        return ds\n",
    "    else:   \n",
    "        return subset_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ba6ff-0399-44b5-bc39-6f0087556e1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c03abfe-2021-4a17-a995-62ea44df6aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Vegetation</th>\n",
       "      <th>forest_height_2020_mean</th>\n",
       "      <th>m</th>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forest_height_2020_stdev</th>\n",
       "      <th>m</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Topography</th>\n",
       "      <th>dem_mean</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "      <td>100</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dem_stdev</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               01   02   03   04\n",
       "Category   Attribute                Unit                        \n",
       "Vegetation forest_height_2020_mean  m          25   30   35   40\n",
       "           forest_height_2020_stdev m           5    3    3    4\n",
       "Topography dem_mean                 m.a.s.l.  300  400  100  600\n",
       "           dem_stdev                m.a.s.l.   30    1   10    7"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample data\n",
    "l_station_id = ['01', '02', '03', '04']\n",
    "l_values = []\n",
    "l_values.append([25, 5, 300, 30])\n",
    "l_values.append([30, 3, 400, 1])\n",
    "l_values.append([35, 3, 100, 10])\n",
    "l_values.append([40, 4, 600, 7])\n",
    "\n",
    "# Make the dataframe\n",
    "input_dict = dict(zip(l_station_id, l_values))\n",
    "df = pd.DataFrame(input_dict)\n",
    "\n",
    "# Set the index\n",
    "multi_index = pd.MultiIndex.from_tuples([\n",
    "    ('Vegetation', 'forest_height_2020_mean', 'm'),\n",
    "    ('Vegetation', 'forest_height_2020_stdev', 'm'),\n",
    "    ('Topography', 'dem_mean', 'm.a.s.l.'),\n",
    "    ('Topography', 'dem_stdev', 'm.a.s.l.')\n",
    "], names=['Category', 'Attribute', 'Unit'])\n",
    "df.index = multi_index\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f4a4ab7d-5c44-463c-8c31-859f3bdf5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test_attributes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f90a83-2162-4d9d-ac9e-79fc89641910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels-spat-env",
   "language": "python",
   "name": "camels-spat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
