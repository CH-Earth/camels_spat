{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db3e732-426b-4b22-b055-b9395f29fa09",
   "metadata": {},
   "source": [
    "## Calculate attributes\n",
    "Takes prepared geospatial data and computes various attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac7b17d-07d9-4985-bb9c-8508fb2c7554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w4/hh0hkdr92cg8llf8s0l5rzj80000gq/T/ipykernel_48257/3056185177.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "from python_cs_functions import config as cs, attributes as csa\n",
    "from python_cs_functions.delineate import prepare_delineation_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bcba8f-d99c-4058-9de9-23ae255c19e9",
   "metadata": {},
   "source": [
    "### Config handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dfdcc03-f734-4df6-8bee-25ae59f5560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where the config file can be found\n",
    "config_file = '../0_config/config.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa42aa7-cf4d-4c51-8555-05638e2ffc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the required info from the config file\n",
    "data_path            = cs.read_from_config(config_file,'data_path')\n",
    "\n",
    "# CAMELS-spat metadata\n",
    "cs_meta_path = cs.read_from_config(config_file,'cs_basin_path')\n",
    "cs_meta_name = cs.read_from_config(config_file,'cs_meta_name')\n",
    "cs_unusable_name = cs.read_from_config(config_file,'cs_unusable_name')\n",
    "\n",
    "# Basin folder\n",
    "cs_basin_folder = cs.read_from_config(config_file, 'cs_basin_path')\n",
    "basins_path = Path(data_path) / cs_basin_folder\n",
    "\n",
    "# Get the temporary data folder\n",
    "cs_temp_folder = cs.read_from_config(config_file, 'temp_path')\n",
    "temp_path = Path(cs_temp_folder)\n",
    "temp_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933dae9c-b5fe-42d2-9cc9-2ee54661e2c7",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "333bd63a-b0a1-417d-b9c6-d28306a0120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAMELS-spat metadata file\n",
    "cs_meta_path = Path(data_path) / cs_meta_path\n",
    "cs_meta = pd.read_csv(cs_meta_path / cs_meta_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a497a0-a9a2-404c-9b2a-8173cbb1436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open list of unusable stations; Enforce reading IDs as string to keep leading 0's\n",
    "cs_unusable = pd.read_csv(cs_meta_path / cs_unusable_name, dtype={'Station_id': object})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87113cc-bfb3-48dd-bae1-4c7f19fd39b3",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3185dc90-2a46-46b2-bdd1-c0032c2c452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_message = f'\\n!!! CHECK DEBUGGING STATUS: \\n- Testing 1 file \\n- Testing 1 basin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c395a8-7e5d-4140-b61d-65581fc05a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_subfolders = ['worldclim', 'forest_height', 'lai'] #'era5', \n",
    "# 'glclu2019', 'glhymps', 'hydrolakes','lgrip30','merit','modis_land','pelletier','soilgrids', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11e6b92c-35d3-4358-a3fe-0daec6cd96f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every attribute needs a list, so that we can efficiently construct a dataframe later\n",
    "l_gauges = [] # station ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b44331c-4cd6-4b2a-ba99-db3a767f9ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!!! CHECK DEBUGGING STATUS: \n",
      "- Testing 1 file \n",
      "- Testing 1 basin\n",
      "Processing geospatial data into attributes for CAN_01AD002\n",
      " - processing worldclim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wmk934/data/CAMELS_spat/camels-spat-env/lib/python3.11/site-packages/rasterstats/io.py:328: NodataWarning: Setting nodata to -999; specify nodata explicitly\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - processing forest_height\n",
      " - processing lai\n",
      "\n",
      "!!! CHECK DEBUGGING STATUS: \n",
      "- Testing 1 file \n",
      "- Testing 1 basin\n"
     ]
    }
   ],
   "source": [
    "print(debug_message)\n",
    "for ix,row in cs_meta.iterrows():\n",
    "\n",
    "    # DEBUGGING\n",
    "    if ix != 0: continue\n",
    "\n",
    "    # Get the paths\n",
    "    basin_id, shp_lump_path, shp_dist_path, _, _ = prepare_delineation_outputs(cs_meta, ix, basins_path)\n",
    "    geo_folder = basins_path / 'basin_data' / basin_id / 'geospatial'\n",
    "    met_folder = basins_path / 'basin_data' / basin_id / 'forcing'\n",
    "\n",
    "    # Data storage\n",
    "    l_gauges.append(basin_id) # Update the Station list\n",
    "    l_values = [] # Initialize an empty list where we'll store this basin's attributes\n",
    "    l_index = [] # Initialize an empty list where we'll store the attribute descriptions\n",
    "\n",
    "    # Load the shapefile\n",
    "    #shp = gpd.read_file(shp_lump_path)\n",
    "    #shp_dist = gpd.read_file( Path(str(shp_dist_path).format('basin')) )\n",
    "    shp = str(shp_lump_path) # because zonalstats wants a file path, not a geodataframe\n",
    "    \n",
    "    # Data-specific processing\n",
    "    print(f'Processing geospatial data into attributes for {basin_id}')\n",
    "    for dataset in geo_subfolders:\n",
    "        print(f' - processing {dataset}')\n",
    "\n",
    "        ## CLIMATE\n",
    "        if dataset == 'era5':\n",
    "            l_values, l_index = csa.attributes_from_era5(met_folder, shp, 'era5', l_values, l_index)                                \n",
    "        if dataset == 'worldclim':\n",
    "            csa.oudin_pet_from_worldclim(geo_folder, dataset) # Get an extra PET estimate to sanity check ERA5 outcomes\n",
    "            csa.aridity_and_fraction_snow_from_worldclim(geo_folder, dataset) # Get monthly aridity and fraction snow maps\n",
    "            l_values, l_index = csa.attributes_from_worldclim(geo_folder, dataset, shp, l_values, l_index)\n",
    "\n",
    "        ## VEGETATION\n",
    "        if dataset == 'forest_height':\n",
    "            l_values, l_index = csa.attributes_from_forest_height(geo_folder, dataset, shp, l_values, l_index)\n",
    "        if dataset == 'lai':\n",
    "            l_values, l_index = csa.attributes_from_lai(geo_folder, dataset, temp_path, shp, l_values, l_index)\n",
    "        \n",
    "\n",
    "        ## LAND COVER\n",
    "        # root depth from IGBP, ala A\n",
    "\n",
    "print(debug_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d197b84-b519-487b-aaf4-b14929671829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(272, 272)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l_values),len(l_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f740e80-faf5-4c56-a379-48b3fb2b6b5f",
   "metadata": {},
   "source": [
    "#### Make the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f038ae6-5dff-43cf-a462-8d31eea3d39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>CAN_01AD002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Climate</th>\n",
       "      <th>prec_mean_month_01</th>\n",
       "      <th>mm</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>76.048027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prec_std_month_01</th>\n",
       "      <th>mm</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>6.384933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prec_mean_month_02</th>\n",
       "      <th>mm</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>62.119127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prec_std_month_02</th>\n",
       "      <th>mm</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>6.596292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prec_mean_month_03</th>\n",
       "      <th>mm</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>72.676307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Vegetation</th>\n",
       "      <th>lai_stdev_month_10</th>\n",
       "      <th>m^2 m^-2</th>\n",
       "      <th>MCD15A2H.061</th>\n",
       "      <td>0.433981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lai_mean_month_11</th>\n",
       "      <th>m^2 m^-2</th>\n",
       "      <th>MCD15A2H.061</th>\n",
       "      <td>0.802690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lai_stdev_month_11</th>\n",
       "      <th>m^2 m^-2</th>\n",
       "      <th>MCD15A2H.061</th>\n",
       "      <td>0.350392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lai_mean_month_12</th>\n",
       "      <th>m^2 m^-2</th>\n",
       "      <th>MCD15A2H.061</th>\n",
       "      <td>1.019866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lai_stdev_month_12</th>\n",
       "      <th>m^2 m^-2</th>\n",
       "      <th>MCD15A2H.061</th>\n",
       "      <td>0.591287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>272 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     CAN_01AD002\n",
       "Category   Attribute          Unit     Source                   \n",
       "Climate    prec_mean_month_01 mm       WorldClim       76.048027\n",
       "           prec_std_month_01  mm       WorldClim        6.384933\n",
       "           prec_mean_month_02 mm       WorldClim       62.119127\n",
       "           prec_std_month_02  mm       WorldClim        6.596292\n",
       "           prec_mean_month_03 mm       WorldClim       72.676307\n",
       "...                                                          ...\n",
       "Vegetation lai_stdev_month_10 m^2 m^-2 MCD15A2H.061     0.433981\n",
       "           lai_mean_month_11  m^2 m^-2 MCD15A2H.061     0.802690\n",
       "           lai_stdev_month_11 m^2 m^-2 MCD15A2H.061     0.350392\n",
       "           lai_mean_month_12  m^2 m^-2 MCD15A2H.061     1.019866\n",
       "           lai_stdev_month_12 m^2 m^-2 MCD15A2H.061     0.591287\n",
       "\n",
       "[272 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with a fake second station\n",
    "l_gauges = ['CAN_01AD002','CAN_01AD003']\n",
    "\n",
    "# Make the dataframe\n",
    "input_dict = dict(zip(l_gauges, [l_values,l_values]))\n",
    "df = pd.DataFrame(input_dict)\n",
    "\n",
    "# Set the index\n",
    "multi_index = pd.MultiIndex.from_tuples(l_index, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "df.index = multi_index\n",
    "\n",
    "# Drop the fake extra column\n",
    "df = df.drop(columns=['CAN_01AD003'], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b27b6090-0a20-48aa-8c85-0a0c5e3ceff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>CAN_01AD002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"24\" valign=\"top\">Climate</th>\n",
       "      <th>tavg_mean_month_01</th>\n",
       "      <th>C</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>-12.009491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_02</th>\n",
       "      <th>C</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>-10.300385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_03</th>\n",
       "      <th>C</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>-4.774078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_04</th>\n",
       "      <th>C</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>1.900391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_05</th>\n",
       "      <th>C</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>9.495361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_06</th>\n",
       "      <th>C</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>15.152039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_07</th>\n",
       "      <th>C</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>17.933105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_08</th>\n",
       "      <th>C</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>16.772797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_09</th>\n",
       "      <th>C</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>12.28125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_10</th>\n",
       "      <th>C</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>5.706238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_11</th>\n",
       "      <th>C</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>-1.107147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_12</th>\n",
       "      <th>C</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>-8.796722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_01</th>\n",
       "      <th>C</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>-13.865517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_02</th>\n",
       "      <th>C</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>-11.883213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_03</th>\n",
       "      <th>C</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>-5.758489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_04</th>\n",
       "      <th>C</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>1.46014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_05</th>\n",
       "      <th>C</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>9.16127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_06</th>\n",
       "      <th>C</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>14.533471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_07</th>\n",
       "      <th>C</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>17.432329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_08</th>\n",
       "      <th>C</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>16.394973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_09</th>\n",
       "      <th>C</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>11.304585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_10</th>\n",
       "      <th>C</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>5.141538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_11</th>\n",
       "      <th>C</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>-1.611484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean_month_12</th>\n",
       "      <th>C</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>-9.8653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           CAN_01AD002\n",
       "Category Attribute          Unit Source               \n",
       "Climate  tavg_mean_month_01 C    ERA5       -12.009491\n",
       "         tavg_mean_month_02 C    ERA5       -10.300385\n",
       "         tavg_mean_month_03 C    ERA5        -4.774078\n",
       "         tavg_mean_month_04 C    ERA5         1.900391\n",
       "         tavg_mean_month_05 C    ERA5         9.495361\n",
       "         tavg_mean_month_06 C    ERA5        15.152039\n",
       "         tavg_mean_month_07 C    ERA5        17.933105\n",
       "         tavg_mean_month_08 C    ERA5        16.772797\n",
       "         tavg_mean_month_09 C    ERA5         12.28125\n",
       "         tavg_mean_month_10 C    ERA5         5.706238\n",
       "         tavg_mean_month_11 C    ERA5        -1.107147\n",
       "         tavg_mean_month_12 C    ERA5        -8.796722\n",
       "         tavg_mean_month_01 C    WorldClim  -13.865517\n",
       "         tavg_mean_month_02 C    WorldClim  -11.883213\n",
       "         tavg_mean_month_03 C    WorldClim   -5.758489\n",
       "         tavg_mean_month_04 C    WorldClim     1.46014\n",
       "         tavg_mean_month_05 C    WorldClim     9.16127\n",
       "         tavg_mean_month_06 C    WorldClim   14.533471\n",
       "         tavg_mean_month_07 C    WorldClim   17.432329\n",
       "         tavg_mean_month_08 C    WorldClim   16.394973\n",
       "         tavg_mean_month_09 C    WorldClim   11.304585\n",
       "         tavg_mean_month_10 C    WorldClim    5.141538\n",
       "         tavg_mean_month_11 C    WorldClim   -1.611484\n",
       "         tavg_mean_month_12 C    WorldClim     -9.8653"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of selection code\n",
    "tmp = df.loc[df.index.get_level_values('Attribute').str.contains('tavg') & \n",
    "             df.index.get_level_values('Attribute').str.contains('mean')].copy()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127feb5-ed86-424b-8367-d4ec38177968",
   "metadata": {},
   "source": [
    "## DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "7a7975ae-61d7-4abc-9d65-17e632a0c033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/wmk934/data/CAMELS_spat/camels-spat-data/basin_data/CAN_01AD002/geospatial')"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'worldclim'\n",
    "geo_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "06951678-f661-424e-927d-a80deed39e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aridity_and_fraction_snow_from_worldclim(geo_folder,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "591fc49c-e207-4d52-8a6b-7daf7ecf80fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aridity_and_fraction_snow_from_worldclim(geo_folder, dataset):\n",
    "    \n",
    "    '''Calculates aridity and fraction snow maps from WorldClim data'''\n",
    "\n",
    "    # Find files\n",
    "    clim_folder = geo_folder / dataset / 'raw'\n",
    "    prc_files = sorted( glob.glob(str(clim_folder / 'prec' / '*.tif')) ) # [mm]\n",
    "    pet_files = sorted( glob.glob(str(clim_folder / 'pet' / '*.tif')) ) # [mm]\n",
    "    tmp_files = sorted( glob.glob(str(clim_folder / 'tavg' / '*.tif')) ) # [C]\n",
    "    \n",
    "    # Make the output locations\n",
    "    ari_folder = clim_folder / 'aridity'\n",
    "    ari_folder.mkdir(parents=True, exist_ok=True)\n",
    "    snow_folder = clim_folder / 'fraction_snow'\n",
    "    snow_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Loop over files and calculate aridity\n",
    "    for prc_file, pet_file, tmp_file in zip(prc_files, pet_files, tmp_files):\n",
    "\n",
    "        # Define month\n",
    "        month = prc_file.split('_')[-1].split('.')[0] # 'wc2.1_30s_prec_01.tif' > '01', ..., '12'\n",
    "        month_ix = int(month)-1 # -1 to account for zero-based indexing: Jan value is at index 0, not 1\n",
    "\n",
    "        # Load data\n",
    "        prc_path = clim_folder / 'prec' / prc_file\n",
    "        pet_path = clim_folder / 'pet'  / pet_file\n",
    "        tmp_path = clim_folder / 'tavg' / tmp_file      \n",
    "        prc = get_geotif_data_as_array(prc_path) # [mm]\n",
    "        pet = get_geotif_data_as_array(pet_path) # [mm]\n",
    "        tmp = get_geotif_data_as_array(tmp_path) # [C]\n",
    "\n",
    "        # Calculate variables\n",
    "        snow = np.where(tmp < 0, prc, 0) # get snow first, because this needs precip and we'll (possibly) be updating the precip value below\n",
    "        if (prc == 0).any():\n",
    "            prc[prc == 0] = 1 # add 1 mm to avoid divide by zero errors\n",
    "        ari = pet/prc # [-]\n",
    "        frac_snow = snow/prc # [-]\n",
    "\n",
    "        # Define output file name and write to disk\n",
    "        ari_name = prc_file.replace('prec','aridity')\n",
    "        ari_file = str(ari_folder / ari_name)\n",
    "        write_geotif_sameDomain(prc_path, ari_file, ari)\n",
    "        snow_name = prc_file.replace('prec','fraction_snow')\n",
    "        snow_file = str(snow_folder / snow_name)\n",
    "        write_geotif_sameDomain(prc_path, snow_file, frac_snow)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd30bf-7f36-4a08-89bb-cc86515a05ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### High-level collection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "101eb959-c4a2-41dc-8e60-7694dca0de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_era5(met_folder, shp_path, dataset, l_values, l_index, use_mfdataset=False):\n",
    "\n",
    "    '''Calculates a variety of metrics from ERA5 data'''\n",
    "\n",
    "    # Define various conversion constants\n",
    "    water_density = 1000 # kg m-3\n",
    "    mm_per_m = 1000 # mm m-1\n",
    "    seconds_per_hour = 60*60 # s h-1\n",
    "    seconds_per_day = seconds_per_hour*24 # s d-1\n",
    "    days_per_month = np.array([31, 28.25, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]).reshape(-1, 1) # d month-1\n",
    "    flip_sign = -1 # -; used to convert PET from negative (by convention this indicates an upward flux) to positive\n",
    "    kelvin_to_celsius = -273.15\n",
    "    pa_per_kpa = 1000 # Pa kPa-1\n",
    "\n",
    "    # Define file locations, depending on if we are dealing with lumped or distributed cases\n",
    "    if 'lumped' in shp_path:\n",
    "        era_folder = met_folder / 'lumped'\n",
    "    elif 'distributed' in shp_path:\n",
    "        era_folder = met_folder / 'distributed'\n",
    "    era_files = sorted( glob.glob( str(era_folder / 'ERA5_*.nc') ) )\n",
    "\n",
    "    # Open the data\n",
    "    if use_mfdataset:\n",
    "        ds = xr.open_mfdataset(era_files, engine='netcdf4')\n",
    "        ds = ds.load() # load the whole thing into memory instead of lazy-loading\n",
    "    else:\n",
    "        ds = xr.merge([xr.open_dataset(f) for f in era_files])\n",
    "        ds = ds.load()\n",
    "    \n",
    "    # Select whole years only\n",
    "    #   This avoids issues in cases where we have incomplete whole data years\n",
    "    #   (e.g. 2000-06-01 to 2007-12-31) in basins with very seasonal weather\n",
    "    #   (e.g. all precip occurs in Jan, Feb, Mar). By using only full years\n",
    "    #   we avoid accidentally biasing the attributes.\n",
    "    ds = subset_dataset_to_max_full_years(ds)\n",
    "    \n",
    "    # --- Monthly attributes\n",
    "    # Calculate monthly PET in mm\n",
    "    #      kg m-2 s-1 / kg m-3\n",
    "    # mm month-1 = kg m-2 s-1 * kg-1 m3 * s d-1 * d month-1 * mm m-1 * -\n",
    "    monthly_mper = ds['mper'].resample(time='1ME').mean().groupby('time.month') \n",
    "    mper_m = monthly_mper.mean() / water_density * seconds_per_day * days_per_month * mm_per_m * flip_sign  # [kg m-2 s-1] to [mm month-1]; negative to indicate upward flux\n",
    "    mper_s = monthly_mper.std() / water_density * seconds_per_day * days_per_month * mm_per_m  # [kg m-2 s-1] to [mm month-1]; negative to indicate upward flux\n",
    "    l_values, l_index = process_era5_means_to_lists(mper_m, 'mean', l_values, l_index, 'mper', 'mm')\n",
    "    l_values, l_index = process_era5_means_to_lists(mper_s, 'std', l_values, l_index, 'mper', 'mm')\n",
    "        \n",
    "    # Same for precipitation: [mm month-1]\n",
    "    monthly_mtpr = ds['mtpr'].resample(time='1ME').mean().groupby('time.month')\n",
    "    mtpr_m = monthly_mtpr.mean() / water_density * seconds_per_day * days_per_month * mm_per_m # [kg m-2 s-1] to [mm month-1]\n",
    "    mtpr_s = monthly_mtpr.std() / water_density * seconds_per_day * days_per_month * mm_per_m # [kg m-2 s-1] to [mm month-1]\n",
    "    l_values, l_index = process_era5_means_to_lists(mtpr_m, 'mean', l_values, l_index, 'mtpr', 'mm')\n",
    "    l_values, l_index = process_era5_means_to_lists(mtpr_s, 'std', l_values, l_index, 'mtpr', 'mm')\n",
    "    \n",
    "    # Monthly temperature statistics [C]\n",
    "    monthly_tavg = (ds['t'].resample(time='1D').mean().resample(time='1ME').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tavg_m = monthly_tavg.mean()\n",
    "    tavg_s = monthly_tavg.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(tavg_m, 'mean', l_values, l_index, 'tdavg', 'C')\n",
    "    l_values, l_index = process_era5_means_to_lists(tavg_s, 'std', l_values, l_index, 'tdavg', 'C')\n",
    "    \n",
    "    monthly_tmin = (ds['t'].resample(time='1D').min().resample(time='1ME').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tmin_m = monthly_tmin.mean()\n",
    "    tmin_s = monthly_tmin.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(tmin_m, 'mean', l_values, l_index, 'tdmin', 'C')\n",
    "    l_values, l_index = process_era5_means_to_lists(tmin_m, 'std', l_values, l_index, 'tdmin', 'C')\n",
    "    \n",
    "    monthly_tmax = (ds['t'].resample(time='1D').max().resample(time='1ME').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tmax_m = monthly_tmax.mean()\n",
    "    tmax_s = monthly_tmax.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(tmax_m, 'mean', l_values, l_index, 'tdmax', 'C')\n",
    "    l_values, l_index = process_era5_means_to_lists(tmax_s, 'std', l_values, l_index, 'tdmax', 'C')\n",
    "    \n",
    "    # Monthly shortwave and longwave [W m-2]\n",
    "    monthly_sw = ds['msdwswrf'].resample(time='1ME').mean().groupby('time.month')\n",
    "    sw_m = monthly_sw.mean()\n",
    "    sw_s = monthly_sw.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(sw_m, 'mean', l_values, l_index, 'msdwswrf', 'W m^-2')\n",
    "    l_values, l_index = process_era5_means_to_lists(sw_s, 'std', l_values, l_index, 'msdwswrf', 'W m^-2')\n",
    "    \n",
    "    monthly_lw = ds['msdwlwrf'].resample(time='1ME').mean().groupby('time.month')\n",
    "    lw_m = monthly_lw.mean(dim='time')\n",
    "    lw_s = monthly_lw.std(dim='time')\n",
    "    l_values, l_index = process_era5_means_to_lists(lw_m, 'mean', l_values, l_index, 'msdwlwrf', 'W m^-2')\n",
    "    l_values, l_index = process_era5_means_to_lists(lw_s, 'std', l_values, l_index, 'msdwlwrf', 'W m^-2')\n",
    "\n",
    "    # Surface pressure [Pa]\n",
    "    monthly_sp = ds['sp'].resample(time='1ME').mean().groupby('time.month')\n",
    "    sp_m = monthly_sp.mean() / pa_per_kpa # [Pa] > [kPa]\n",
    "    sp_s = monthly_sp.std() / pa_per_kpa\n",
    "    l_values, l_index = process_era5_means_to_lists(sp_m, 'mean', l_values, l_index, 'sp', 'kPa')\n",
    "    l_values, l_index = process_era5_means_to_lists(sp_s, 'std', l_values, l_index, 'sp', 'kPa')\n",
    "    \n",
    "    # Humidity [-]\n",
    "    monthly_q = ds['q'].resample(time='1ME').mean().groupby('time.month') # specific\n",
    "    q_m = monthly_q.mean()\n",
    "    q_s = monthly_q.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(q_m, 'mean', l_values, l_index, 'q', 'kg kg^-1')\n",
    "    l_values, l_index = process_era5_means_to_lists(q_s, 'std', l_values, l_index, 'q', 'kg kg^-1')\n",
    "    \n",
    "    monthly_rh = ds['rh'].resample(time='1ME').mean().groupby('time.month') # relative\n",
    "    rh_m = monthly_rh.mean()\n",
    "    rh_s = monthly_rh.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(rh_m, 'mean', l_values, l_index, 'rh', 'kPa kPa^-1')\n",
    "    l_values, l_index = process_era5_means_to_lists(rh_s, 'std', l_values, l_index, 'rh', 'kPa kPa^-1')\n",
    "    \n",
    "    # Wind speed [m s-1]\n",
    "    monthly_w = ds['w'].resample(time='1ME').mean().groupby('time.month')\n",
    "    w_m = monthly_w.mean()\n",
    "    w_s = monthly_w.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(w_m, 'mean', l_values, l_index, 'w', 'm s^-1')\n",
    "    l_values, l_index = process_era5_means_to_lists(w_s, 'std', l_values, l_index, 'w', 'm s^-1')\n",
    "    \n",
    "    # Wind direction\n",
    "    monthly_phi = ds['phi'].resample(time='1ME').apply(circmean_group).groupby('time.month')\n",
    "    phi_m = monthly_phi.apply(circmean_group)\n",
    "    phi_s = monthly_phi.apply(circstd_group)\n",
    "    l_values, l_index = process_era5_means_to_lists(phi_m, 'mean', l_values, l_index, 'phi', 'degrees')\n",
    "    l_values, l_index = process_era5_means_to_lists(phi_s, 'std', l_values, l_index, 'phi', 'degrees')\n",
    "    \n",
    "    # --- Long-term statistics (aridity, seasonality, snow)\n",
    "    monthly_ari = ((ds['mper'].resample(time='1ME').mean() * flip_sign) / ds['mtpr'].resample(time='1ME').mean()).groupby('time.month')\n",
    "    ari_m = monthly_ari.mean()\n",
    "    ari_s = monthly_ari.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(ari_m, 'mean', l_values, l_index, 'aridity', '-')\n",
    "    l_values, l_index = process_era5_means_to_lists(ari_s, 'std', l_values, l_index, 'aridity', '-')\n",
    "\n",
    "    ds['snow'] = xr.where(ds['t'] < 273.15, ds['mtpr'],0)\n",
    "    monthly_snow = (ds['snow'].resample(time='1ME').mean() / ds['mtpr'].resample(time='1ME').mean()).groupby('time.month')\n",
    "    snow_m = monthly_snow.mean()\n",
    "    snow_s = monthly_snow.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(snow_m, 'mean', l_values, l_index, 'fracsnow', '-')\n",
    "    l_values, l_index = process_era5_means_to_lists(snow_s, 'std', l_values, l_index, 'fracsnow', '-')\n",
    "\n",
    "    # --- High-frequency statistics (high/low duration/timing/magnitude)\n",
    "    #  Everyone does precip. We'll add temperature too as a drought/frost indicator\n",
    "    #  ERA5 only\n",
    "    \n",
    "    # -- LOW TEMPERATURE\n",
    "    variable  = 't'\n",
    "    low_threshold = 273.15 # K, freezing point\n",
    "    low_condition = ds[variable] < low_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('temp',low_condition,'low',l_values,l_index)\n",
    "    \n",
    "    # -- HIGH TEMPERATURE\n",
    "    # WMO defines a heat wave as a 5-day or longer period with maximum daily temperatures 5C above \n",
    "    # \"standard\" daily max temperature (1961-1990; source:\n",
    "    # https://www.ifrc.org/sites/default/files/2021-06/10-HEAT-WAVE-HR.pdf).\n",
    "    # We define a \"hot day\" therefore as a day with a maximum temperature 5 degrees over the \n",
    "    # the long-term mean maximum temperature.\n",
    "    #   Note: we don't have 1961-1990 data for some stations, so we stick with long-term mean.\n",
    "    #   Note: this will in most cases slightly underestimate heat waves compared to WMO definition\n",
    "    \n",
    "    # First, we identify the long-term mean daily maximum temperature in a dedicated function\n",
    "    high_threshold = create_mean_daily_max_series(ds,var='t')\n",
    "    \n",
    "    # Next, we check if which 't' values are 5 degrees above the long-term mean daily max \n",
    "    #  (\"(ds['t'] > result_array + 5)\"), and resample this to a daily time series \n",
    "    #  (\"resample(time='1D')\") filled with \"True\" if any value in that day was True.\n",
    "    daily_flags = (ds['t'] > high_threshold + 5).resample(time='1D').any()\n",
    "    \n",
    "    # Finally, we reindex these daily flags back onto the hourly time series by filling values\n",
    "    high_condition = daily_flags.reindex_like(ds['t'], method='ffill')\n",
    "    \n",
    "    # Now calculate stats like before\n",
    "    l_values,l_index = calculate_temp_prcp_stats('temp',high_condition,'high',l_values,l_index)\n",
    "    \n",
    "    # -- LOW PRECIPITATION\n",
    "    variable = 'mtpr'\n",
    "    # We'll stick with the original CAMELS definition of low precipitation: < 1 mm day-1\n",
    "    # It may not make too much sense to look at \"dry hours\" so we'll do this analysis at daily step\n",
    "    low_threshold = 1 # [mm d-1]\n",
    "    # Create daily precipitation sum (divided by density, times mm m-1 cancels out)\n",
    "    # [kg m-2 s-1] * [s h-1] / [kg m-3] * [mm m-1] = [mm h-1]\n",
    "    low_condition = (ds[variable] * seconds_per_hour).resample(time='1D').sum() < low_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('prec',low_condition,'low',l_values,l_index,\n",
    "                                             units='days') # this 'units' argument prevents conversion to days inside the functiom\n",
    "    \n",
    "    # -- HIGH PRECIPITATION\n",
    "    # CAMELS: > 5 times mean daily precip\n",
    "    high_threshold = 5 * (ds[variable] * seconds_per_hour).resample(time='1D').sum().mean()\n",
    "    high_condition = (ds[variable] * seconds_per_hour).resample(time='1D').sum() >= high_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('prec',high_condition,'high',l_values,l_index,\n",
    "                                                 units='days')\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3686d712-b932-4cb7-b153-0bfaab2feb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_worldclim(geo_folder, dataset, shp_str, l_values, index):\n",
    "\n",
    "    '''Calculates mean and stdv for tifs of monthly WorldClim values'''\n",
    "\n",
    "    # Define file locations\n",
    "    # Units source: https://www.worldclim.org/data/worldclim21.html\n",
    "    clim_folder = geo_folder / dataset / 'raw'\n",
    "    sub_folders =      ['prec', 'srad',         'tavg', 'tmax', 'tmin', 'vapr', 'wind',   'pet']\n",
    "    sub_folder_units = ['mm',   'kJ m^-2 d^-1', 'C',    'C',    'C',    'kPa',  'm s^-1', 'mm']\n",
    "\n",
    "    # Loop over the files and calculate the stats\n",
    "    for sub_folder, sub_folder_unit in zip(sub_folders, sub_folder_units):\n",
    "        month_files = sorted( glob.glob(str(clim_folder / sub_folder / '*.tif')) )\n",
    "        for month_file in month_files:\n",
    "            month_file = clim_folder / sub_folder / month_file # Construct the full path, because listdir() gives only files\n",
    "            stats = ['mean', 'std']\n",
    "            zonal_out = zonal_stats(shp_str, month_file, stats=stats)\n",
    "\n",
    "            scale, offset = csa.read_scale_and_offset(month_file)\n",
    "            scale, offset = read_scale_and_offset(month_file)\n",
    "            if sub_folder == 'srad':\n",
    "                zonal_out = zonal_stats_unit_conversion(zonal_out,stats,'srad', scale, offset)\n",
    "            l_values = csa.update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "            \n",
    "            month = os.path.basename(month_file).split('_')[3].split('.')[0]\n",
    "            var = os.path.basename(month_file).split('_')[2]\n",
    "            source = 'WorldClim'\n",
    "            if var == 'pet': source = 'WorldClim (derived, Oudin et al., 2005)'\n",
    "            index += [('Climate', f'{var}_mean_month_{month}', f'{sub_folder_unit}',  source),\n",
    "                      ('Climate', f'{var}_stdev_month_{month}', f'{sub_folder_unit}', source)]\n",
    "\n",
    "    return l_values, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1af9044e-e75b-48a7-815a-c8869f6212b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_forest_height(geo_folder, dataset, shp_str, l_values):\n",
    "\n",
    "    '''Calculates mean, min, max and stdv for forest height 2000 and 2020 tifs'''\n",
    "\n",
    "    # Year 2000 min, mean, max, stdev\n",
    "    tif = str( geo_folder / dataset / 'raw' / 'forest_height_2000.tif' )\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    zonal_out = zonal_stats(shp, tif, stats=stats)\n",
    "    scale,offset = read_scale_and_offset(tif)\n",
    "    l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "\n",
    "    # Year 2020 mean, stdev\n",
    "    tif = geo_folder / dataset / 'raw' / 'forest_height_2020.tif'\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    zonal_out = zonal_stats(shp, tif, stats=stats)\n",
    "    scale,offset = read_scale_and_offset(tif)\n",
    "    l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "\n",
    "    return l_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b4204-865e-45ad-85f5-aff233536d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_lai(geo_folder, dataset, temp_path, shp_str, l_values):\n",
    "\n",
    "    '''Calculates mean and stdv for tifs of monthly LAI values'''\n",
    "\n",
    "    # Calculate monthly mean maps (e.g. mean Jan, Feb, etc.)\n",
    "    lai_folder = geo_folder / dataset / 'raw' \n",
    "    lai_files = sorted( glob.glob(str(lai_folder / '*.tif')) ) # Find LAI files\n",
    "    month_files = calculate_monthly_lai_maps(lai_files, temp_path) # Create 12 monthly maps\n",
    "\n",
    "    # Monthly mean, stdev LAI; monthly mean, stdev GVF\n",
    "    for month_file in month_files:\n",
    "        stats = ['mean', 'std']\n",
    "        zonal_out = zonal_stats(str(shp_lump_path), month_file, stats=stats)\n",
    "        scale, offset = read_scale_and_offset(month_file)\n",
    "        scale,offset = read_scale_and_offset(month_file)\n",
    "        l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "    \n",
    "    # Clear temp folder\n",
    "    files_to_remove = os.listdir(temp_path)\n",
    "    for file_to_remove in files_to_remove:\n",
    "        file_remove_path = os.path.join(temp_path, file_to_remove)\n",
    "        if os.path.isfile(file_remove_path):\n",
    "            os.remove(file_remove_path)\n",
    "    \n",
    "    return l_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d63472-1778-4ab1-9eee-11c597265944",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "5ecf09c0-e431-4dd9-8e93-522ddbc9b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from osgeo import gdal, osr\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import circmean, circstd, skew, kurtosis\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22da84d-7099-4744-9b09-1ec1dcf270f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonal_stats_unit_conversion(zonal_out, stat_to_convert, variable, scale, offset):\n",
    "    '''Takes a zonal_stats output and converts the units of any variable listed in stat_to_convert'''\n",
    "\n",
    "    # Constants\n",
    "    j_per_kj = 1000 # [J kJ-1]\n",
    "    seconds_per_day = 24*60*60 # [s day-1]\n",
    "\n",
    "    # Keep track of scale and offset\n",
    "    # Update scale and offset to usable values - we get None if scale and offset are 1 and 0 in the GeoTIFF\n",
    "    if scale is None: scale = 1 # If scale is undefined that means we simply multiply by 1\n",
    "    if offset is None: offset = 0 # Undefined offset > add 0\n",
    "\n",
    "    #  We'll need code to handle this if these aren't 1 and 0 respectively\n",
    "    if (scale != 1) or (offset !=0):\n",
    "        print(f'--- ERROR: zonal_stats_unit_conversion(): code needed to deal with scale {scale} and offset {offset}')\n",
    "        return -1\n",
    "\n",
    "    # Select conversion factor\n",
    "    if variable == 'srad':\n",
    "        # From [kJ m-2 day-1] to [W m-2]:\n",
    "        # [kJ m-2 day-1] * 1/[s day-1] * [J kJ-1] = [J m-2 s-1] = [W m-2]\n",
    "        c_factor = 1/seconds_per_day * j_per_kj\n",
    "\n",
    "    # loop over all list elements\n",
    "    for list_id in range(0,len(zonal_out)):\n",
    "        zonal_dict = zonal_out[list_id]\n",
    "\n",
    "        # Loop over dictionary entries\n",
    "        for key,val in zonal_dict.items():\n",
    "            if k in stat_to_convert:\n",
    "                zonal_out[list_id][key] = zonal_out[list_id][key] * c_factor\n",
    "\n",
    "    return zonal_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9b9c7de4-1e98-4626-a041-241145c73a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oudin_pet_from_worldclim(geo_folder, dataset, debug=False):\n",
    "\n",
    "    '''Calculates PET estimates from WorldClim data, using the Oudin (2005; 10.1016/j.jhydrol.2004.08.026) formulation'''\n",
    "\n",
    "    # Constants\n",
    "    lh = 2.45 # latent heat flux, MJ kg-1\n",
    "    rw = 1000 # rho water, kg m-3\n",
    "    days_per_month = [31, 28.25, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31] # days month-1\n",
    "    mm_per_m = 1000 # mm m-1\n",
    "    \n",
    "    # Find files\n",
    "    clim_folder = geo_folder / dataset / 'raw'\n",
    "    srad_files = sorted( glob.glob(str(clim_folder / 'srad' / '*.tif')) ) # website says [kJ m-2 day-1], but paper says [MJ m-2 day-1]\n",
    "    tavg_files = sorted( glob.glob(str(clim_folder / 'tavg' / '*.tif')) ) # C\n",
    "\n",
    "    # Make the output location\n",
    "    pet_folder = clim_folder / 'pet'\n",
    "    pet_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Loop over files and calculate PET\n",
    "    for srad_file, tavg_file in zip(srad_files, tavg_files):\n",
    "\n",
    "        # Define month\n",
    "        month = srad_file.split('_')[-1].split('.')[0] # 'wc2.1_30s_srad_01.tif' > '01', ..., '12'\n",
    "        month_ix = int(month)-1 # -1 to account for zero-based indexing: Jan value is at index 0, not 1\n",
    "        \n",
    "        # Load data\n",
    "        srad_path = clim_folder / 'srad' / srad_file\n",
    "        tavg_path = clim_folder / 'tavg' / tavg_file      \n",
    "        srad = get_geotif_data_as_array(srad_path) / 1000 # [kJ m-2 day-1] / 1000 = [MJ m-2 day-1]\n",
    "        tavg = get_geotif_data_as_array(tavg_path)\n",
    "        \n",
    "        # Oudin et al, 2005, Eq. 3\n",
    "        pet = np.where(tavg+5 > 0, (srad / (lh*rw)) * ((tavg+5)/100) * mm_per_m, 0) # mm day-1\n",
    "        pet_month = pet * days_per_month[month_ix] # mm month-1\n",
    "        if debug: print(f'Calculating monthly PET for month {month} at day-index {month_ix}')\n",
    "\n",
    "        # Define output file name and write to disk\n",
    "        pet_name = srad_file.replace('srad','pet')\n",
    "        pet_file = str(pet_folder / pet_name)\n",
    "        write_geotif_sameDomain(srad_path, pet_file, pet_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e7499d25-ef22-40e4-a506-6bcc351348f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_values_list(l_values, stats, zonal_out, scale, offset):\n",
    "\n",
    "    # Update scale and offset to usable values\n",
    "    if scale is None: scale = 1 # If scale is undefined that means we simply multiply by 1\n",
    "    if offset is None: offset = 0 # Undefined offset > add 0\n",
    "\n",
    "    # We loop through the calculated stats in a pre-determined order:\n",
    "    # 1. min\n",
    "    # 2. mean\n",
    "    # 3. max\n",
    "    # 4. stdev\n",
    "    # 5. ..\n",
    "    if 'min' in stats:  l_values.append(zonal_out[0]['min']  * scale + offset)\n",
    "    if 'mean' in stats: l_values.append(zonal_out[0]['mean'] * scale + offset)\n",
    "    if 'max' in stats:  l_values.append(zonal_out[0]['max']  * scale + offset)\n",
    "    if 'std' in stats:  l_values.append(zonal_out[0]['std']  * scale + offset)\n",
    "\n",
    "    return l_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "382324e2-789c-4e3a-ac08-18d777e689c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_scale_and_offset(geotiff_path):\n",
    "    # Open the GeoTIFF file\n",
    "    dataset = gdal.Open(geotiff_path)\n",
    "\n",
    "    if dataset is None:\n",
    "        raise FileNotFoundError(f\"File not found: {geotiff_path}\")\n",
    "\n",
    "    # Get the scale and offset values\n",
    "    scale = dataset.GetRasterBand(1).GetScale()\n",
    "    offset = dataset.GetRasterBand(1).GetOffset()\n",
    "\n",
    "    # Close the dataset\n",
    "    dataset = None\n",
    "\n",
    "    return scale, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ff0619b3-e43b-418d-adab-2e946c3f08d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lai_files_by_date(files, last_n_years=[], last_n_months=[], last_n_days=[],\n",
    "                                    years=[], months=[], days=[]):\n",
    "\n",
    "    '''Filters list of LAI file names by last n years/months/days and/or by year/month/day x.\n",
    "       Assumes date is given as 'yyyymmdd_*.tif', as part of the filename.\n",
    "       Use years/months/days (input as list) to subset further.'''\n",
    "\n",
    "    # Check inputs\n",
    "    if (last_n_years and last_n_months) or \\\n",
    "       (last_n_years and last_n_days) or \\\n",
    "       (last_n_months and last_n_days):\n",
    "        print('WARNING: filter_lai_files_by_date(): specify only one of last_n_years, last_n_months, last_n_days')\n",
    "        return\n",
    "\n",
    "    # Create a DatetimeIndex from filenames\n",
    "    dates = []\n",
    "    for file in files:\n",
    "        file_name = os.path.basename(file)\n",
    "        yyyymmdd = file_name[0:8]\n",
    "        dates.append(yyyymmdd)\n",
    "    dti = pd.to_datetime(dates,format='%Y%m%d')\n",
    "\n",
    "    # Set the first entry\n",
    "    start_date = dti[0]\n",
    "    \n",
    "    # Find the last entry\n",
    "    last_year  = dti[-1].year\n",
    "    last_month = dti[-1].month\n",
    "    last_day   = dti[-1].day\n",
    "    \n",
    "    # Select the last n entries\n",
    "    if last_n_years:    start_date = dti[-1] - relativedelta(years = last_n_years)\n",
    "    elif last_n_months: start_date = dti[-1] - relativedelta(months = last_n_months)\n",
    "    elif last_n_days:   start_date = dti[-1] - relativedelta(days = last_n_days)\n",
    "    last_n = (dti >= start_date) & (dti <= dti[-1])\n",
    "\n",
    "    # Specify filters to include all if no specific years/months/days were requested\n",
    "    if not years:  years  = list(set(dti.year))  # i.e. filter to include all unique years in dti, \\\n",
    "    if not months: months = list(set(dti.month)) #    else use user input\n",
    "    if not days:   days   = list(set(dti.day))\n",
    "    mask = dti.year.isin(years) & dti.month.isin(months) & dti.day.isin(days)\n",
    "\n",
    "    # Return the filtered list\n",
    "    return [file for file, bool1, bool2 in zip(files,last_n,mask) if bool1 and bool2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e05e737-0a94-4c5d-9ac5-2fb1f06af17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geotif_data_as_array(file, band=1):\n",
    "    ds = gdal.Open(file) # open the file\n",
    "    band = ds.GetRasterBand(band) # get the data band\n",
    "    data = band.ReadAsArray() # convert to numpy array for further manipulation   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a4c52d08-25f2-4771-8632-db225075f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_data_range(data,min,max,replace_with='limit'):\n",
    "\n",
    "    '''Clamps data at min and max values'''\n",
    "\n",
    "    if replace_with =='limit':\n",
    "        data[data<min] = min\n",
    "        data[data>max] = max\n",
    "    else:\n",
    "        data[data<min] = replace_with\n",
    "        data[data>max] = replace_with\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e2028938-66f1-4887-ae77-075606c7dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_geotif_sameDomain(src_file,des_file,des_data):\n",
    "    \n",
    "    # load the source file to get the appropriate attributes\n",
    "    src_ds = gdal.Open(src_file)\n",
    "    \n",
    "    # get the geotransform\n",
    "    des_transform = src_ds.GetGeoTransform()\n",
    "\n",
    "    # Get the scale factor from the source metadata\n",
    "    scale_factor = src_ds.GetRasterBand(1).GetScale()\n",
    "    offset = src_ds.GetRasterBand(1).GetOffset()\n",
    "    \n",
    "    # get the data dimensions\n",
    "    ncols = des_data.shape[1]\n",
    "    nrows = des_data.shape[0]\n",
    "    \n",
    "    # make the file\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    dst_ds = driver.Create(des_file,ncols,nrows,1,gdal.GDT_Float32, options = [ 'COMPRESS=DEFLATE' ])\n",
    "    dst_ds.GetRasterBand(1).WriteArray( des_data )\n",
    "\n",
    "    # Set the scale factor in the destination band, if they were specified in the source\n",
    "    if scale_factor: dst_ds.GetRasterBand(1).SetScale(scale_factor)\n",
    "    if offset: dst_ds.GetRasterBand(1).SetOffset(offset)\n",
    "    \n",
    "    # Set the geotransform\n",
    "    dst_ds.SetGeoTransform(des_transform)\n",
    "\n",
    "    # Set the projection\n",
    "    wkt = src_ds.GetProjection()\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromWkt(wkt)\n",
    "    dst_ds.SetProjection( srs.ExportToWkt() )\n",
    "    \n",
    "    # close files\n",
    "    src_ds = None\n",
    "    des_ds = None\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3c78f63f-6a8b-4c4a-ba36-f783d4afc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monthly_lai_maps(lai_files, des_path):\n",
    "    des_files = []\n",
    "    for month in range(1,13):\n",
    "\n",
    "        # Define valid data range\n",
    "        # See docs, Table 4: https://lpdaac.usgs.gov/documents/926/MOD15_User_Guide_V61.pdf\n",
    "        modis_min = 0\n",
    "        modis_max = 100\n",
    "    \n",
    "        # Get the files we have for this month, for the last n years\n",
    "        #print(f'Processing month {month:02d}')\n",
    "        month_files = filter_lai_files_by_date(lai_files, months=[month])\n",
    "    \n",
    "        # Remove the one file we know is incomplete, 2022-10-16\n",
    "        month_files = [file for file in month_files if '20221016' not in file]\n",
    "        \n",
    "        # Load the data as numpy arrays, stack vertically, and find the mean value (ignoring nan)\n",
    "        data = [get_geotif_data_as_array(file) for file in month_files] # Get data as uint8\n",
    "        stacked = np.dstack(data) # Create a 3D stack\n",
    "        stacked_msk = np.ma.masked_array(stacked, mask=(stacked<modis_min) | (stacked>modis_max)) # Retain valid values only\n",
    "        mean_lai = np.ma.mean(stacked_msk, axis=2)\n",
    "    \n",
    "        # Define the no-data locations\n",
    "        #mean_all = np.nanmean(stacked, axis=2) # Any pixel that consistently has no-data in the source files (>= 249) should have a >= 249 mean\n",
    "        #mean_lai[mean_all >= 249] = mean_all[mean_all >= 249] # Place the no-data values in the new monthly-mean-lai file\n",
    "        \n",
    "        # Define output file name and write to disk\n",
    "        src_file = month_files[0] # We use this to copy over domain, projection, data scaling, etc\n",
    "        des_file = str( des_path / f'month_mean_{month:02d}_MOD_Grid_MOD15A2H_Lai_500m.tif' )\n",
    "        write_geotif_sameDomain(src_file, des_file, mean_lai)\n",
    "        des_files.append(des_file)\n",
    "    return des_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ad7eba19-9572-454f-81d9-6971d032ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to apply circmean to each group\n",
    "# Without this, xarray chokes on dimensions when converting the circmean output back into something with the right month indices\n",
    "def circmean_group(group):\n",
    "    return xr.DataArray(circmean(group, high=360, low=0), name='phi')\n",
    "\n",
    "def circstd_group(group):\n",
    "    return xr.DataArray(circstd(group, high=360, low=0), name='phi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa7fb4-9c67-4ee1-8ff3-046385c4f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing function to update the two main lists we're populating\n",
    "def process_era5_means_to_lists(da, l_values, l_index, var, unit):\n",
    "    '''Takes an xarray data array with monthly means and processes into l_values and l_index lists'''\n",
    "    for month in range(1,13):\n",
    "        val = da.sel(month=month).values.flatten()[0]\n",
    "        txt = (f'Climate', f'{var}_mean_month_{month:02}', f'{unit}', 'ERA5')\n",
    "        l_values += [val] # Needs to be this way because we're appending to a list\n",
    "        l_index  += [txt]\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad326f-a555-4e94-a0dd-7195037869c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to map months to seasons\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'djf'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'mam'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'jja'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'son'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a678b51-a610-4f2b-a4de-ee6ad82eb4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds duration counts in a vector of True/False values\n",
    "def find_durations(condition):\n",
    "    '''Counts the duration(s) of True values in an xarray dataseries'''\n",
    "\n",
    "    previous = False\n",
    "    duration = 0\n",
    "    durations = []\n",
    "    for flag in condition.values:\n",
    "        \n",
    "        # Time step where we reset the count\n",
    "        if not previous and flag:\n",
    "            duration = 0 # New first timestep where condition = True, so duration = 0\n",
    "            previous = True\n",
    "    \n",
    "        # Time step where we're in a sequence of condition = True\n",
    "        if previous and flag:\n",
    "            duration += 1 # Update duration, implicitly retain previous = True by not changing it\n",
    "        \n",
    "        # Time step where we reach the end of a condition = True duration\n",
    "        if previous and not flag:\n",
    "            durations.append(duration) # Save the maximum duration length to list\n",
    "            previous = False # Update previous; duration will be reset next time we encounter a condition = True\n",
    "    \n",
    "        # Time step where we're in a continuation of condition = False\n",
    "        if not previous and not flag:\n",
    "            continue # do nothing\n",
    "    \n",
    "    return np.array(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bce3f5-d19e-4cf1-baf7-7c00ce1381d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds a long-term mean daily maximum temperature in a way that doesn't rely on \n",
    "#  time.dt.dayofyear, because the latter will have Dec-31 as day 365 in non-leap-years,\n",
    "#  and as 366 in leap years. Hence the day-of-year means do not use the same dates for \n",
    "#  a given DoY in leap years as they do in regular years.\n",
    "def create_mean_daily_max_series(ds,var='t'):\n",
    "    '''Finds the long-term mean daily maximum value of a variable'''\n",
    "    \n",
    "    # Create an array of all the month-days we have (e.g. 1949-12-31 00:00 becomes 1231)\n",
    "    month_days_all = ds.time.dt.month * 100 + ds.time.dt.day\n",
    "\n",
    "    # Loop over the unique month-days we have, and find the mean daily maximum value for each\n",
    "    month_days_unique = np.unique(month_days_all)\n",
    "    mean_daily_max = []\n",
    "    for month_day in month_days_unique:\n",
    "        val = ds[var].sel(time=(month_days_all==month_day)).groupby('time.year').max().mean().values\n",
    "        mean_daily_max.append(val)\n",
    "\n",
    "    # Convert the list to an array for further processing\n",
    "    mean_daily_max = np.array(mean_daily_max)\n",
    "\n",
    "    # Extract month_day values from the long xarray DataArray\n",
    "    month_day_values = month_days_all.values\n",
    "    \n",
    "    # Find the indices of each month_day in the unique_month_days array\n",
    "    indices = np.searchsorted(month_days_unique, month_day_values)\n",
    "    \n",
    "    # Use the indices to extract the corresponding data values\n",
    "    corresponding_data_values = mean_daily_max[indices]\n",
    "    \n",
    "    # Create a new DataArray with the corresponding data values\n",
    "    result_array = xr.DataArray(corresponding_data_values, \n",
    "                                coords=month_days_all.coords, \n",
    "                                dims=month_days_all.dims)\n",
    "\n",
    "    return result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42539dd6-391f-4ede-b38a-4ccf695e8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General processing of high/low temperature/precipitation frequency/duration/timing stats\n",
    "def calculate_temp_prcp_stats(var, condition, hilo, l_values,l_index,\n",
    "                              dataset='ERA5', units='hours'):\n",
    "    \n",
    "    '''Calculates frequency (mean) and duration (mean, median, skew, kurtosis) \n",
    "        of temperature/precipitation periods'''\n",
    "\n",
    "    # Constants. We want everything in [days] for consistency with original CAMELS\n",
    "    hours_per_day = 24 # [hours day-1]\n",
    "    days_per_year = 365.25 # [days year-1]\n",
    "\n",
    "    # Calculate frequencies\n",
    "    freq = condition.mean(dim='time') * days_per_year # [-] * [days year-1]\n",
    "    l_values.append(freq.values[0])\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_freq', 'days year^-1', dataset) )\n",
    "    \n",
    "    # Calculate duration statistics\n",
    "    durations = find_durations(condition) # [time steps]\n",
    "    if units == 'hours':\n",
    "        durations = durations / hours_per_day # [days] = [hours] / [hours day-1]\n",
    "    l_values.append(np.mean(durations)) # [days]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_mean', 'days', dataset) ) # Consistency with\n",
    "    l_values.append(np.median(durations)) # [days]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_median', 'days', dataset) )\n",
    "    l_values.append(skew(durations)) # [-]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_skew', '-', dataset) )\n",
    "    l_values.append(kurtosis(durations)) # [-]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_kurtosis', '-', dataset) )\n",
    "\n",
    "    # Calculate timing statistic\n",
    "    condition['season'] = ('time', \n",
    "        [get_season(month) for month in condition['time.month'].values]) # add seasons\n",
    "    max_season_id = condition.groupby('season').sum().argmax(dim='season') # find season with most True values\n",
    "    l_values.append(condition.season[max_season_id].values[0]) # add season abbrev\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_timing', 'season', dataset) )\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8cccc-dc64-4df5-af73-7659280451cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select only whole years in the dataset, if that's possible\n",
    "def subset_dataset_to_max_full_years(ds, time='time') -> xr.Dataset:\n",
    "    '''Takes an xarray dataset and subsets this to the longest stretch of whole years, counting back from the final date'''\n",
    "\n",
    "    # Find start and end years\n",
    "    final_timestamp = pd.Timestamp(ds[time][-1].values)\n",
    "    start_year = ds[time][0].dt.year\n",
    "    final_year = ds[time][-1].dt.year\n",
    "    max_years = (final_year - start_year).values # subtraction returns DataArray, so we need to extract just the array itself\n",
    "    \n",
    "    # Iteratively try years until we have found something that works, starting at longest possible\n",
    "    for duration in range(max_years,-1,-1):\n",
    "\n",
    "        # Calculate the start datetime of the current duration\n",
    "        start_timestamp = final_timestamp - pd.DateOffset(years=duration)\n",
    "        print(f'checking {start_timestamp}')\n",
    "\n",
    "        # Select the subset of the dataset for the current duration\n",
    "        # Note: if either start or final are not part of the time series,\n",
    "        #  this will silently just use whatever is available\n",
    "        subset_ds = ds.sel(time=slice(start_timestamp, final_timestamp))\n",
    "\n",
    "        # Check if we actually selected the duration we requested\n",
    "        subset_start = pd.Timestamp(subset_ds[time][0].values)\n",
    "        if subset_start == start_timestamp:\n",
    "            break # stop searching. We're counting down the durations, so we have the longest possible one now\n",
    "\n",
    "    # Now check if we have selected a zero-year period\n",
    "    # This would imply we have less than a full year of data\n",
    "    # In this case, just return the original data set with a warning\n",
    "    if duration == 0:\n",
    "        print(f'--- WARNING: subset_dataset_to_max_full_years(): Found no full data years. Returning original DataArray')\n",
    "        return ds\n",
    "    else:   \n",
    "        return subset_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ba6ff-0399-44b5-bc39-6f0087556e1f",
   "metadata": {},
   "source": [
    "### EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c03abfe-2021-4a17-a995-62ea44df6aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Vegetation</th>\n",
       "      <th>forest_height_2020_mean</th>\n",
       "      <th>m</th>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forest_height_2020_stdev</th>\n",
       "      <th>m</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Topography</th>\n",
       "      <th>dem_mean</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "      <td>100</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dem_stdev</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               01   02   03   04\n",
       "Category   Attribute                Unit                        \n",
       "Vegetation forest_height_2020_mean  m          25   30   35   40\n",
       "           forest_height_2020_stdev m           5    3    3    4\n",
       "Topography dem_mean                 m.a.s.l.  300  400  100  600\n",
       "           dem_stdev                m.a.s.l.   30    1   10    7"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample data\n",
    "l_station_id = ['01', '02', '03', '04']\n",
    "l_values = []\n",
    "l_values.append([25, 5, 300, 30])\n",
    "l_values.append([30, 3, 400, 1])\n",
    "l_values.append([35, 3, 100, 10])\n",
    "l_values.append([40, 4, 600, 7])\n",
    "\n",
    "# Make the dataframe\n",
    "input_dict = dict(zip(l_station_id, l_values))\n",
    "df = pd.DataFrame(input_dict)\n",
    "\n",
    "# Set the index\n",
    "multi_index = pd.MultiIndex.from_tuples([\n",
    "    ('Vegetation', 'forest_height_2020_mean', 'm'),\n",
    "    ('Vegetation', 'forest_height_2020_stdev', 'm'),\n",
    "    ('Topography', 'dem_mean', 'm.a.s.l.'),\n",
    "    ('Topography', 'dem_stdev', 'm.a.s.l.')\n",
    "], names=['Category', 'Attribute', 'Unit'])\n",
    "df.index = multi_index\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f4a4ab7d-5c44-463c-8c31-859f3bdf5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test_attributes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f90a83-2162-4d9d-ac9e-79fc89641910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels-spat-env",
   "language": "python",
   "name": "camels-spat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
