{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db3e732-426b-4b22-b055-b9395f29fa09",
   "metadata": {},
   "source": [
    "## Calculate attributes\n",
    "Takes prepared geospatial data and computes various attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac7b17d-07d9-4985-bb9c-8508fb2c7554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w4/hh0hkdr92cg8llf8s0l5rzj80000gq/T/ipykernel_38121/3056185177.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "from python_cs_functions import config as cs, attributes as csa\n",
    "from python_cs_functions.delineate import prepare_delineation_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bcba8f-d99c-4058-9de9-23ae255c19e9",
   "metadata": {},
   "source": [
    "### Config handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dfdcc03-f734-4df6-8bee-25ae59f5560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where the config file can be found\n",
    "config_file = '../0_config/config.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa42aa7-cf4d-4c51-8555-05638e2ffc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the required info from the config file\n",
    "data_path            = cs.read_from_config(config_file,'data_path')\n",
    "\n",
    "# CAMELS-spat metadata\n",
    "cs_meta_path = cs.read_from_config(config_file,'cs_basin_path')\n",
    "cs_meta_name = cs.read_from_config(config_file,'cs_meta_name')\n",
    "cs_unusable_name = cs.read_from_config(config_file,'cs_unusable_name')\n",
    "\n",
    "# Basin folder\n",
    "cs_basin_folder = cs.read_from_config(config_file, 'cs_basin_path')\n",
    "basins_path = Path(data_path) / cs_basin_folder\n",
    "\n",
    "# Get the temporary data folder\n",
    "cs_temp_folder = cs.read_from_config(config_file, 'temp_path')\n",
    "temp_path = Path(cs_temp_folder)\n",
    "temp_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933dae9c-b5fe-42d2-9cc9-2ee54661e2c7",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "333bd63a-b0a1-417d-b9c6-d28306a0120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAMELS-spat metadata file\n",
    "cs_meta_path = Path(data_path) / cs_meta_path\n",
    "cs_meta = pd.read_csv(cs_meta_path / cs_meta_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a497a0-a9a2-404c-9b2a-8173cbb1436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open list of unusable stations; Enforce reading IDs as string to keep leading 0's\n",
    "cs_unusable = pd.read_csv(cs_meta_path / cs_unusable_name, dtype={'Station_id': object})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87113cc-bfb3-48dd-bae1-4c7f19fd39b3",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3185dc90-2a46-46b2-bdd1-c0032c2c452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_message = f'\\n!!! CHECK DEBUGGING STATUS: \\n- Testing 1 file \\n- Testing 1 basin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c395a8-7e5d-4140-b61d-65581fc05a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subfolders = ['era5', 'worldclim', 'lai', 'forest_height', 'glclu2019', 'modis_land', 'lgrip30', 'merit', 'hydrolakes'] #\n",
    "# 'glhymps', 'pelletier','soilgrids', 'hydrology'\n",
    "#geo_subfolders = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11e6b92c-35d3-4358-a3fe-0daec6cd96f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every attribute needs a list, so that we can efficiently construct a dataframe later\n",
    "l_gauges = [] # station ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b44331c-4cd6-4b2a-ba99-db3a767f9ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!!! CHECK DEBUGGING STATUS: \n",
      "- Testing 1 file \n",
      "- Testing 1 basin\n",
      "Processing geospatial data into attributes for CAN_01AD002\n",
      " - processing era5\n",
      " - processing worldclim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wmk934/data/CAMELS_spat/camels-spat-env/lib/python3.11/site-packages/rasterstats/io.py:328: NodataWarning: Setting nodata to -999; specify nodata explicitly\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - processing lai\n",
      " - processing forest_height\n",
      " - processing glclu2019\n",
      " - processing modis_land\n",
      " - processing lgrip30\n",
      " - processing merit\n",
      " - processing hydrolakes\n",
      "\n",
      "!!! CHECK DEBUGGING STATUS: \n",
      "- Testing 1 file \n",
      "- Testing 1 basin\n"
     ]
    }
   ],
   "source": [
    "print(debug_message)\n",
    "for ix,row in cs_meta.iterrows():\n",
    "\n",
    "    # DEBUGGING\n",
    "    if ix != 0: continue\n",
    "\n",
    "    # Get the paths\n",
    "    basin_id, shp_lump_path, shp_dist_path, _, _ = prepare_delineation_outputs(cs_meta, ix, basins_path)\n",
    "    geo_folder = basins_path / 'basin_data' / basin_id / 'geospatial'\n",
    "    met_folder = basins_path / 'basin_data' / basin_id / 'forcing'\n",
    "    hyd_folder = basins_path / 'basin_data' / basin_id / 'observations'\n",
    "\n",
    "    # Data storage\n",
    "    l_gauges.append(basin_id) # Update the Station list\n",
    "    l_values = [] # Initialize an empty list where we'll store this basin's attributes\n",
    "    l_index = [] # Initialize an empty list where we'll store the attribute descriptions\n",
    "\n",
    "    # Define the shapefiles\n",
    "    shp = str(shp_lump_path) # because zonalstats wants a file path, not a geodataframe\n",
    "    riv = str(shp_dist_path).format('river') # For topographic attributes\n",
    "    \n",
    "    # Data-specific processing\n",
    "    print(f'Processing geospatial data into attributes for {basin_id}')\n",
    "    for dataset in data_subfolders:\n",
    "        print(f' - processing {dataset}')\n",
    "\n",
    "        ## CLIMATE\n",
    "        if dataset == 'era5':\n",
    "            l_values, l_index, ds_precip, ds_era5 = csa.attributes_from_era5(met_folder, shp, 'era5', l_values, l_index)                                \n",
    "        if dataset == 'worldclim':\n",
    "            csa.oudin_pet_from_worldclim(geo_folder, dataset) # Get an extra PET estimate to sanity check ERA5 outcomes\n",
    "            csa.aridity_and_fraction_snow_from_worldclim(geo_folder, dataset) # Get monthly aridity and fraction snow maps\n",
    "            l_values, l_index = csa.attributes_from_worldclim(geo_folder, dataset, shp, l_values, l_index)\n",
    "\n",
    "        ## LAND COVER\n",
    "        if dataset == 'forest_height':\n",
    "            l_values, l_index = csa.attributes_from_forest_height(geo_folder, dataset, shp, l_values, l_index)\n",
    "        if dataset == 'lai':\n",
    "            l_values, l_index = csa.attributes_from_lai(geo_folder, dataset, temp_path, shp, l_values, l_index)\n",
    "        if dataset == 'glclu2019':\n",
    "            l_values, l_index = csa.attributes_from_glclu2019(geo_folder, dataset, shp, l_values, l_index)\n",
    "        if dataset == 'modis_land':\n",
    "            l_values, l_index = csa.attributes_from_modis_land(geo_folder, dataset, shp, l_values, l_index)\n",
    "        if dataset == 'lgrip30':\n",
    "            l_values, l_index = csa.attributes_from_lgrip30(geo_folder, dataset, shp, l_values, l_index)\n",
    "\n",
    "        ## TOPOGRAPHY\n",
    "        if dataset == 'merit':\n",
    "            l_values, l_index = csa.attributes_from_merit(geo_folder, dataset, shp, riv, row, l_values, l_index)\n",
    "\n",
    "        ## OPENWATER\n",
    "        if dataset == 'hydrolakes':\n",
    "            l_values, l_index = csa.attributes_from_hydrolakes(geo_folder, dataset, l_values, l_index)\n",
    "        if dataset == 'hydrology':\n",
    "            a=1\n",
    "        \n",
    "print(debug_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d197b84-b519-487b-aaf4-b14929671829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 720)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l_values),len(l_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f740e80-faf5-4c56-a379-48b3fb2b6b5f",
   "metadata": {},
   "source": [
    "#### Make the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f038ae6-5dff-43cf-a462-8d31eea3d39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>CAN_01AD002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Climate</th>\n",
       "      <th>num_years_era5</th>\n",
       "      <th>years</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mper_mean_month_01</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>5.552775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mper_mean_month_02</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>6.924757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mper_mean_month_03</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>11.433318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mper_mean_month_04</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>17.048635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Open water</th>\n",
       "      <th>reservoir_volume_min</th>\n",
       "      <th>million~m^3</th>\n",
       "      <th>HydroLAKES</th>\n",
       "      <td>129.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reservoir_volume_mean</th>\n",
       "      <th>million~m^3</th>\n",
       "      <th>HydroLAKES</th>\n",
       "      <td>129.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reservoir_volume_max</th>\n",
       "      <th>million~m^3</th>\n",
       "      <th>HydroLAKES</th>\n",
       "      <td>129.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reservoir_volume_std</th>\n",
       "      <th>million~m^3</th>\n",
       "      <th>HydroLAKES</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reservoir_volume_total</th>\n",
       "      <th>million~m^3</th>\n",
       "      <th>HydroLAKES</th>\n",
       "      <td>129.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         CAN_01AD002\n",
       "Category   Attribute              Unit        Source                \n",
       "Climate    num_years_era5         years       ERA5                70\n",
       "           mper_mean_month_01     mm          ERA5          5.552775\n",
       "           mper_mean_month_02     mm          ERA5          6.924757\n",
       "           mper_mean_month_03     mm          ERA5         11.433318\n",
       "           mper_mean_month_04     mm          ERA5         17.048635\n",
       "...                                                              ...\n",
       "Open water reservoir_volume_min   million~m^3 HydroLAKES       129.8\n",
       "           reservoir_volume_mean  million~m^3 HydroLAKES       129.8\n",
       "           reservoir_volume_max   million~m^3 HydroLAKES       129.8\n",
       "           reservoir_volume_std   million~m^3 HydroLAKES         NaN\n",
       "           reservoir_volume_total million~m^3 HydroLAKES       129.8\n",
       "\n",
       "[720 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with a fake second station\n",
    "l_gauges = ['CAN_01AD002','CAN_01AD003']\n",
    "\n",
    "# Make the dataframe\n",
    "input_dict = dict(zip(l_gauges, [l_values,l_values]))\n",
    "df = pd.DataFrame(input_dict)\n",
    "\n",
    "# Set the index\n",
    "multi_index = pd.MultiIndex.from_tuples(l_index, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "df.index = multi_index\n",
    "\n",
    "# Drop the fake extra column\n",
    "df = df.drop(columns=['CAN_01AD003'], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b27b6090-0a20-48aa-8c85-0a0c5e3ceff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>CAN_01AD002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">Climate</th>\n",
       "      <th>mper_mean_month_01</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>5.603236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mper_mean_month_02</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>6.93889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mper_mean_month_03</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>11.45397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mper_mean_month_04</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>17.082485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mper_mean_month_05</th>\n",
       "      <th>mm</th>\n",
       "      <th>ERA5</th>\n",
       "      <td>29.253726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fracsnow2_std_month_10</th>\n",
       "      <th>-</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fracsnow2_mean_month_11</th>\n",
       "      <th>-</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>0.999119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fracsnow2_std_month_11</th>\n",
       "      <th>-</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>0.029672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fracsnow2_mean_month_12</th>\n",
       "      <th>-</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fracsnow2_std_month_12</th>\n",
       "      <th>-</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                CAN_01AD002\n",
       "Category Attribute               Unit Source               \n",
       "Climate  mper_mean_month_01      mm   ERA5         5.603236\n",
       "         mper_mean_month_02      mm   ERA5          6.93889\n",
       "         mper_mean_month_03      mm   ERA5         11.45397\n",
       "         mper_mean_month_04      mm   ERA5        17.082485\n",
       "         mper_mean_month_05      mm   ERA5        29.253726\n",
       "...                                                     ...\n",
       "         fracsnow2_std_month_10  -    WorldClim         0.0\n",
       "         fracsnow2_mean_month_11 -    WorldClim    0.999119\n",
       "         fracsnow2_std_month_11  -    WorldClim    0.029672\n",
       "         fracsnow2_mean_month_12 -    WorldClim         1.0\n",
       "         fracsnow2_std_month_12  -    WorldClim         0.0\n",
       "\n",
       "[600 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of selection code\n",
    "tmp = df.loc[df.index.get_level_values('Category').str.contains('Climate')]# & \n",
    "             #df.index.get_level_values('Attribute').str.contains('mean')].copy()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127feb5-ed86-424b-8367-d4ec38177968",
   "metadata": {},
   "source": [
    "## DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fe183cc-8879-4189-9e21-014a1a76204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from rasterstats import zonal_stats\n",
    "import rasterio\n",
    "from scipy.stats import circmean, circstd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ebdacb40-2dd6-43e3-b6f1-472f9e990faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = 0\n",
    "row = cs_meta.iloc[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "591570a6-d8e2-401a-95b7-6162972770b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'hydrology'\n",
    "hyd_folder = basins_path / 'basin_data' / basin_id / 'observations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac0c2779-5253-4d02-84f4-5ccdf9a393c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_km2 = row['Basin_area_km2'] # km2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "76775fcb-432f-4947-b2a3-4ef31224066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "seconds_per_minute = 60 # s min-1\n",
    "seconds_per_hour = 60 * seconds_per_minute # s hr-1\n",
    "seconds_per_day = 24 * seconds_per_hour # s d-1\n",
    "water_density = 1000 # kg m-3\n",
    "mm_per_m = 1000 # mm m-1\n",
    "m_per_km = 1000 # m km-1\n",
    "\n",
    "# Load observations\n",
    "hyd_file = hyd_folder / f'{basin_id}_daily_flow_observations.nc'\n",
    "hyd = xr.open_dataset(hyd_file)\n",
    "hyd = subset_dataset_to_max_full_years(hyd, res='day')\n",
    "\n",
    "# Convert observations to mm d-1\n",
    "area_m2 = area_km2 * m_per_km**2 # [km2] * ([m km-1]^2 = [m2 km-2]) = [m2]\n",
    "hyd['q_obs'] = hyd['q_obs'] * seconds_per_day / area_m2 * mm_per_m # [m3 s-1] * [s d-1] / [m2] * [mm m-1] = [m d-1]\n",
    "\n",
    "# Convert precipitation into mm d-1\n",
    "pre = (ds_precip * seconds_per_hour).resample(time='1D').sum() / water_density * mm_per_m # ([kg m-2 s-1] * [s hr-1]).resample(time='1D').sum() / [kg m-3] * [mm m-1] = [mm d-1]\n",
    "\n",
    "# Match times between hydrologic data and precipitation\n",
    "pre = pre.sel(time=slice(hyd['time'][0].values, hyd['time'][-1].values))\n",
    "assert hyd['time'][0].values == pre['time'][0].values, 'attributes_from_streamflow(): mismatch between precipitation and streamflow start timestamp'\n",
    "assert hyd['time'][-1].values == pre['time'][-1].values, 'attributes_from_streamflow(): mismatch between precipitation and streamflow final timestamp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b32bd8bd-dca7-4dfb-8b85-c3274419e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_values = []\n",
    "l_index = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9a71315e-c52a-41aa-9dfe-2d23f644eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyd,pre = attributes_from_streamflow(hyd_folder, dataset, basin_id, ds_precip, row, l_values, l_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1aab11b4-d975-4c80-ab8f-b35440ec84b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_streamflow(hyd_folder, dataset, basin_id, pre, row, l_values, l_index):\n",
    "\n",
    "    '''Calculates various streamflow signatures'''\n",
    "\n",
    "    # Constants\n",
    "    seconds_per_minute = 60 # s min-1\n",
    "    seconds_per_hour = 60 * seconds_per_minute # s hr-1\n",
    "    seconds_per_day = 24 * seconds_per_hour # s d-1\n",
    "    water_density = 1000 # kg m-3\n",
    "    mm_per_m = 1000 # mm m-1\n",
    "    m_per_km = 1000 # m km-1\n",
    "\n",
    "    # Find the data source\n",
    "    if row['Country'] == 'CAN':\n",
    "        source = 'WSC'\n",
    "    elif row['Country'] == 'USA':\n",
    "        source = 'USGS'\n",
    "\n",
    "    # Load observations\n",
    "    hyd_file = hyd_folder / f'{basin_id}_daily_flow_observations.nc'\n",
    "    hyd = xr.open_dataset(hyd_file)\n",
    "    hyd = subset_dataset_to_max_full_years(hyd, res='day', water_year=True)\n",
    "\n",
    "    # Convert observations to mm d-1\n",
    "    area_km2 = row['Basin_area_km2']\n",
    "    area_m2 = area_km2 * m_per_km**2 # [km2] * ([m km-1]^2 = [m2 km-2]) = [m2]\n",
    "    hyd['q_obs'] = hyd['q_obs'] * seconds_per_day / area_m2 * mm_per_m # [m3 s-1] * [s d-1] / [m2] * [mm m-1] = [m d-1]\n",
    "\n",
    "    # Convert precipitation into mm d-1\n",
    "    pre = (pre * seconds_per_hour).resample(time='1D').sum() / water_density * mm_per_m # ([kg m-2 s-1] * [s hr-1]).resample(time='1D').sum() / [kg m-3] * [mm m-1] = [mm d-1]\n",
    "    \n",
    "    # Match times between hydrologic data and precipitation\n",
    "    pre = pre.sel(time=slice(hyd['time'][0].values, hyd['time'][-1].values))\n",
    "    assert hyd['time'][0].values == pre['time'][0].values, 'attributes_from_streamflow(): mismatch between precipitation and streamflow start timestamp'\n",
    "    assert hyd['time'][-1].values == pre['time'][-1].values, 'attributes_from_streamflow(): mismatch between precipitation and streamflow final timestamp'\n",
    "\n",
    "    # Create a water-year time variable\n",
    "    hyd['water_year'] = hyd['time'].dt.year.where(hyd['time'].dt.month < 10, hyd['time'].dt.year + 1)\n",
    "    pre['water_year'] = pre['time'].dt.year.where(pre['time'].dt.month < 10, pre['time'].dt.year + 1)\n",
    "    \n",
    "    # Track the data years used\n",
    "    num_years = len(hyd.groupby('time.year'))\n",
    "    l_values.append(num_years)\n",
    "    l_index.append( ('Hydrology', 'num_years_hyd ', 'years', '-') )\n",
    "    \n",
    "    # Signatures\n",
    "    calculate_signatures(hyd, pre, source, l_values, l_index)\n",
    "\n",
    "    return hyd, pre #l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "48d99518-ca0f-475e-9e4f-f257f4585a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_signatures(hyd, pre, source, l_values, l_index):\n",
    "    '''Calculates various signatures'''\n",
    "\n",
    "    ## LONG-TERM STATISTICS\n",
    "    # Mean daily discharge\n",
    "    daily_mean_q = hyd['q_obs'].groupby(hyd['water_year']).mean() # .mean() of daily values gives us [mm d-1]\n",
    "    mean_q_m = daily_mean_q.mean()\n",
    "    mean_q_s = daily_mean_q.std()\n",
    "    l_values.append(float(mean_q_m.values))\n",
    "    l_index.append( ('Hydrology', 'daily_discharge_mean', 'mm d^-1', f'{source}') )\n",
    "    l_values.append(float(mean_q_s.values))\n",
    "    l_index.append( ('Hydrology', 'daily_discharge_std', 'mm d^-1', f'{source}') )\n",
    "    \n",
    "    # Runoff ratio\n",
    "    daily_mean_p = pre.groupby(hyd['water_year']).mean()\n",
    "    yearly_rr = daily_mean_q/daily_mean_p\n",
    "    rr_m = yearly_rr.mean()\n",
    "    rr_s = yearly_rr.std()\n",
    "    l_values.append(float(rr_m.values))\n",
    "    l_index.append( ('Hydrology', 'runoff_ratio_mean', 'mm d^-1 month-1', f'{source}, ERA5') )\n",
    "    l_values.append(float(rr_s.values))\n",
    "    l_index.append( ('Hydrology', 'runoff_ratio_std', 'mm d^-1 month-1', f'{source}, ERA5') )\n",
    "\n",
    "    # Streamflow elasticity\n",
    "    q_elas = np.median(((daily_mean_q - daily_mean_q.mean())/(daily_mean_p - daily_mean_p.mean()))*(daily_mean_p.mean()/daily_mean_q.mean()))\n",
    "    l_values.append(float(q_elas.values))\n",
    "    l_index.append( ('Hydrology', 'streamflow_elasticity', '-', f'{source}, ERA5') )\n",
    "\n",
    "    # Slope of FDC\n",
    "    groups = hyd['q_obs'].groupby(hyd['water_year'])\n",
    "    slopes = []\n",
    "    for year,group in groups:\n",
    "        flows = group.values.copy()\n",
    "        flows.sort()\n",
    "        flows = np.log(flows)\n",
    "        slope = (np.percentile(flows,66) - np.percentile(flows,33)) / (.66*len(flows) - .33*len(flows))\n",
    "        slopes.append(slope)\n",
    "    slopes = np.array(slopes)\n",
    "    slope_m = slopes.mean()\n",
    "    slope_s = slopes.std()\n",
    "    l_values.append(slope_m)\n",
    "    l_index.append( ('Hydrology', 'fdc_slope_mean', '-', f'{source}') )\n",
    "    l_values.append(slope_s)\n",
    "    l_index.append( ('Hydrology', 'fdc_slope_std', '-', f'{source}') )\n",
    "\n",
    "    # Baseflow index\n",
    "    \n",
    "    return l_values,l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b5e78998-4f63-4f9e-b3fe-edd3d1b9eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean daily flows\n",
    "daily_mean_q = hyd['q_obs'].groupby(hyd['water_year']).mean() # .mean() of daily values gives us [mm d-1]\n",
    "mean_q_m = daily_mean_q.mean()\n",
    "mean_q_s = daily_mean_q.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b84df2e4-c40b-43e4-b93f-ee0eeb3785e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runoff ratio\n",
    "daily_mean_p = pre.groupby(hyd['water_year']).mean()\n",
    "yearly_rr = daily_mean_q/daily_mean_p\n",
    "rr_m = yearly_rr.mean()\n",
    "rr_s = yearly_rr.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9621f4ba-40d4-47b8-9ce5-f685df5ffe60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5918387993384353"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Streamflow elasticity\n",
    "np.median(((daily_mean_q - daily_mean_q.mean())/(daily_mean_p - daily_mean_p.mean()))*(daily_mean_p.mean()/daily_mean_q.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "383c34c7-1e82-4f42-8b34-444d816a8664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slope of FDC\n",
    "groups = hyd['q_obs'].groupby(hyd['water_year'])\n",
    "slopes = []\n",
    "for year,group in groups:\n",
    "    flows = group.values.copy()\n",
    "    flows.sort()\n",
    "    flows = np.log(flows)\n",
    "    slope = (np.percentile(flows,66) - np.percentile(flows,33)) / (.66*len(flows) - .33*len(flows))\n",
    "    slopes.append(slope)\n",
    "slopes = np.array(slopes)\n",
    "slope_m = slopes.mean()\n",
    "slope_s = slopes.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "7f2b2bde-c34b-4175-810c-cf182e47ff4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0068120992064634915, 0.0015331007302922383)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slope_m,slope_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "055e1aac-b059-4e6b-ab1f-842d06fe689f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "431e57a3-4764-4c07-b395-c3bca174dcdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006032075160904442"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002bac1b-4798-4675-95c1-28a4171ffc72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "aaeaeb5d-c4ad-4fd8-ba47-590d4968d84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray &#x27;q_obs&#x27; (water_year: 70)&gt;\n",
       "array([0.92330192, 0.61455446, 0.56809531, 1.11149085, 0.87037379,\n",
       "       0.33080085, 0.3928444 , 1.36436972, 0.80568384, 0.73511299,\n",
       "       0.65866124, 0.73511299, 0.81744565, 0.80568384, 0.55280497,\n",
       "       0.66454215, 0.8880165 , 0.70276802, 0.30463082, 0.79392203,\n",
       "       0.62337582, 0.9674087 , 1.12325266, 0.82332655, 0.5939713 ,\n",
       "       1.12619311, 0.98799187, 0.77039842, 0.72923209, 0.80862429,\n",
       "       1.17618079, 0.88213559, 0.83508836, 1.1938235 , 0.63513763,\n",
       "       1.15853808, 0.81156475, 0.66454215, 0.56633107, 0.67630396,\n",
       "       0.93506373, 0.97623006, 0.65278034, 1.21734712, 0.62925672,\n",
       "       1.17912124, 0.76451751, 0.52986944, 0.7409939 , 0.94682554,\n",
       "       0.58632611, 0.4951721 , 0.4528296 , 1.42611921, 0.72923209,\n",
       "       1.56432045, 0.8938974 , 1.17618079, 1.10560994, 0.79392203,\n",
       "       1.66429582, 0.99387277, 1.04092   , 0.62337582, 0.82332655,\n",
       "       1.15559763, 0.78216023, 0.5939713 , 0.62337582, 0.68806576])\n",
       "Coordinates:\n",
       "    quantile    float64 0.5\n",
       "  * water_year  (water_year) int64 1951 1952 1953 1954 ... 2017 2018 2019 2020</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'>'q_obs'</div><ul class='xr-dim-list'><li><span class='xr-has-index'>water_year</span>: 70</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-bcc3f308-0ea9-436d-9ade-ae3bdcd50dd1' class='xr-array-in' type='checkbox' checked><label for='section-bcc3f308-0ea9-436d-9ade-ae3bdcd50dd1' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>0.9233 0.6146 0.5681 1.111 0.8704 ... 1.156 0.7822 0.594 0.6234 0.6881</span></div><div class='xr-array-data'><pre>array([0.92330192, 0.61455446, 0.56809531, 1.11149085, 0.87037379,\n",
       "       0.33080085, 0.3928444 , 1.36436972, 0.80568384, 0.73511299,\n",
       "       0.65866124, 0.73511299, 0.81744565, 0.80568384, 0.55280497,\n",
       "       0.66454215, 0.8880165 , 0.70276802, 0.30463082, 0.79392203,\n",
       "       0.62337582, 0.9674087 , 1.12325266, 0.82332655, 0.5939713 ,\n",
       "       1.12619311, 0.98799187, 0.77039842, 0.72923209, 0.80862429,\n",
       "       1.17618079, 0.88213559, 0.83508836, 1.1938235 , 0.63513763,\n",
       "       1.15853808, 0.81156475, 0.66454215, 0.56633107, 0.67630396,\n",
       "       0.93506373, 0.97623006, 0.65278034, 1.21734712, 0.62925672,\n",
       "       1.17912124, 0.76451751, 0.52986944, 0.7409939 , 0.94682554,\n",
       "       0.58632611, 0.4951721 , 0.4528296 , 1.42611921, 0.72923209,\n",
       "       1.56432045, 0.8938974 , 1.17618079, 1.10560994, 0.79392203,\n",
       "       1.66429582, 0.99387277, 1.04092   , 0.62337582, 0.82332655,\n",
       "       1.15559763, 0.78216023, 0.5939713 , 0.62337582, 0.68806576])</pre></div></div></li><li class='xr-section-item'><input id='section-504a2461-e0c7-453a-8565-15a7bc8a4e56' class='xr-section-summary-in' type='checkbox'  checked><label for='section-504a2461-e0c7-453a-8565-15a7bc8a4e56' class='xr-section-summary' >Coordinates: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>quantile</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>0.5</div><input id='attrs-39578553-0296-48d8-ae72-5c07fff6d817' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-39578553-0296-48d8-ae72-5c07fff6d817' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b6b4a2e1-596d-42e2-8d0e-9034fa53a018' class='xr-var-data-in' type='checkbox'><label for='data-b6b4a2e1-596d-42e2-8d0e-9034fa53a018' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array(0.5)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>water_year</span></div><div class='xr-var-dims'>(water_year)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>1951 1952 1953 ... 2018 2019 2020</div><input id='attrs-ba104882-0b0d-4a9e-9069-b82917300d0e' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-ba104882-0b0d-4a9e-9069-b82917300d0e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-d605122b-11e2-4da7-b189-58841cdc6b77' class='xr-var-data-in' type='checkbox'><label for='data-d605122b-11e2-4da7-b189-58841cdc6b77' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>time</dd><dt><span>bounds :</span></dt><dd>time_bnds</dd></dl></div><div class='xr-var-data'><pre>array([1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962,\n",
       "       1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974,\n",
       "       1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986,\n",
       "       1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998,\n",
       "       1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,\n",
       "       2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-eb8c4e3c-2ce0-4a6a-a7e4-f9f52949248e' class='xr-section-summary-in' type='checkbox'  ><label for='section-eb8c4e3c-2ce0-4a6a-a7e4-f9f52949248e' class='xr-section-summary' >Indexes: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>water_year</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-e51a12bf-c2f9-4516-8af1-6652620d72ae' class='xr-index-data-in' type='checkbox'/><label for='index-e51a12bf-c2f9-4516-8af1-6652620d72ae' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962,\n",
       "       1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974,\n",
       "       1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986,\n",
       "       1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998,\n",
       "       1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010,\n",
       "       2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020],\n",
       "      dtype=&#x27;int64&#x27;, name=&#x27;water_year&#x27;))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-371678a1-30d9-4f60-b79d-e2b7ec8382cf' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-371678a1-30d9-4f60-b79d-e2b7ec8382cf' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.DataArray 'q_obs' (water_year: 70)>\n",
       "array([0.92330192, 0.61455446, 0.56809531, 1.11149085, 0.87037379,\n",
       "       0.33080085, 0.3928444 , 1.36436972, 0.80568384, 0.73511299,\n",
       "       0.65866124, 0.73511299, 0.81744565, 0.80568384, 0.55280497,\n",
       "       0.66454215, 0.8880165 , 0.70276802, 0.30463082, 0.79392203,\n",
       "       0.62337582, 0.9674087 , 1.12325266, 0.82332655, 0.5939713 ,\n",
       "       1.12619311, 0.98799187, 0.77039842, 0.72923209, 0.80862429,\n",
       "       1.17618079, 0.88213559, 0.83508836, 1.1938235 , 0.63513763,\n",
       "       1.15853808, 0.81156475, 0.66454215, 0.56633107, 0.67630396,\n",
       "       0.93506373, 0.97623006, 0.65278034, 1.21734712, 0.62925672,\n",
       "       1.17912124, 0.76451751, 0.52986944, 0.7409939 , 0.94682554,\n",
       "       0.58632611, 0.4951721 , 0.4528296 , 1.42611921, 0.72923209,\n",
       "       1.56432045, 0.8938974 , 1.17618079, 1.10560994, 0.79392203,\n",
       "       1.66429582, 0.99387277, 1.04092   , 0.62337582, 0.82332655,\n",
       "       1.15559763, 0.78216023, 0.5939713 , 0.62337582, 0.68806576])\n",
       "Coordinates:\n",
       "    quantile    float64 0.5\n",
       "  * water_year  (water_year) int64 1951 1952 1953 1954 ... 2017 2018 2019 2020"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantiles\n",
    "groups.quantile(.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9773d9-3c90-4267-9386-739fcff83113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac5fc6f-5fb4-4a5b-9dab-2e339131afef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1dfae26f-3c98-49ac-88f5-169532e115a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean monthly flows\n",
    "monthly_q = hyd['q_obs'].resample(time='1ME').mean().groupby('time.month')\n",
    "monthly_m = monthly_q.mean()\n",
    "monthly_s = monthly_q.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c8a7e7-2736-4e39-a277-3042a6b1dac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5f0b0-7b03-440b-ac6d-16835deb78ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251de474-181e-4222-a13c-66a057de75a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1941b65b-108a-48f0-81dc-811ac226ee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_dataset_to_max_full_years(ds, time='time', res='hour', water_year=False, debug=False) -> xr.Dataset:\n",
    "\n",
    "    # Find final and end years\n",
    "    start_year = ds[time][0].dt.year.values\n",
    "    final_year = ds[time][-1].dt.year.values\n",
    "    final_timestamp = ds[time][-1]\n",
    "\n",
    "    # Set how a year is defined\n",
    "    if water_year: # Oct-1 to Sep-30\n",
    "        start_month = 10\n",
    "        start_day   = 1\n",
    "        final_month = 9\n",
    "        final_day   = 30\n",
    "    else: # Jan-1 to Dec-31\n",
    "        start_month = 1\n",
    "        start_day   = 1\n",
    "        final_month = 12\n",
    "        final_day   = 31\n",
    "\n",
    "    # Find the first occurrence of Jan-01 00:00 as start of the subset\n",
    "    for year in range(start_year,final_year):\n",
    "        \n",
    "        # Define the initial timestamp to check\n",
    "        if res == 'hour':\n",
    "            start_timestamp = pd.Timestamp(year,start_month,start_day,0,0,0)\n",
    "        elif res == 'day':\n",
    "            start_timestamp = pd.Timestamp(year,start_month,start_day)\n",
    "        if debug: print(f'checking {start_timestamp}')\n",
    "\n",
    "        # Select the subset of the dataset for the current duration\n",
    "        # Note: if either start or final are not part of the time series,\n",
    "        #  this will silently just use whatever is available\n",
    "        subset_ds = ds.sel(time=slice(start_timestamp, final_timestamp))\n",
    "\n",
    "        # Check if we actually selected the duration we requested\n",
    "        subset_start = pd.Timestamp(subset_ds[time][0].values)\n",
    "        if subset_start == start_timestamp:\n",
    "            subset_start_year = year # Keep track of where we start\n",
    "            break # stop searching. We've found the first occurrence of Jan-01\n",
    "\n",
    "    # Find the last occurrence of Dec-31 23:00 as end of the subset\n",
    "    for year in range(final_year,subset_start_year,-1):\n",
    "\n",
    "        # Define the initial timestamp to check\n",
    "        if res == 'hour':\n",
    "            end_timestamp = pd.Timestamp(year,final_month,final_day,23,0,0)\n",
    "        elif res == 'day':\n",
    "            end_timestamp = pd.Timestamp(year,final_month,final_day)\n",
    "        if debug: print(f'checking {end_timestamp}')\n",
    "\n",
    "        # Select the subset of the dataset for the current duration\n",
    "        subset_ds = ds.sel(time=slice(start_timestamp, end_timestamp))\n",
    "\n",
    "        # Check if we actually selected the duration we requested\n",
    "        subset_end = pd.Timestamp(subset_ds[time][-1].values)\n",
    "        if subset_end == end_timestamp:\n",
    "            subset_end_year = year # Keep track of where we start\n",
    "            break # stop searching. We've found the first occurrence of Jan-01\n",
    "\n",
    "    # Now check if we have selected a zero-year period\n",
    "    # This would imply we have less than a full year of data\n",
    "    # In this case, just return the original data set with a warning\n",
    "    if len(subset_ds) == 0:\n",
    "        print(f'--- WARNING: subset_dataset_to_max_full_years(): Found no full data years. Returning original DataArray')\n",
    "        return ds\n",
    "    else:   \n",
    "        return subset_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055671a5-53fb-4620-afeb-63fa8bc3829a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3db846-a046-4d6c-846e-6b86bef4a367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d9f4de-a6bc-4544-9e7b-80f793d154c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aee2036-ca15-429c-a82e-8d550ff7799f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ecd30bf-7f36-4a08-89bb-cc86515a05ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### High-level collection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740b45f-f227-41bb-8371-42abda75c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_hydrolakes(geo_folder, dataset, l_values, l_index):\n",
    "    \n",
    "    '''Calculates open water attributes from HydroLAKES'''\n",
    "\n",
    "    # Load the file\n",
    "    lake_str = str(geo_folder / dataset / 'raw'    / 'HydroLAKES_polys_v10_NorthAmerica.shp')\n",
    "    lakes = gpd.read_file(lake_str)\n",
    "\n",
    "    # General stats\n",
    "    res_mask  = lakes['Lake_type'] == 2 # Lake Type 2 == reservoir; see docs (https://data.hydrosheds.org/file/technical-documentation/HydroLAKES_TechDoc_v10.pdf)\n",
    "    num_lakes = len(lakes)\n",
    "    num_resvr = res_mask.sum() \n",
    "    l_values.append(num_lakes)\n",
    "    l_index.append(('Open water', 'open_water_number',  '-', 'HydroLAKES'))\n",
    "    l_values.append(num_resvr)\n",
    "    l_index.append(('Open water', 'known_reservoirs',  '-', 'HydroLAKES'))\n",
    "\n",
    "    # Summary stats\n",
    "    l_values, l_index = get_open_water_stats(lakes, 'Lake_area', 'all', l_values, l_index) # All open water\n",
    "    l_values, l_index = get_open_water_stats(lakes, 'Vol_total', 'all', l_values, l_index)\n",
    "    l_values, l_index = get_open_water_stats(lakes, 'Lake_area', 'reservoir', l_values, l_index) # Reservoirs only\n",
    "    l_values, l_index = get_open_water_stats(lakes, 'Vol_total', 'reservoir', l_values, l_index)\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543601ad-64ad-4d95-8609-4f7cbdb0a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_merit(geo_folder, dataset, shp_str, riv_str, row, l_values, l_index):\n",
    "    \n",
    "    '''Calculates topographic attributes from MERIT data'''\n",
    "\n",
    "    ## Known values\n",
    "    lat = row['Station_lat']\n",
    "    lon = row['Station_lon']\n",
    "    area= row['Basin_area_km2']\n",
    "    src = row['Station_source']\n",
    "    l_values.append(lat)\n",
    "    l_index.append(('Topography', 'gauge_lat',  'degrees', f'{src}'))\n",
    "    l_values.append(lon)\n",
    "    l_index.append(('Topography', 'gauge_lon',  'degrees', f'{src}'))\n",
    "    l_values.append(area)\n",
    "    l_index.append(('Topography', 'basin_area', 'km^2', 'MERIT Hydro'))\n",
    "\n",
    "    ## RASTERS\n",
    "    # Slope and elevation can use zonal stats\n",
    "    files = [str(geo_folder / dataset / 'raw'    / 'merit_hydro_elv.tif'),\n",
    "             str(geo_folder / dataset / 'slope'  / 'merit_hydro_slope.tif')]\n",
    "    attrs = ['merit_hydro_elev', 'merit_hydro_slope']\n",
    "    units = ['m.a.s.l.',         'm m-1']\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    for tif,att,unit in zip(files,attrs,units):\n",
    "        zonal_out = zonal_stats(shp_str, tif, stats=stats)\n",
    "        scale,offset = csa.read_scale_and_offset(tif)\n",
    "        l_values = csa.update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "        l_index += [('Topography', f'{att}_min',  f'{unit}', 'MERIT Hydro'),\n",
    "                    ('Topography', f'{att}_mean', f'{unit}', 'MERIT Hydro'),\n",
    "                    ('Topography', f'{att}_max',  f'{unit}', 'MERIT Hydro'),\n",
    "                    ('Topography', f'{att}_std',  f'{unit}', 'MERIT Hydro')]\n",
    "\n",
    "    # Aspect needs circular stats\n",
    "    tif = str(geo_folder / dataset / 'aspect' / 'merit_hydro_aspect.tif')\n",
    "    l_values, l_index = get_aspect_attributes(tif,l_values,l_index) \n",
    "\n",
    "    ## VECTOR\n",
    "    l_values, l_index = get_river_attributes(riv_str, l_values, l_index, area)\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4033e983-99ba-4070-9a05-ecf0048719e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_lgrip30(geo_folder, dataset, shp_str, l_values, l_index):\n",
    "\n",
    "    '''Calculates percentage occurrence of all classes in LGRIP30 map'''\n",
    "\n",
    "    tif = geo_folder / dataset / 'raw' / 'lgrip30_agriculture.tif'\n",
    "    zonal_out = zonal_stats(shp_str, tif, categorical=True)\n",
    "    check_scale_and_offset(tif)\n",
    "    l_values,l_index = update_values_list_with_categorical(l_values, l_index, zonal_out, 'LGRIP30', prefix='lc3_')\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4579b4a9-d3ae-4665-a16d-e9bead393f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_modis_land(geo_folder, dataset, shp_str, l_values, l_index):\n",
    "\n",
    "    '''Calculates percentage occurrence of all classes in MODIS IGBP map'''\n",
    "\n",
    "    tif = geo_folder / dataset / 'raw' / '2001_2022_mode_MCD12Q1_LC_Type1.tif'\n",
    "    zonal_out = zonal_stats(shp_str, tif, categorical=True)\n",
    "    check_scale_and_offset(tif)\n",
    "    l_values,l_index = update_values_list_with_categorical(l_values, l_index, zonal_out, 'MCD12Q1.061', prefix='lc2_')\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00eb7bfe-a77d-49bf-8468-510437a42aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_glclu2019(geo_folder, dataset, shp_str, l_values, l_index):\n",
    "\n",
    "    '''Calculates percentage occurrence of all classes in GLCLU2019 map'''\n",
    "\n",
    "    tif = geo_folder / dataset / 'raw' / 'glclu2019_map.tif'\n",
    "    zonal_out = zonal_stats(shp_str, tif, categorical=True)\n",
    "    check_scale_and_offset(tif)\n",
    "    l_values,l_index = update_values_list_with_categorical(l_values, l_index, zonal_out, 'GLCLU 2019', prefix='lc1_')\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "101eb959-c4a2-41dc-8e60-7694dca0de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_era5(met_folder, shp_path, dataset, l_values, l_index, use_mfdataset=False):\n",
    "\n",
    "    '''Calculates a variety of metrics from ERA5 data'''\n",
    "\n",
    "    # Define various conversion constants\n",
    "    water_density = 1000 # kg m-3\n",
    "    mm_per_m = 1000 # mm m-1\n",
    "    seconds_per_hour = 60*60 # s h-1\n",
    "    seconds_per_day = seconds_per_hour*24 # s d-1\n",
    "    days_per_month = np.array([31, 28.25, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]).reshape(-1, 1) # d month-1\n",
    "    flip_sign = -1 # -; used to convert PET from negative (by convention this indicates an upward flux) to positive\n",
    "    kelvin_to_celsius = -273.15\n",
    "    pa_per_kpa = 1000 # Pa kPa-1\n",
    "\n",
    "    # Define file locations, depending on if we are dealing with lumped or distributed cases\n",
    "    if 'lumped' in shp_path:\n",
    "        era_folder = met_folder / 'lumped'\n",
    "    elif 'distributed' in shp_path:\n",
    "        era_folder = met_folder / 'distributed'\n",
    "    era_files = sorted( glob.glob( str(era_folder / 'ERA5_*.nc') ) )\n",
    "\n",
    "    # Open the data\n",
    "    if use_mfdataset:\n",
    "        ds = xr.open_mfdataset(era_files, engine='netcdf4')\n",
    "        ds = ds.load() # load the whole thing into memory instead of lazy-loading\n",
    "    else:\n",
    "        ds = xr.merge([xr.open_dataset(f) for f in era_files])\n",
    "        ds = ds.load()\n",
    "    \n",
    "    # Select whole years only\n",
    "    #   This avoids issues in cases where we have incomplete whole data years\n",
    "    #   (e.g. 2000-06-01 to 2007-12-31) in basins with very seasonal weather\n",
    "    #   (e.g. all precip occurs in Jan, Feb, Mar). By using only full years\n",
    "    #   we avoid accidentally biasing the attributes.\n",
    "    ds = subset_dataset_to_max_full_years(ds)\n",
    "    \n",
    "    # --- Monthly attributes\n",
    "    # Calculate monthly PET in mm\n",
    "    #      kg m-2 s-1 / kg m-3\n",
    "    # mm month-1 = kg m-2 s-1 * kg-1 m3 * s d-1 * d month-1 * mm m-1 * -\n",
    "    monthly_mper = ds['mper'].resample(time='1ME').mean().groupby('time.month') \n",
    "    mper_m = monthly_mper.mean() / water_density * seconds_per_day * days_per_month * mm_per_m * flip_sign  # [kg m-2 s-1] to [mm month-1]; negative to indicate upward flux\n",
    "    mper_s = monthly_mper.std() / water_density * seconds_per_day * days_per_month * mm_per_m  # [kg m-2 s-1] to [mm month-1]; negative to indicate upward flux\n",
    "    l_values, l_index = process_era5_means_to_lists(mper_m, 'mean', l_values, l_index, 'mper', 'mm')\n",
    "    l_values, l_index = process_era5_means_to_lists(mper_s, 'std', l_values, l_index, 'mper', 'mm')\n",
    "        \n",
    "    # Same for precipitation: [mm month-1]\n",
    "    monthly_mtpr = ds['mtpr'].resample(time='1ME').mean().groupby('time.month')\n",
    "    mtpr_m = monthly_mtpr.mean() / water_density * seconds_per_day * days_per_month * mm_per_m # [kg m-2 s-1] to [mm month-1]\n",
    "    mtpr_s = monthly_mtpr.std() / water_density * seconds_per_day * days_per_month * mm_per_m # [kg m-2 s-1] to [mm month-1]\n",
    "    l_values, l_index = process_era5_means_to_lists(mtpr_m, 'mean', l_values, l_index, 'mtpr', 'mm')\n",
    "    l_values, l_index = process_era5_means_to_lists(mtpr_s, 'std', l_values, l_index, 'mtpr', 'mm')\n",
    "    \n",
    "    # Monthly temperature statistics [C]\n",
    "    monthly_tavg = (ds['t'].resample(time='1D').mean().resample(time='1ME').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tavg_m = monthly_tavg.mean()\n",
    "    tavg_s = monthly_tavg.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(tavg_m, 'mean', l_values, l_index, 'tdavg', 'C')\n",
    "    l_values, l_index = process_era5_means_to_lists(tavg_s, 'std', l_values, l_index, 'tdavg', 'C')\n",
    "    \n",
    "    monthly_tmin = (ds['t'].resample(time='1D').min().resample(time='1ME').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tmin_m = monthly_tmin.mean()\n",
    "    tmin_s = monthly_tmin.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(tmin_m, 'mean', l_values, l_index, 'tdmin', 'C')\n",
    "    l_values, l_index = process_era5_means_to_lists(tmin_m, 'std', l_values, l_index, 'tdmin', 'C')\n",
    "    \n",
    "    monthly_tmax = (ds['t'].resample(time='1D').max().resample(time='1ME').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tmax_m = monthly_tmax.mean()\n",
    "    tmax_s = monthly_tmax.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(tmax_m, 'mean', l_values, l_index, 'tdmax', 'C')\n",
    "    l_values, l_index = process_era5_means_to_lists(tmax_s, 'std', l_values, l_index, 'tdmax', 'C')\n",
    "    \n",
    "    # Monthly shortwave and longwave [W m-2]\n",
    "    monthly_sw = ds['msdwswrf'].resample(time='1ME').mean().groupby('time.month')\n",
    "    sw_m = monthly_sw.mean()\n",
    "    sw_s = monthly_sw.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(sw_m, 'mean', l_values, l_index, 'msdwswrf', 'W m^-2')\n",
    "    l_values, l_index = process_era5_means_to_lists(sw_s, 'std', l_values, l_index, 'msdwswrf', 'W m^-2')\n",
    "    \n",
    "    monthly_lw = ds['msdwlwrf'].resample(time='1ME').mean().groupby('time.month')\n",
    "    lw_m = monthly_lw.mean(dim='time')\n",
    "    lw_s = monthly_lw.std(dim='time')\n",
    "    l_values, l_index = process_era5_means_to_lists(lw_m, 'mean', l_values, l_index, 'msdwlwrf', 'W m^-2')\n",
    "    l_values, l_index = process_era5_means_to_lists(lw_s, 'std', l_values, l_index, 'msdwlwrf', 'W m^-2')\n",
    "\n",
    "    # Surface pressure [Pa]\n",
    "    monthly_sp = ds['sp'].resample(time='1ME').mean().groupby('time.month')\n",
    "    sp_m = monthly_sp.mean() / pa_per_kpa # [Pa] > [kPa]\n",
    "    sp_s = monthly_sp.std() / pa_per_kpa\n",
    "    l_values, l_index = process_era5_means_to_lists(sp_m, 'mean', l_values, l_index, 'sp', 'kPa')\n",
    "    l_values, l_index = process_era5_means_to_lists(sp_s, 'std', l_values, l_index, 'sp', 'kPa')\n",
    "    \n",
    "    # Humidity [-]\n",
    "    monthly_q = ds['q'].resample(time='1ME').mean().groupby('time.month') # specific\n",
    "    q_m = monthly_q.mean()\n",
    "    q_s = monthly_q.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(q_m, 'mean', l_values, l_index, 'q', 'kg kg^-1')\n",
    "    l_values, l_index = process_era5_means_to_lists(q_s, 'std', l_values, l_index, 'q', 'kg kg^-1')\n",
    "    \n",
    "    monthly_rh = ds['rh'].resample(time='1ME').mean().groupby('time.month') # relative\n",
    "    rh_m = monthly_rh.mean()\n",
    "    rh_s = monthly_rh.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(rh_m, 'mean', l_values, l_index, 'rh', 'kPa kPa^-1')\n",
    "    l_values, l_index = process_era5_means_to_lists(rh_s, 'std', l_values, l_index, 'rh', 'kPa kPa^-1')\n",
    "    \n",
    "    # Wind speed [m s-1]\n",
    "    monthly_w = ds['w'].resample(time='1ME').mean().groupby('time.month')\n",
    "    w_m = monthly_w.mean()\n",
    "    w_s = monthly_w.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(w_m, 'mean', l_values, l_index, 'w', 'm s^-1')\n",
    "    l_values, l_index = process_era5_means_to_lists(w_s, 'std', l_values, l_index, 'w', 'm s^-1')\n",
    "    \n",
    "    # Wind direction\n",
    "    monthly_phi = ds['phi'].resample(time='1ME').apply(circmean_group).groupby('time.month')\n",
    "    phi_m = monthly_phi.apply(circmean_group)\n",
    "    phi_s = monthly_phi.apply(circstd_group)\n",
    "    l_values, l_index = process_era5_means_to_lists(phi_m, 'mean', l_values, l_index, 'phi', 'degrees')\n",
    "    l_values, l_index = process_era5_means_to_lists(phi_s, 'std', l_values, l_index, 'phi', 'degrees')\n",
    "    \n",
    "    # --- Long-term statistics (aridity, seasonality, snow)\n",
    "    monthly_ari = ((ds['mper'].resample(time='1ME').mean() * flip_sign) / ds['mtpr'].resample(time='1ME').mean()).groupby('time.month')\n",
    "    ari_m = monthly_ari.mean()\n",
    "    ari_s = monthly_ari.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(ari_m, 'mean', l_values, l_index, 'aridity', '-')\n",
    "    l_values, l_index = process_era5_means_to_lists(ari_s, 'std', l_values, l_index, 'aridity', '-')\n",
    "\n",
    "    ds['snow'] = xr.where(ds['t'] < 273.15, ds['mtpr'],0)\n",
    "    monthly_snow = (ds['snow'].resample(time='1ME').mean() / ds['mtpr'].resample(time='1ME').mean()).groupby('time.month')\n",
    "    snow_m = monthly_snow.mean()\n",
    "    snow_s = monthly_snow.std()\n",
    "    l_values, l_index = process_era5_means_to_lists(snow_m, 'mean', l_values, l_index, 'fracsnow', '-')\n",
    "    l_values, l_index = process_era5_means_to_lists(snow_s, 'std', l_values, l_index, 'fracsnow', '-')\n",
    "\n",
    "    # --- High-frequency statistics (high/low duration/timing/magnitude)\n",
    "    #  Everyone does precip. We'll add temperature too as a drought/frost indicator\n",
    "    #  ERA5 only\n",
    "    \n",
    "    # -- LOW TEMPERATURE\n",
    "    variable  = 't'\n",
    "    low_threshold = 273.15 # K, freezing point\n",
    "    low_condition = ds[variable] < low_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('temp',low_condition,'low',l_values,l_index)\n",
    "    \n",
    "    # -- HIGH TEMPERATURE\n",
    "    # WMO defines a heat wave as a 5-day or longer period with maximum daily temperatures 5C above \n",
    "    # \"standard\" daily max temperature (1961-1990; source:\n",
    "    # https://www.ifrc.org/sites/default/files/2021-06/10-HEAT-WAVE-HR.pdf).\n",
    "    # We define a \"hot day\" therefore as a day with a maximum temperature 5 degrees over the \n",
    "    # the long-term mean maximum temperature.\n",
    "    #   Note: we don't have 1961-1990 data for some stations, so we stick with long-term mean.\n",
    "    #   Note: this will in most cases slightly underestimate heat waves compared to WMO definition\n",
    "    \n",
    "    # First, we identify the long-term mean daily maximum temperature in a dedicated function\n",
    "    high_threshold = create_mean_daily_max_series(ds,var='t')\n",
    "    \n",
    "    # Next, we check if which 't' values are 5 degrees above the long-term mean daily max \n",
    "    #  (\"(ds['t'] > result_array + 5)\"), and resample this to a daily time series \n",
    "    #  (\"resample(time='1D')\") filled with \"True\" if any value in that day was True.\n",
    "    daily_flags = (ds['t'] > high_threshold + 5).resample(time='1D').any()\n",
    "    \n",
    "    # Finally, we reindex these daily flags back onto the hourly time series by filling values\n",
    "    high_condition = daily_flags.reindex_like(ds['t'], method='ffill')\n",
    "    \n",
    "    # Now calculate stats like before\n",
    "    l_values,l_index = calculate_temp_prcp_stats('temp',high_condition,'high',l_values,l_index)\n",
    "    \n",
    "    # -- LOW PRECIPITATION\n",
    "    variable = 'mtpr'\n",
    "    # We'll stick with the original CAMELS definition of low precipitation: < 1 mm day-1\n",
    "    # It may not make too much sense to look at \"dry hours\" so we'll do this analysis at daily step\n",
    "    low_threshold = 1 # [mm d-1]\n",
    "    # Create daily precipitation sum (divided by density, times mm m-1 cancels out)\n",
    "    # [kg m-2 s-1] * [s h-1] / [kg m-3] * [mm m-1] = [mm h-1]\n",
    "    low_condition = (ds[variable] * seconds_per_hour).resample(time='1D').sum() < low_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('prec',low_condition,'low',l_values,l_index,\n",
    "                                             units='days') # this 'units' argument prevents conversion to days inside the functiom\n",
    "    \n",
    "    # -- HIGH PRECIPITATION\n",
    "    # CAMELS: > 5 times mean daily precip\n",
    "    high_threshold = 5 * (ds[variable] * seconds_per_hour).resample(time='1D').sum().mean()\n",
    "    high_condition = (ds[variable] * seconds_per_hour).resample(time='1D').sum() >= high_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('prec',high_condition,'high',l_values,l_index,\n",
    "                                                 units='days')\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3686d712-b932-4cb7-b153-0bfaab2feb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_worldclim(geo_folder, dataset, shp_str, l_values, index):\n",
    "\n",
    "    '''Calculates mean and stdv for tifs of monthly WorldClim values'''\n",
    "\n",
    "    # Define file locations\n",
    "    # Units source: https://www.worldclim.org/data/worldclim21.html\n",
    "    clim_folder = geo_folder / dataset / 'raw'\n",
    "    sub_folders =      ['prec', 'srad',         'tavg', 'tmax', 'tmin', 'vapr', 'wind',   'pet']\n",
    "    sub_folder_units = ['mm',   'kJ m^-2 d^-1', 'C',    'C',    'C',    'kPa',  'm s^-1', 'mm']\n",
    "\n",
    "    # Loop over the files and calculate the stats\n",
    "    for sub_folder, sub_folder_unit in zip(sub_folders, sub_folder_units):\n",
    "        month_files = sorted( glob.glob(str(clim_folder / sub_folder / '*.tif')) )\n",
    "        for month_file in month_files:\n",
    "            month_file = clim_folder / sub_folder / month_file # Construct the full path, because listdir() gives only files\n",
    "            stats = ['mean', 'std']\n",
    "            zonal_out = zonal_stats(shp_str, month_file, stats=stats)\n",
    "\n",
    "            scale, offset = csa.read_scale_and_offset(month_file)\n",
    "            scale, offset = read_scale_and_offset(month_file)\n",
    "            if sub_folder == 'srad':\n",
    "                zonal_out = zonal_stats_unit_conversion(zonal_out,stats,'srad', scale, offset)\n",
    "            l_values = csa.update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "            \n",
    "            month = os.path.basename(month_file).split('_')[3].split('.')[0]\n",
    "            var = os.path.basename(month_file).split('_')[2]\n",
    "            source = 'WorldClim'\n",
    "            if var == 'pet': source = 'WorldClim (derived, Oudin et al., 2005)'\n",
    "            index += [('Climate', f'{var}_mean_month_{month}', f'{sub_folder_unit}',  source),\n",
    "                      ('Climate', f'{var}_stdev_month_{month}', f'{sub_folder_unit}', source)]\n",
    "\n",
    "    return l_values, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1af9044e-e75b-48a7-815a-c8869f6212b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_forest_height(geo_folder, dataset, shp_str, l_values):\n",
    "\n",
    "    '''Calculates mean, min, max and stdv for forest height 2000 and 2020 tifs'''\n",
    "\n",
    "    # Year 2000 min, mean, max, stdev\n",
    "    tif = str( geo_folder / dataset / 'raw' / 'forest_height_2000.tif' )\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    zonal_out = zonal_stats(shp, tif, stats=stats)\n",
    "    scale,offset = read_scale_and_offset(tif)\n",
    "    l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "\n",
    "    # Year 2020 mean, stdev\n",
    "    tif = geo_folder / dataset / 'raw' / 'forest_height_2020.tif'\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    zonal_out = zonal_stats(shp, tif, stats=stats)\n",
    "    scale,offset = read_scale_and_offset(tif)\n",
    "    l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "\n",
    "    return l_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b4204-865e-45ad-85f5-aff233536d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_lai(geo_folder, dataset, temp_path, shp_str, l_values):\n",
    "\n",
    "    '''Calculates mean and stdv for tifs of monthly LAI values'''\n",
    "\n",
    "    # Calculate monthly mean maps (e.g. mean Jan, Feb, etc.)\n",
    "    lai_folder = geo_folder / dataset / 'raw' \n",
    "    lai_files = sorted( glob.glob(str(lai_folder / '*.tif')) ) # Find LAI files\n",
    "    month_files = calculate_monthly_lai_maps(lai_files, temp_path) # Create 12 monthly maps\n",
    "\n",
    "    # Monthly mean, stdev LAI; monthly mean, stdev GVF\n",
    "    for month_file in month_files:\n",
    "        stats = ['mean', 'std']\n",
    "        zonal_out = zonal_stats(str(shp_lump_path), month_file, stats=stats)\n",
    "        scale, offset = read_scale_and_offset(month_file)\n",
    "        scale,offset = read_scale_and_offset(month_file)\n",
    "        l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "    \n",
    "    # Clear temp folder\n",
    "    files_to_remove = os.listdir(temp_path)\n",
    "    for file_to_remove in files_to_remove:\n",
    "        file_remove_path = os.path.join(temp_path, file_to_remove)\n",
    "        if os.path.isfile(file_remove_path):\n",
    "            os.remove(file_remove_path)\n",
    "    \n",
    "    return l_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d63472-1778-4ab1-9eee-11c597265944",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ecf09c0-e431-4dd9-8e93-522ddbc9b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from osgeo import gdal, osr\n",
    "import os\n",
    "import pandas as pd\n",
    "from rasterstats import zonal_stats\n",
    "from scipy.stats import circmean, circstd, skew, kurtosis\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180ad99-60a7-43ee-aab4-78f7f1d6b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_open_water_stats(gdf, att, mask, l_values, l_index):\n",
    "    '''Calculates min, mean, max, std and total att ('Lake_area', 'Vol_total'), optionally using a reservoir mask'''\n",
    "\n",
    "    # Setup\n",
    "    if mask == 'all':\n",
    "        mask = gdf.index >= 0 # Selects everything if no mask is provided\n",
    "        type = 'open_water' # no reservoir mask, hence we're working with lakes\n",
    "    elif mask == 'reservoir':\n",
    "        mask = gdf['Lake_type'] == 2\n",
    "        type = 'reservoir'\n",
    "    if att == 'Lake_area':\n",
    "        units = 'km^2' # https://data.hydrosheds.org/file/technical-documentation/HydroLAKES_TechDoc_v10.pdf\n",
    "        att_d = 'area'\n",
    "    elif att == 'Vol_total':\n",
    "        units = 'million~m^3' # https://data.hydrosheds.org/file/technical-documentation/HydroLAKES_TechDoc_v10.pdf\n",
    "        att_d = 'volume'\n",
    "\n",
    "    # Stats\n",
    "    l_values.append(gdf[mask][att].min())\n",
    "    l_index.append(('Open water', f'{type}_{att_d}_min',  f'{units}', 'HydroLAKES'))\n",
    "    l_values.append(gdf[mask][att].mean())\n",
    "    l_index.append(('Open water', f'{type}_{att_d}_mean',  f'{units}', 'HydroLAKES'))\n",
    "    l_values.append(gdf[mask][att].max())\n",
    "    l_index.append(('Open water', f'{type}_{att_d}_max',  f'{units}', 'HydroLAKES'))\n",
    "    l_values.append(gdf[mask][att].std())\n",
    "    l_index.append(('Open water', f'{type}_{att_d}_std',  f'{units}', 'HydroLAKES'))\n",
    "    l_values.append(gdf[mask][att].sum()) # total\n",
    "    l_index.append(('Open water', f'{type}_{att_d}_total',  f'{units}', 'HydroLAKES'))\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c8d1a1-cf4d-4965-9569-ccca01c136d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_attributes(tif,l_values,l_index):\n",
    "    '''Calculates circular statistics for MERIT Hydro aspect'''\n",
    "\n",
    "    # Get data as a masked array - we know there are no-data values outside the catchment boundaries\n",
    "    aspect = csa.get_geotif_data_as_array(tif)\n",
    "    no_data = get_geotif_nodata_value(tif)\n",
    "    masked_aspect = np.ma.masked_array(aspect, aspect == no_data)\n",
    "\n",
    "    ## Calculate the statistics\n",
    "    l_values.append(masked_aspect.min())\n",
    "    l_index.append(('Topography', 'merit_hydro_aspect_min',  'degrees', 'MERIT Hydro'))\n",
    "    \n",
    "    l_values.append(circmean(masked_aspect,high=360))\n",
    "    l_index.append(('Topography', 'merit_hydro_aspect_mean',  'degrees', 'MERIT Hydro'))\n",
    "\n",
    "    l_values.append(circstd(masked_aspect,high=360))\n",
    "    l_index.append(('Topography', 'merit_hydro_aspect_std',  'degrees', 'MERIT Hydro'))\n",
    "\n",
    "    l_values.append(masked_aspect.max())\n",
    "    l_index.append(('Topography', 'merit_hydro_aspect_max',  'degrees', 'MERIT Hydro'))\n",
    "    \n",
    "    return l_values,l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e848e1-6858-4a5f-aebd-56a13af6d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_river_attributes(riv_str, l_values, l_index, area):\n",
    "    \n",
    "    '''Calculates topographic attributes from a MERIT Hydro Basins river polygon'''\n",
    "\n",
    "    # Load shapefiles\n",
    "    river = gpd.read_file(riv_str)\n",
    "    river = river.set_index('COMID')\n",
    "    \n",
    "    # Raw data\n",
    "    stream_lengths = []\n",
    "    headwaters = river[river['maxup'] == 0] # identify reaches with no upstream\n",
    "    for COMID in headwaters.index:\n",
    "        stream_length = 0\n",
    "        while COMID in river.index:\n",
    "            stream_length += river.loc[COMID]['lengthkm'] # Add the length of the current segment\n",
    "            COMID = river.loc[COMID]['NextDownID'] # Get the downstream reach\n",
    "        stream_lengths.append(stream_length) # If we get here we ran out of downstream IDs\n",
    "    \n",
    "    # Stats\n",
    "    stream_total = river['lengthkm'].sum()\n",
    "    stream_lengths = np.array(stream_lengths)\n",
    "    l_values.append(stream_lengths.min())\n",
    "    l_index.append(('Topography', 'stream_length_min',  'km', 'MERIT Hydro Basins'))\n",
    "    l_values.append(stream_lengths.mean())\n",
    "    l_index.append(('Topography', 'stream_length_mean',  'km', 'MERIT Hydro Basins'))\n",
    "    l_values.append(stream_lengths.max())\n",
    "    l_index.append(('Topography', 'stream_length_max',  'km', 'MERIT Hydro Basins'))\n",
    "    l_values.append(stream_lengths.std())\n",
    "    l_index.append(('Topography', 'stream_length_std',  'km', 'MERIT Hydro Basins'))\n",
    "    l_values.append(stream_total)\n",
    "    l_index.append(('Topography', 'stream_length_total',  'km', 'MERIT Hydro Basins'))\n",
    "    \n",
    "    # Order\n",
    "    l_values.append(river['order'].max())\n",
    "    l_index.append(('Topography', 'steam_order_max',  '-', 'MERIT Hydro Basins'))\n",
    "\n",
    "    # Derived\n",
    "    density = stream_total/area\n",
    "    elongation = 2*np.sqrt(area/np.pi)/stream_lengths.max()\n",
    "    l_values.append(density)\n",
    "    l_index.append(('Topography', 'stream_density',  'km^-1', 'Derived'))\n",
    "    l_values.append(elongation)\n",
    "    l_index.append(('Topography', 'elongation_ratio','-', 'Derived'))\n",
    "    \n",
    "    return l_values,l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4992b34-8064-4536-8be8-89bab8eac544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geotif_nodata_value(tif):\n",
    "    with rasterio.open(tif) as src:\n",
    "        nodata_value = src.nodata\n",
    "    return nodata_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e664b557-93a8-4a10-9151-03441bc91fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_scale_and_offset(tif):\n",
    "    scale,offset = read_scale_and_offset(tif) # just to check we don't have any scale/offset going on\n",
    "    if scale is None: scale = 1 # If scale is undefined that means we simply multiply by 1\n",
    "    if offset is None: offset = 0 # Undefined offset > add 0\n",
    "    if not (scale == 1) and not (offset == 0):\n",
    "        print(f'--- WARNING: check_scale_and_offset(): scale or offset not 1 or 0 respectively.')\n",
    "    return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060773f4-11fd-4e8c-ab53-344abec5d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_dict(source):\n",
    "    '''Contains dictionaries for categorical variables'''\n",
    "    \n",
    "    if source == 'GLCLU 2019':\n",
    "        cat_dict = {1: 'true_desert',\n",
    "                    2: 'semi_arid',\n",
    "                    3: 'dense_short_vegetation',\n",
    "                    4: 'open_tree_cover',\n",
    "                    5: 'dense_tree_cover',\n",
    "                    6: 'tree_cover_gain',\n",
    "                    7: 'tree_cover_loss',\n",
    "                    8: 'salt_pan',\n",
    "                    9: 'wetland_sparse_vegetation',\n",
    "                   10: 'wetland_dense_short_vegetation',\n",
    "                   11: 'wetland_open_tree_cover',\n",
    "                   12: 'wetland_dense_tree_cover',\n",
    "                   13: 'wetland_tree_cover_gain',\n",
    "                   14: 'wetland_tree_cover_loss',\n",
    "                   15: 'ice',\n",
    "                   16: 'water',\n",
    "                   17: 'cropland',\n",
    "                   18: 'built_up',\n",
    "                   19: 'ocean',\n",
    "                   20: 'no_data'}\n",
    "\n",
    "    if source == 'MCD12Q1.061':\n",
    "        cat_dict = {1: 'evergreen_needleleaf_forest',\n",
    "                    2: 'evergreen_broadleaf_forest',\n",
    "                    3: 'deciduous_needleleaf_forest',\n",
    "                    4: 'deciduous_broadleaf_forest',\n",
    "                    5: 'mixed_forest',\n",
    "                    6: 'closed_shrubland',\n",
    "                    7: 'open_shrubland',\n",
    "                    8: 'woody_savanna',\n",
    "                    9: 'savanna',\n",
    "                   10: 'grassland',\n",
    "                   11: 'permanent_wetland',\n",
    "                   12: 'cropland',\n",
    "                   13: 'urban_and_built_up',\n",
    "                   14: 'cropland_natural_mosaic',\n",
    "                   15: 'permanent_snow_ice',\n",
    "                   16: 'barren',\n",
    "                   17: 'water',\n",
    "                  255: 'unclassified'}\n",
    "\n",
    "    if source == 'LGRIP30':\n",
    "        cat_dict = {0: 'water',\n",
    "                    1: 'non_cropland',\n",
    "                    2: 'irrigated_cropland',\n",
    "                    3: 'rainfed_cropland'}\n",
    "    \n",
    "    return cat_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7597647-5d0c-475b-bc59-8e1882e5c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_values_list_with_categorical(l_values, l_index, zonal_out, source, prefix=''):\n",
    "    '''Maps a zonal histogram of categorical classes onto descriptions and adds to lists'''\n",
    "\n",
    "    # Get the category definitions\n",
    "    cat_dict = get_categorical_dict(source)    \n",
    "\n",
    "    # Find the total number of classified pixels\n",
    "    total_pixels = 0\n",
    "    for land_id,count in zonal_out[0].items():\n",
    "        total_pixels += count\n",
    "    \n",
    "    # Loop over all categories and see what we have in this catchment\n",
    "    for land_id,text in cat_dict.items():\n",
    "        land_prct = 0\n",
    "        if land_id in zonal_out[0].keys():\n",
    "            land_prct = zonal_out[0][land_id] / total_pixels\n",
    "        l_values.append(land_prct)\n",
    "        l_index.append(('Land cover', f'{prefix}{text}_fraction', '-', f'{source}'))\n",
    "\n",
    "    return l_values,l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681110fc-128c-4599-b82b-34d42c53dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aridity_and_fraction_snow_from_worldclim(geo_folder, dataset):\n",
    "    \n",
    "    '''Calculates aridity and fraction snow maps from WorldClim data'''\n",
    "\n",
    "    # Find files\n",
    "    clim_folder = geo_folder / dataset / 'raw'\n",
    "    prc_files = sorted( glob.glob(str(clim_folder / 'prec' / '*.tif')) ) # [mm]\n",
    "    pet_files = sorted( glob.glob(str(clim_folder / 'pet' / '*.tif')) ) # [mm]\n",
    "    tmp_files = sorted( glob.glob(str(clim_folder / 'tavg' / '*.tif')) ) # [C]\n",
    "    \n",
    "    # Make the output locations\n",
    "    ari_folder = clim_folder / 'aridity'\n",
    "    ari_folder.mkdir(parents=True, exist_ok=True)\n",
    "    snow_folder = clim_folder / 'fraction_snow'\n",
    "    snow_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Loop over files and calculate aridity\n",
    "    for prc_file, pet_file, tmp_file in zip(prc_files, pet_files, tmp_files):\n",
    "\n",
    "        # Define month\n",
    "        month = prc_file.split('_')[-1].split('.')[0] # 'wc2.1_30s_prec_01.tif' > '01', ..., '12'\n",
    "        month_ix = int(month)-1 # -1 to account for zero-based indexing: Jan value is at index 0, not 1\n",
    "\n",
    "        # Load data\n",
    "        prc_path = clim_folder / 'prec' / prc_file\n",
    "        pet_path = clim_folder / 'pet'  / pet_file\n",
    "        tmp_path = clim_folder / 'tavg' / tmp_file      \n",
    "        prc = get_geotif_data_as_array(prc_path) # [mm]\n",
    "        pet = get_geotif_data_as_array(pet_path) # [mm]\n",
    "        tmp = get_geotif_data_as_array(tmp_path) # [C]\n",
    "\n",
    "        # Calculate variables\n",
    "        snow = np.where(tmp < 0, prc, 0) # get snow first, because this needs precip and we'll (possibly) be updating the precip value below\n",
    "        if (prc == 0).any():\n",
    "            prc[prc == 0] = 1 # add 1 mm to avoid divide by zero errors\n",
    "        ari = pet/prc # [-]\n",
    "        frac_snow = snow/prc # [-]\n",
    "\n",
    "        # Define output file name and write to disk\n",
    "        ari_name = prc_file.replace('prec','aridity')\n",
    "        ari_file = str(ari_folder / ari_name)\n",
    "        write_geotif_sameDomain(prc_path, ari_file, ari)\n",
    "        snow_name = prc_file.replace('prec','fraction_snow')\n",
    "        snow_file = str(snow_folder / snow_name)\n",
    "        write_geotif_sameDomain(prc_path, snow_file, frac_snow)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22da84d-7099-4744-9b09-1ec1dcf270f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonal_stats_unit_conversion(zonal_out, stat_to_convert, variable, scale, offset):\n",
    "    '''Takes a zonal_stats output and converts the units of any variable listed in stat_to_convert'''\n",
    "\n",
    "    # Constants\n",
    "    j_per_kj = 1000 # [J kJ-1]\n",
    "    seconds_per_day = 24*60*60 # [s day-1]\n",
    "\n",
    "    # Keep track of scale and offset\n",
    "    # Update scale and offset to usable values - we get None if scale and offset are 1 and 0 in the GeoTIFF\n",
    "    if scale is None: scale = 1 # If scale is undefined that means we simply multiply by 1\n",
    "    if offset is None: offset = 0 # Undefined offset > add 0\n",
    "\n",
    "    #  We'll need code to handle this if these aren't 1 and 0 respectively\n",
    "    if (scale != 1) or (offset !=0):\n",
    "        print(f'--- ERROR: zonal_stats_unit_conversion(): code needed to deal with scale {scale} and offset {offset}')\n",
    "        return -1\n",
    "\n",
    "    # Select conversion factor\n",
    "    if variable == 'srad':\n",
    "        # From [kJ m-2 day-1] to [W m-2]:\n",
    "        # [kJ m-2 day-1] * 1/[s day-1] * [J kJ-1] = [J m-2 s-1] = [W m-2]\n",
    "        c_factor = 1/seconds_per_day * j_per_kj\n",
    "\n",
    "    # loop over all list elements\n",
    "    for list_id in range(0,len(zonal_out)):\n",
    "        zonal_dict = zonal_out[list_id]\n",
    "\n",
    "        # Loop over dictionary entries\n",
    "        for key,val in zonal_dict.items():\n",
    "            if k in stat_to_convert:\n",
    "                zonal_out[list_id][key] = zonal_out[list_id][key] * c_factor\n",
    "\n",
    "    return zonal_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9b9c7de4-1e98-4626-a041-241145c73a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oudin_pet_from_worldclim(geo_folder, dataset, debug=False):\n",
    "\n",
    "    '''Calculates PET estimates from WorldClim data, using the Oudin (2005; 10.1016/j.jhydrol.2004.08.026) formulation'''\n",
    "\n",
    "    # Constants\n",
    "    lh = 2.45 # latent heat flux, MJ kg-1\n",
    "    rw = 1000 # rho water, kg m-3\n",
    "    days_per_month = [31, 28.25, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31] # days month-1\n",
    "    mm_per_m = 1000 # mm m-1\n",
    "    \n",
    "    # Find files\n",
    "    clim_folder = geo_folder / dataset / 'raw'\n",
    "    srad_files = sorted( glob.glob(str(clim_folder / 'srad' / '*.tif')) ) # website says [kJ m-2 day-1], but paper says [MJ m-2 day-1]\n",
    "    tavg_files = sorted( glob.glob(str(clim_folder / 'tavg' / '*.tif')) ) # C\n",
    "\n",
    "    # Make the output location\n",
    "    pet_folder = clim_folder / 'pet'\n",
    "    pet_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Loop over files and calculate PET\n",
    "    for srad_file, tavg_file in zip(srad_files, tavg_files):\n",
    "\n",
    "        # Define month\n",
    "        month = srad_file.split('_')[-1].split('.')[0] # 'wc2.1_30s_srad_01.tif' > '01', ..., '12'\n",
    "        month_ix = int(month)-1 # -1 to account for zero-based indexing: Jan value is at index 0, not 1\n",
    "        \n",
    "        # Load data\n",
    "        srad_path = clim_folder / 'srad' / srad_file\n",
    "        tavg_path = clim_folder / 'tavg' / tavg_file      \n",
    "        srad = get_geotif_data_as_array(srad_path) / 1000 # [kJ m-2 day-1] / 1000 = [MJ m-2 day-1]\n",
    "        tavg = get_geotif_data_as_array(tavg_path)\n",
    "        \n",
    "        # Oudin et al, 2005, Eq. 3\n",
    "        pet = np.where(tavg+5 > 0, (srad / (lh*rw)) * ((tavg+5)/100) * mm_per_m, 0) # mm day-1\n",
    "        pet_month = pet * days_per_month[month_ix] # mm month-1\n",
    "        if debug: print(f'Calculating monthly PET for month {month} at day-index {month_ix}')\n",
    "\n",
    "        # Define output file name and write to disk\n",
    "        pet_name = srad_file.replace('srad','pet')\n",
    "        pet_file = str(pet_folder / pet_name)\n",
    "        write_geotif_sameDomain(srad_path, pet_file, pet_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e7499d25-ef22-40e4-a506-6bcc351348f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_values_list(l_values, stats, zonal_out, scale, offset):\n",
    "\n",
    "    # Update scale and offset to usable values\n",
    "    if scale is None: scale = 1 # If scale is undefined that means we simply multiply by 1\n",
    "    if offset is None: offset = 0 # Undefined offset > add 0\n",
    "\n",
    "    # We loop through the calculated stats in a pre-determined order:\n",
    "    # 1. min\n",
    "    # 2. mean\n",
    "    # 3. max\n",
    "    # 4. stdev\n",
    "    # 5. ..\n",
    "    if 'min' in stats:  l_values.append(zonal_out[0]['min']  * scale + offset)\n",
    "    if 'mean' in stats: l_values.append(zonal_out[0]['mean'] * scale + offset)\n",
    "    if 'max' in stats:  l_values.append(zonal_out[0]['max']  * scale + offset)\n",
    "    if 'std' in stats:  l_values.append(zonal_out[0]['std']  * scale + offset)\n",
    "\n",
    "    return l_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "382324e2-789c-4e3a-ac08-18d777e689c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_scale_and_offset(geotiff_path):\n",
    "    # Open the GeoTIFF file\n",
    "    dataset = gdal.Open(geotiff_path)\n",
    "\n",
    "    if dataset is None:\n",
    "        raise FileNotFoundError(f\"File not found: {geotiff_path}\")\n",
    "\n",
    "    # Get the scale and offset values\n",
    "    scale = dataset.GetRasterBand(1).GetScale()\n",
    "    offset = dataset.GetRasterBand(1).GetOffset()\n",
    "\n",
    "    # Close the dataset\n",
    "    dataset = None\n",
    "\n",
    "    return scale, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ff0619b3-e43b-418d-adab-2e946c3f08d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lai_files_by_date(files, last_n_years=[], last_n_months=[], last_n_days=[],\n",
    "                                    years=[], months=[], days=[]):\n",
    "\n",
    "    '''Filters list of LAI file names by last n years/months/days and/or by year/month/day x.\n",
    "       Assumes date is given as 'yyyymmdd_*.tif', as part of the filename.\n",
    "       Use years/months/days (input as list) to subset further.'''\n",
    "\n",
    "    # Check inputs\n",
    "    if (last_n_years and last_n_months) or \\\n",
    "       (last_n_years and last_n_days) or \\\n",
    "       (last_n_months and last_n_days):\n",
    "        print('WARNING: filter_lai_files_by_date(): specify only one of last_n_years, last_n_months, last_n_days')\n",
    "        return\n",
    "\n",
    "    # Create a DatetimeIndex from filenames\n",
    "    dates = []\n",
    "    for file in files:\n",
    "        file_name = os.path.basename(file)\n",
    "        yyyymmdd = file_name[0:8]\n",
    "        dates.append(yyyymmdd)\n",
    "    dti = pd.to_datetime(dates,format='%Y%m%d')\n",
    "\n",
    "    # Set the first entry\n",
    "    start_date = dti[0]\n",
    "    \n",
    "    # Find the last entry\n",
    "    last_year  = dti[-1].year\n",
    "    last_month = dti[-1].month\n",
    "    last_day   = dti[-1].day\n",
    "    \n",
    "    # Select the last n entries\n",
    "    if last_n_years:    start_date = dti[-1] - relativedelta(years = last_n_years)\n",
    "    elif last_n_months: start_date = dti[-1] - relativedelta(months = last_n_months)\n",
    "    elif last_n_days:   start_date = dti[-1] - relativedelta(days = last_n_days)\n",
    "    last_n = (dti >= start_date) & (dti <= dti[-1])\n",
    "\n",
    "    # Specify filters to include all if no specific years/months/days were requested\n",
    "    if not years:  years  = list(set(dti.year))  # i.e. filter to include all unique years in dti, \\\n",
    "    if not months: months = list(set(dti.month)) #    else use user input\n",
    "    if not days:   days   = list(set(dti.day))\n",
    "    mask = dti.year.isin(years) & dti.month.isin(months) & dti.day.isin(days)\n",
    "\n",
    "    # Return the filtered list\n",
    "    return [file for file, bool1, bool2 in zip(files,last_n,mask) if bool1 and bool2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e05e737-0a94-4c5d-9ac5-2fb1f06af17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geotif_data_as_array(file, band=1):\n",
    "    ds = gdal.Open(file) # open the file\n",
    "    band = ds.GetRasterBand(band) # get the data band\n",
    "    data = band.ReadAsArray() # convert to numpy array for further manipulation   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a4c52d08-25f2-4771-8632-db225075f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_data_range(data,min,max,replace_with='limit'):\n",
    "\n",
    "    '''Clamps data at min and max values'''\n",
    "\n",
    "    if replace_with =='limit':\n",
    "        data[data<min] = min\n",
    "        data[data>max] = max\n",
    "    else:\n",
    "        data[data<min] = replace_with\n",
    "        data[data>max] = replace_with\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e2028938-66f1-4887-ae77-075606c7dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_geotif_sameDomain(src_file,des_file,des_data):\n",
    "    \n",
    "    # load the source file to get the appropriate attributes\n",
    "    src_ds = gdal.Open(src_file)\n",
    "    \n",
    "    # get the geotransform\n",
    "    des_transform = src_ds.GetGeoTransform()\n",
    "\n",
    "    # Get the scale factor from the source metadata\n",
    "    scale_factor = src_ds.GetRasterBand(1).GetScale()\n",
    "    offset = src_ds.GetRasterBand(1).GetOffset()\n",
    "    \n",
    "    # get the data dimensions\n",
    "    ncols = des_data.shape[1]\n",
    "    nrows = des_data.shape[0]\n",
    "    \n",
    "    # make the file\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    dst_ds = driver.Create(des_file,ncols,nrows,1,gdal.GDT_Float32, options = [ 'COMPRESS=DEFLATE' ])\n",
    "    dst_ds.GetRasterBand(1).WriteArray( des_data )\n",
    "\n",
    "    # Set the scale factor in the destination band, if they were specified in the source\n",
    "    if scale_factor: dst_ds.GetRasterBand(1).SetScale(scale_factor)\n",
    "    if offset: dst_ds.GetRasterBand(1).SetOffset(offset)\n",
    "    \n",
    "    # Set the geotransform\n",
    "    dst_ds.SetGeoTransform(des_transform)\n",
    "\n",
    "    # Set the projection\n",
    "    wkt = src_ds.GetProjection()\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromWkt(wkt)\n",
    "    dst_ds.SetProjection( srs.ExportToWkt() )\n",
    "    \n",
    "    # close files\n",
    "    src_ds = None\n",
    "    des_ds = None\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3c78f63f-6a8b-4c4a-ba36-f783d4afc9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monthly_lai_maps(lai_files, des_path):\n",
    "    des_files = []\n",
    "    for month in range(1,13):\n",
    "\n",
    "        # Define valid data range\n",
    "        # See docs, Table 4: https://lpdaac.usgs.gov/documents/926/MOD15_User_Guide_V61.pdf\n",
    "        modis_min = 0\n",
    "        modis_max = 100\n",
    "    \n",
    "        # Get the files we have for this month, for the last n years\n",
    "        #print(f'Processing month {month:02d}')\n",
    "        month_files = filter_lai_files_by_date(lai_files, months=[month])\n",
    "    \n",
    "        # Remove the one file we know is incomplete, 2022-10-16\n",
    "        month_files = [file for file in month_files if '20221016' not in file]\n",
    "        \n",
    "        # Load the data as numpy arrays, stack vertically, and find the mean value (ignoring nan)\n",
    "        data = [get_geotif_data_as_array(file) for file in month_files] # Get data as uint8\n",
    "        stacked = np.dstack(data) # Create a 3D stack\n",
    "        stacked_msk = np.ma.masked_array(stacked, mask=(stacked<modis_min) | (stacked>modis_max)) # Retain valid values only\n",
    "        mean_lai = np.ma.mean(stacked_msk, axis=2)\n",
    "    \n",
    "        # Define the no-data locations\n",
    "        #mean_all = np.nanmean(stacked, axis=2) # Any pixel that consistently has no-data in the source files (>= 249) should have a >= 249 mean\n",
    "        #mean_lai[mean_all >= 249] = mean_all[mean_all >= 249] # Place the no-data values in the new monthly-mean-lai file\n",
    "        \n",
    "        # Define output file name and write to disk\n",
    "        src_file = month_files[0] # We use this to copy over domain, projection, data scaling, etc\n",
    "        des_file = str( des_path / f'month_mean_{month:02d}_MOD_Grid_MOD15A2H_Lai_500m.tif' )\n",
    "        write_geotif_sameDomain(src_file, des_file, mean_lai)\n",
    "        des_files.append(des_file)\n",
    "    return des_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ad7eba19-9572-454f-81d9-6971d032ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to apply circmean to each group\n",
    "# Without this, xarray chokes on dimensions when converting the circmean output back into something with the right month indices\n",
    "def circmean_group(group):\n",
    "    return xr.DataArray(circmean(group, high=360, low=0), name='phi')\n",
    "\n",
    "def circstd_group(group):\n",
    "    return xr.DataArray(circstd(group, high=360, low=0), name='phi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa7fb4-9c67-4ee1-8ff3-046385c4f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing function to update the two main lists we're populating\n",
    "def process_era5_means_to_lists(da, l_values, l_index, var, unit):\n",
    "    '''Takes an xarray data array with monthly means and processes into l_values and l_index lists'''\n",
    "    for month in range(1,13):\n",
    "        val = da.sel(month=month).values.flatten()[0]\n",
    "        txt = (f'Climate', f'{var}_mean_month_{month:02}', f'{unit}', 'ERA5')\n",
    "        l_values += [val] # Needs to be this way because we're appending to a list\n",
    "        l_index  += [txt]\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad326f-a555-4e94-a0dd-7195037869c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to map months to seasons\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'djf'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'mam'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'jja'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'son'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a678b51-a610-4f2b-a4de-ee6ad82eb4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds duration counts in a vector of True/False values\n",
    "def find_durations(condition):\n",
    "    '''Counts the duration(s) of True values in an xarray dataseries'''\n",
    "\n",
    "    previous = False\n",
    "    duration = 0\n",
    "    durations = []\n",
    "    for flag in condition.values:\n",
    "        \n",
    "        # Time step where we reset the count\n",
    "        if not previous and flag:\n",
    "            duration = 0 # New first timestep where condition = True, so duration = 0\n",
    "            previous = True\n",
    "    \n",
    "        # Time step where we're in a sequence of condition = True\n",
    "        if previous and flag:\n",
    "            duration += 1 # Update duration, implicitly retain previous = True by not changing it\n",
    "        \n",
    "        # Time step where we reach the end of a condition = True duration\n",
    "        if previous and not flag:\n",
    "            durations.append(duration) # Save the maximum duration length to list\n",
    "            previous = False # Update previous; duration will be reset next time we encounter a condition = True\n",
    "    \n",
    "        # Time step where we're in a continuation of condition = False\n",
    "        if not previous and not flag:\n",
    "            continue # do nothing\n",
    "    \n",
    "    return np.array(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bce3f5-d19e-4cf1-baf7-7c00ce1381d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds a long-term mean daily maximum temperature in a way that doesn't rely on \n",
    "#  time.dt.dayofyear, because the latter will have Dec-31 as day 365 in non-leap-years,\n",
    "#  and as 366 in leap years. Hence the day-of-year means do not use the same dates for \n",
    "#  a given DoY in leap years as they do in regular years.\n",
    "def create_mean_daily_max_series(ds,var='t'):\n",
    "    '''Finds the long-term mean daily maximum value of a variable'''\n",
    "    \n",
    "    # Create an array of all the month-days we have (e.g. 1949-12-31 00:00 becomes 1231)\n",
    "    month_days_all = ds.time.dt.month * 100 + ds.time.dt.day\n",
    "\n",
    "    # Loop over the unique month-days we have, and find the mean daily maximum value for each\n",
    "    month_days_unique = np.unique(month_days_all)\n",
    "    mean_daily_max = []\n",
    "    for month_day in month_days_unique:\n",
    "        val = ds[var].sel(time=(month_days_all==month_day)).groupby('time.year').max().mean().values\n",
    "        mean_daily_max.append(val)\n",
    "\n",
    "    # Convert the list to an array for further processing\n",
    "    mean_daily_max = np.array(mean_daily_max)\n",
    "\n",
    "    # Extract month_day values from the long xarray DataArray\n",
    "    month_day_values = month_days_all.values\n",
    "    \n",
    "    # Find the indices of each month_day in the unique_month_days array\n",
    "    indices = np.searchsorted(month_days_unique, month_day_values)\n",
    "    \n",
    "    # Use the indices to extract the corresponding data values\n",
    "    corresponding_data_values = mean_daily_max[indices]\n",
    "    \n",
    "    # Create a new DataArray with the corresponding data values\n",
    "    result_array = xr.DataArray(corresponding_data_values, \n",
    "                                coords=month_days_all.coords, \n",
    "                                dims=month_days_all.dims)\n",
    "\n",
    "    return result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42539dd6-391f-4ede-b38a-4ccf695e8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General processing of high/low temperature/precipitation frequency/duration/timing stats\n",
    "def calculate_temp_prcp_stats(var, condition, hilo, l_values,l_index,\n",
    "                              dataset='ERA5', units='hours'):\n",
    "    \n",
    "    '''Calculates frequency (mean) and duration (mean, median, skew, kurtosis) \n",
    "        of temperature/precipitation periods'''\n",
    "\n",
    "    # Constants. We want everything in [days] for consistency with original CAMELS\n",
    "    hours_per_day = 24 # [hours day-1]\n",
    "    days_per_year = 365.25 # [days year-1]\n",
    "\n",
    "    # Calculate frequencies\n",
    "    freq = condition.mean(dim='time') * days_per_year # [-] * [days year-1]\n",
    "    l_values.append(freq.values[0])\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_freq', 'days year^-1', dataset) )\n",
    "    \n",
    "    # Calculate duration statistics\n",
    "    durations = find_durations(condition) # [time steps]\n",
    "    if units == 'hours':\n",
    "        durations = durations / hours_per_day # [days] = [hours] / [hours day-1]\n",
    "    l_values.append(np.mean(durations)) # [days]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_mean', 'days', dataset) ) # Consistency with\n",
    "    l_values.append(np.median(durations)) # [days]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_median', 'days', dataset) )\n",
    "    l_values.append(skew(durations)) # [-]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_skew', '-', dataset) )\n",
    "    l_values.append(kurtosis(durations)) # [-]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_kurtosis', '-', dataset) )\n",
    "\n",
    "    # Calculate timing statistic\n",
    "    condition['season'] = ('time', \n",
    "        [get_season(month) for month in condition['time.month'].values]) # add seasons\n",
    "    max_season_id = condition.groupby('season').sum().argmax(dim='season') # find season with most True values\n",
    "    l_values.append(condition.season[max_season_id].values[0]) # add season abbrev\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_timing', 'season', dataset) )\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8cccc-dc64-4df5-af73-7659280451cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_dataset_to_max_full_years(ds, time='time', debug=False) -> xr.Dataset:\n",
    "\n",
    "    # Find final and end years\n",
    "    start_year = ds[time][0].dt.year.values\n",
    "    final_year = ds[time][-1].dt.year.values\n",
    "    final_timestamp = ds[time][-1]\n",
    "\n",
    "    # Find the first occurrence of Jan-01 00:00 as start of the subset\n",
    "    for year in range(start_year,final_year):\n",
    "        \n",
    "        # Define the initial timestamp to check\n",
    "        start_timestamp = pd.Timestamp(year,1,1,0,0,0)\n",
    "        if debug: print(f'checking {start_timestamp}')\n",
    "\n",
    "        # Select the subset of the dataset for the current duration\n",
    "        # Note: if either start or final are not part of the time series,\n",
    "        #  this will silently just use whatever is available\n",
    "        subset_ds = ds.sel(time=slice(start_timestamp, final_timestamp))\n",
    "\n",
    "        # Check if we actually selected the duration we requested\n",
    "        subset_start = pd.Timestamp(subset_ds[time][0].values)\n",
    "        if subset_start == start_timestamp:\n",
    "            subset_start_year = year # Keep track of where we start\n",
    "            break # stop searching. We've found the first occurrence of Jan-01\n",
    "\n",
    "    # Find the last occurrence of Dec-31 23:00 as end of the subset\n",
    "    for year in range(final_year,subset_start_year,-1):\n",
    "\n",
    "        # Define the initial timestamp to check\n",
    "        end_timestamp = pd.Timestamp(year,12,31,23,0,0)\n",
    "        if debug: print(f'checking {end_timestamp}')\n",
    "\n",
    "        # Select the subset of the dataset for the current duration\n",
    "        # Note: if either start or final are not part of the time series,\n",
    "        #  this will silently just use whatever is available\n",
    "        subset_ds = ds.sel(time=slice(start_timestamp, end_timestamp))\n",
    "\n",
    "        # Check if we actually selected the duration we requested\n",
    "        subset_end = pd.Timestamp(subset_ds[time][-1].values)\n",
    "        if subset_end == end_timestamp:\n",
    "            subset_end_year = year # Keep track of where we start\n",
    "            break # stop searching. We've found the first occurrence of Jan-01\n",
    "\n",
    "    # Now check if we have selected a zero-year period\n",
    "    # This would imply we have less than a full year of data\n",
    "    # In this case, just return the original data set with a warning\n",
    "    if len(subset_ds) == 0:\n",
    "        print(f'--- WARNING: subset_dataset_to_max_full_years(): Found no full data years. Returning original DataArray')\n",
    "        return ds\n",
    "    else:   \n",
    "        return subset_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ba6ff-0399-44b5-bc39-6f0087556e1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c03abfe-2021-4a17-a995-62ea44df6aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Vegetation</th>\n",
       "      <th>forest_height_2020_mean</th>\n",
       "      <th>m</th>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forest_height_2020_stdev</th>\n",
       "      <th>m</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Topography</th>\n",
       "      <th>dem_mean</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <td>300</td>\n",
       "      <td>400</td>\n",
       "      <td>100</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dem_stdev</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               01   02   03   04\n",
       "Category   Attribute                Unit                        \n",
       "Vegetation forest_height_2020_mean  m          25   30   35   40\n",
       "           forest_height_2020_stdev m           5    3    3    4\n",
       "Topography dem_mean                 m.a.s.l.  300  400  100  600\n",
       "           dem_stdev                m.a.s.l.   30    1   10    7"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample data\n",
    "l_station_id = ['01', '02', '03', '04']\n",
    "l_values = []\n",
    "l_values.append([25, 5, 300, 30])\n",
    "l_values.append([30, 3, 400, 1])\n",
    "l_values.append([35, 3, 100, 10])\n",
    "l_values.append([40, 4, 600, 7])\n",
    "\n",
    "# Make the dataframe\n",
    "input_dict = dict(zip(l_station_id, l_values))\n",
    "df = pd.DataFrame(input_dict)\n",
    "\n",
    "# Set the index\n",
    "multi_index = pd.MultiIndex.from_tuples([\n",
    "    ('Vegetation', 'forest_height_2020_mean', 'm'),\n",
    "    ('Vegetation', 'forest_height_2020_stdev', 'm'),\n",
    "    ('Topography', 'dem_mean', 'm.a.s.l.'),\n",
    "    ('Topography', 'dem_stdev', 'm.a.s.l.')\n",
    "], names=['Category', 'Attribute', 'Unit'])\n",
    "df.index = multi_index\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f4a4ab7d-5c44-463c-8c31-859f3bdf5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test_attributes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f90a83-2162-4d9d-ac9e-79fc89641910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels-spat-env",
   "language": "python",
   "name": "camels-spat-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
