{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db3e732-426b-4b22-b055-b9395f29fa09",
   "metadata": {},
   "source": [
    "## Calculate attributes per subbasin\n",
    "Takes prepared geospatial data and computes various attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac7b17d-07d9-4985-bb9c-8508fb2c7554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "from python_cs_functions import config as cs, attributes as csa\n",
    "from python_cs_functions.delineate import prepare_delineation_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a14e3a-4a96-4308-9f4b-4bc1a2eceab8",
   "metadata": {},
   "source": [
    "#### DEV only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7931b34c-e133-4a6c-a137-8366898954dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import glob\n",
    "import numpy as np \n",
    "import os\n",
    "import xarray as xr\n",
    "from rasterstats import zonal_stats\n",
    "from python_cs_functions.attributes import read_scale_and_offset, zonal_out_none2nan, subset_dataset_to_max_full_years, process_monthly_means_to_lists\n",
    "from python_cs_functions.attributes import find_climate_seasonality_rdrs, circmean_group, circstd_group, calculate_temp_prcp_stats, find_durations, get_season\n",
    "from python_cs_functions.attributes import create_mean_daily_max_series, get_open_water_stats, get_categorical_dict, check_scale_and_offset\n",
    "from scipy.stats import circmean, circstd, skew, kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bcba8f-d99c-4058-9de9-23ae255c19e9",
   "metadata": {},
   "source": [
    "### Config handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dfdcc03-f734-4df6-8bee-25ae59f5560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where the config file can be found\n",
    "config_file = '../0_config/config.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa42aa7-cf4d-4c51-8555-05638e2ffc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the required info from the config file\n",
    "data_path            = cs.read_from_config(config_file,'data_path')\n",
    "\n",
    "# CAMELS-spat metadata\n",
    "cs_meta_path = cs.read_from_config(config_file,'cs_basin_path')\n",
    "cs_meta_name = cs.read_from_config(config_file,'cs_meta_name')\n",
    "cs_unusable_name = cs.read_from_config(config_file,'cs_unusable_name')\n",
    "\n",
    "# Basin folder\n",
    "cs_basin_folder = cs.read_from_config(config_file, 'cs_basin_path')\n",
    "basins_path = Path(data_path) / cs_basin_folder\n",
    "\n",
    "# Get the temporary data folder\n",
    "cs_temp_folder = cs.read_from_config(config_file, 'temp_path')\n",
    "temp_path = Path(cs_temp_folder)\n",
    "temp_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Get the attribute folder\n",
    "att_folder = cs.read_from_config(config_file, 'att_path')\n",
    "att_path = basins_path / att_folder\n",
    "att_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get the CRS we want to do area calculations in (possibly helpful for GLHYMPS and HydroLAKES)\n",
    "ea_crs = cs.read_from_config(config_file, 'equal_area_crs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933dae9c-b5fe-42d2-9cc9-2ee54661e2c7",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "333bd63a-b0a1-417d-b9c6-d28306a0120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAMELS-spat metadata file\n",
    "cs_meta_path = Path(data_path) / cs_meta_path\n",
    "cs_meta = pd.read_csv(cs_meta_path / cs_meta_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5a497a0-a9a2-404c-9b2a-8173cbb1436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open list of unusable stations; Enforce reading IDs as string to keep leading 0's\n",
    "cs_unusable = pd.read_csv(cs_meta_path / cs_unusable_name, dtype={'Station_id': object})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87113cc-bfb3-48dd-bae1-4c7f19fd39b3",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3185dc90-2a46-46b2-bdd1-c0032c2c452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_message = f'\\n!!! CHECK DEBUGGING STATUS: \\n- Testing 1 file \\n- Testing 1 basin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c395a8-7e5d-4140-b61d-65581fc05a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed 'hydrology', because we don't have gauges for every subbasin\n",
    "data_subfolders = ['rdrs', 'worldclim', 'lai', 'forest_height', 'glclu2019', 'modis_land', 'lgrip30', 'merit', 'hydrolakes', 'pelletier', 'soilgrids', 'glhymps']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b52ec8-3e62-458b-bdee-ea394b72e304",
   "metadata": {},
   "source": [
    "### DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03391c31-fc73-4824-bfe2-8e45edb48478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAN_01DJ005\n"
     ]
    }
   ],
   "source": [
    "# Set up a relatively simple test case (not too big)\n",
    "ix = 46\n",
    "row = cs_meta.iloc[ix]\n",
    "print(f\"{row['Country']}_{row['Station_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3e28b28-c07c-47df-9676-d38b0a607010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the paths\n",
    "basin_id, shp_lump_path, shp_dist_path, _, _ = prepare_delineation_outputs(cs_meta, ix, basins_path)\n",
    "geo_folder = basins_path / 'basin_data' / basin_id / 'geospatial'\n",
    "met_folder = basins_path / 'basin_data' / basin_id / 'forcing'\n",
    "hyd_folder = basins_path / 'basin_data' / basin_id / 'observations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea1ed379-dc7e-4057-b44c-775a77130aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the shapefiles\n",
    "shp = str(shp_dist_path).format('basin') # because zonalstats wants a file path, not a geodataframe\n",
    "riv = str(shp_dist_path).format('river') # For topographic attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331a258-7214-4574-b885-60cd7e68c089",
   "metadata": {},
   "source": [
    "Test the different data products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ca8f2-699b-4302-a081-2a0635243cf5",
   "metadata": {},
   "source": [
    "#### Fix the distributed case for geotiffs\n",
    "Trial with forest_height (general case), soilgrids (special averaging function), glclu (categorical outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41536796-3f72-4e29-a05f-6dcc3710c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile to get the sub-basin order for geotiffs and shapefiles\n",
    "gdf = gpd.read_file(shp)\n",
    "l_comids_geo = gdf['COMID'].to_list() # Store the polygon IDs into a list, we'll later use this as columns in the attribute dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "10167e23-29f0-474f-8b54-15ef02f8e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize storage lists for geotiff and shapefile attributes\n",
    "l_values_geo = [[] for _ in range(len(l_comids_geo))] # Initialize an empty list where we'll store this basin's attributes - nested lists for each subbasin\n",
    "l_index_geo = [] # Initialize an empty list where we'll store the attribute descriptions - this will be our dataframe index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f97ddb05-525d-48ef-b40d-76c6b6af5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geotiffs\n",
    "dataset = 'forest_height'\n",
    "if dataset == 'forest_height':\n",
    "    l_values_geo, l_index_geo = attributes_from_forest_height(geo_folder, dataset, shp, l_values_geo, l_index_geo)\n",
    "\n",
    "dataset = 'soilgrids'\n",
    "if dataset == 'soilgrids':\n",
    "    l_values_geo, l_index_geo = attributes_from_soilgrids(geo_folder, dataset, shp, l_values_geo, l_index_geo)\n",
    "\n",
    "dataset = 'glclu2019'\n",
    "if dataset == 'glclu2019':\n",
    "    l_values_geo, l_index_geo = attributes_from_glclu2019(geo_folder, dataset, shp, l_values_geo, l_index_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9c50f681-d2f9-4120-b98e-c33f4a1406b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the geospatial attribute dataframe\n",
    "att_df_geo = pd.DataFrame(data = l_values_geo, index = l_comids_geo).transpose()\n",
    "multi_index = pd.MultiIndex.from_tuples(l_index_geo, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "att_df_geo.index = multi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "61dd0f74-8436-47e9-a73e-5125873a6418",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>72048321</th>\n",
       "      <th>72049179</th>\n",
       "      <th>72049188</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">Land cover</th>\n",
       "      <th>forest_height_2000_min</th>\n",
       "      <th>m</th>\n",
       "      <th>GLCLUC 2000-2020</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forest_height_2000_mean</th>\n",
       "      <th>m</th>\n",
       "      <th>GLCLUC 2000-2020</th>\n",
       "      <td>5.996761</td>\n",
       "      <td>7.750167</td>\n",
       "      <td>8.707013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forest_height_2000_max</th>\n",
       "      <th>m</th>\n",
       "      <th>GLCLUC 2000-2020</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forest_height_2000_std</th>\n",
       "      <th>m</th>\n",
       "      <th>GLCLUC 2000-2020</th>\n",
       "      <td>5.784255</td>\n",
       "      <td>6.947842</td>\n",
       "      <td>6.964776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forest_height_2020_min</th>\n",
       "      <th>m</th>\n",
       "      <th>GLCLUC 2000-2020</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc1_water_fraction</th>\n",
       "      <th>-</th>\n",
       "      <th>GLCLU 2019</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.016539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc1_cropland_fraction</th>\n",
       "      <th>-</th>\n",
       "      <th>GLCLU 2019</th>\n",
       "      <td>0.007780</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc1_built_up_fraction</th>\n",
       "      <th>-</th>\n",
       "      <th>GLCLU 2019</th>\n",
       "      <td>0.037319</td>\n",
       "      <td>0.018674</td>\n",
       "      <td>0.035412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc1_ocean_fraction</th>\n",
       "      <th>-</th>\n",
       "      <th>GLCLU 2019</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lc1_no_data_fraction</th>\n",
       "      <th>-</th>\n",
       "      <th>GLCLU 2019</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>364 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           72048321  \\\n",
       "Category   Attribute               Unit Source                        \n",
       "Land cover forest_height_2000_min  m    GLCLUC 2000-2020   0.000000   \n",
       "           forest_height_2000_mean m    GLCLUC 2000-2020   5.996761   \n",
       "           forest_height_2000_max  m    GLCLUC 2000-2020  23.000000   \n",
       "           forest_height_2000_std  m    GLCLUC 2000-2020   5.784255   \n",
       "           forest_height_2020_min  m    GLCLUC 2000-2020   0.000000   \n",
       "...                                                             ...   \n",
       "           lc1_water_fraction      -    GLCLU 2019         0.000000   \n",
       "           lc1_cropland_fraction   -    GLCLU 2019         0.007780   \n",
       "           lc1_built_up_fraction   -    GLCLU 2019         0.037319   \n",
       "           lc1_ocean_fraction      -    GLCLU 2019         0.000000   \n",
       "           lc1_no_data_fraction    -    GLCLU 2019         0.000000   \n",
       "\n",
       "                                                           72049179   72049188  \n",
       "Category   Attribute               Unit Source                                  \n",
       "Land cover forest_height_2000_min  m    GLCLUC 2000-2020   0.000000   0.000000  \n",
       "           forest_height_2000_mean m    GLCLUC 2000-2020   7.750167   8.707013  \n",
       "           forest_height_2000_max  m    GLCLUC 2000-2020  25.000000  23.000000  \n",
       "           forest_height_2000_std  m    GLCLUC 2000-2020   6.947842   6.964776  \n",
       "           forest_height_2020_min  m    GLCLUC 2000-2020   0.000000   0.000000  \n",
       "...                                                             ...        ...  \n",
       "           lc1_water_fraction      -    GLCLU 2019         0.000315   0.016539  \n",
       "           lc1_cropland_fraction   -    GLCLU 2019         0.000333   0.000224  \n",
       "           lc1_built_up_fraction   -    GLCLU 2019         0.018674   0.035412  \n",
       "           lc1_ocean_fraction      -    GLCLU 2019         0.000000   0.000000  \n",
       "           lc1_no_data_fraction    -    GLCLU 2019         0.000000   0.000000  \n",
       "\n",
       "[364 rows x 3 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_df_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "100e788c-a330-466b-b57c-952cc70d0d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_values_list(l_values, stats, zonal_out, scale, offset):\n",
    "\n",
    "    # Update scale and offset to usable values\n",
    "    if scale is None: scale = 1 # If scale is undefined that means we simply multiply by 1\n",
    "    if offset is None: offset = 0 # Undefined offset > add 0\n",
    "\n",
    "    # Deal with the occassional None that we get when raster data input to zonal-stats is missing\n",
    "    # This fixes 9 occurrences of missing data in the soilgrids maps\n",
    "    zonal_out = zonal_out_none2nan(zonal_out)\n",
    "    \n",
    "    # We loop through the calculated stats in a pre-determined order:\n",
    "    # 1. min\n",
    "    # 2. mean\n",
    "    # 3. max\n",
    "    # 4. stdev\n",
    "    # 5. ..\n",
    "    if len(zonal_out) == 1: # lumped case\n",
    "        if 'min' in stats:  l_values.append(zonal_out[0]['min']  * scale + offset)\n",
    "        if 'mean' in stats: l_values.append(zonal_out[0]['mean'] * scale + offset)\n",
    "        if 'harmonic_mean'  in stats: l_values.append(zonal_out[0]['harmonic_mean'] * scale + offset) # only here for soilgrids conductivity, in which case we don't have 'mean'\n",
    "        if 'max' in stats:  l_values.append(zonal_out[0]['max']  * scale + offset)\n",
    "        if 'std' in stats:  l_values.append(zonal_out[0]['std']  * scale + offset)\n",
    "    else: # distributed case, multiple polygons\n",
    "        \n",
    "        # confirm that l_values has as many nested lists as we have zonal stats outputs\n",
    "        num_nested_lists = sum(1 for item in l_values if isinstance(item, list))\n",
    "        assert num_nested_lists == len(zonal_out), f\"zonal_out length does not match expected list length {num_nested_lists}. zonal_out: {zonal_out}\"\n",
    "        \n",
    "        # now loop over the zonal outputs and append to relevant lists\n",
    "        for i in range(0,num_nested_lists):\n",
    "            if 'min' in stats:  l_values[i].append(zonal_out[i]['min']  * scale + offset)\n",
    "            if 'mean' in stats: l_values[i].append(zonal_out[i]['mean'] * scale + offset)\n",
    "            if 'harmonic_mean'  in stats: l_values[i].append(zonal_out[i]['harmonic_mean'] * scale + offset) # only here for soilgrids conductivity, in which case we don't have 'mean'\n",
    "            if 'max' in stats:  l_values[i].append(zonal_out[i]['max']  * scale + offset)\n",
    "            if 'std' in stats:  l_values[i].append(zonal_out[i]['std']  * scale + offset)\n",
    "\n",
    "    return l_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53f105a2-3ca8-42fc-a91e-42d2892b61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_mean(x):\n",
    "    return np.ma.count(x) / (1/x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a111a2a-79c6-4178-b239-b2291f5f30c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def attributes_from_soilgrids(geo_folder, dataset, shp_str, l_values, l_index):\n",
    "\n",
    "    '''Calculates attributes from SOILGRIDS maps'''\n",
    "\n",
    "    # File specifiction\n",
    "    sub_folders = ['bdod',     'cfvo',       'clay',    'sand',    'silt',    'soc',      'porosity']\n",
    "    units       = ['cg cm^-3', 'cm^3 dm^-3', 'g kg^-1', 'g kg^-1', 'g kg^-1', 'dg kg^-1', '-']\n",
    "    depths = ['0-5cm', '5-15cm', '15-30cm', '30-60cm', '60-100cm', '100-200cm']\n",
    "    fields = ['mean', 'uncertainty']\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    \n",
    "    # Loop over the files and calculate stats\n",
    "    for sub_folder, unit in zip(sub_folders, units):\n",
    "        for depth in depths:\n",
    "            for field in fields:\n",
    "                tif = str(geo_folder / dataset / 'raw' / f'{sub_folder}' / f'{sub_folder}_{depth}_{field}.tif')\n",
    "                if not os.path.exists(tif): continue # porosity has no uncertainty field\n",
    "                zonal_out = zonal_stats(shp_str, tif, stats=stats)\n",
    "                scale,offset = read_scale_and_offset(tif)\n",
    "                l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "                l_index += [('Soil', f'{sub_folder}_{depth}_{field}_min',  f'{unit}', 'SOILGRIDS'),\n",
    "                            ('Soil', f'{sub_folder}_{depth}_{field}_mean', f'{unit}', 'SOILGRIDS'),\n",
    "                            ('Soil', f'{sub_folder}_{depth}_{field}_max',  f'{unit}', 'SOILGRIDS'),\n",
    "                            ('Soil', f'{sub_folder}_{depth}_{field}_std',  f'{unit}', 'SOILGRIDS')]\n",
    "           \n",
    "    # --- Specifc processing for the conductivity fields\n",
    "    # For conductivity we want to have a harmonic mean because that should be more\n",
    "    # representative as a spatial average. rasterstats doesn't have that, so we \n",
    "    # need a custom function. We're splitting out the processing from the rest for\n",
    "    # clarity. Could also have done this with a bunch of if-statements, but this \n",
    "    # seems cleaner, also because conductivity doesn't have uncertainty maps.\n",
    "    \n",
    "    # Process the data fields\n",
    "    sub_folder = 'conductivity'\n",
    "    unit       = 'cm hr^-1'\n",
    "    depths = ['0-5cm', '5-15cm', '15-30cm', '30-60cm', '60-100cm', '100-200cm']\n",
    "    field  = 'mean' # no uncertainty maps for these\n",
    "    stats = ['min', 'max', 'std']\n",
    "\n",
    "    for depth in depths:\n",
    "        tif = str(geo_folder / dataset / 'raw' / f'{sub_folder}' / f'{sub_folder}_{depth}_{field}.tif')\n",
    "        if not os.path.exists(tif): continue # porosity has no uncertainty field\n",
    "        zonal_out = zonal_stats(shp_str, tif, stats=stats, add_stats={'harmonic_mean': harmonic_mean})\n",
    "        scale,offset = read_scale_and_offset(tif)\n",
    "        l_values = update_values_list(l_values, ['min', 'max', 'std', 'harmonic_mean'], zonal_out, scale, offset)\n",
    "        l_index += [('Soil', f'{sub_folder}_{depth}_{field}_min',  f'{unit}', 'SOILGRIDS'),\n",
    "                    ('Soil', f'{sub_folder}_{depth}_{field}_harmonic_mean', f'{unit}', 'SOILGRIDS'),\n",
    "                    ('Soil', f'{sub_folder}_{depth}_{field}_max',  f'{unit}', 'SOILGRIDS'),\n",
    "                    ('Soil', f'{sub_folder}_{depth}_{field}_std',  f'{unit}', 'SOILGRIDS')]\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2a28dcf-8f8f-4c27-90f7-5b501dbba25b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def attributes_from_forest_height(geo_folder, dataset, shp_str, l_values, index):\n",
    "\n",
    "    '''Calculates mean, min, max and stdv for forest height 2000 and 2020 tifs'''\n",
    "\n",
    "    # Year 2000 min, mean, max, stdev\n",
    "    tif = str( geo_folder / dataset / 'raw' / 'forest_height_2000.tif' )\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    zonal_out = zonal_stats(shp_str, tif, stats=stats)\n",
    "    scale,offset = read_scale_and_offset(tif)\n",
    "    l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "    index += [('Land cover', 'forest_height_2000_min',   'm', 'GLCLUC 2000-2020'),\n",
    "              ('Land cover', 'forest_height_2000_mean',  'm', 'GLCLUC 2000-2020'),\n",
    "              ('Land cover', 'forest_height_2000_max',   'm', 'GLCLUC 2000-2020'),\n",
    "              ('Land cover', 'forest_height_2000_std',   'm', 'GLCLUC 2000-2020')]\n",
    "\n",
    "    # Year 2020 mean, stdev\n",
    "    tif = geo_folder / dataset / 'raw' / 'forest_height_2020.tif'\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    zonal_out = zonal_stats(shp_str, tif, stats=stats)\n",
    "    scale,offset = read_scale_and_offset(tif)\n",
    "    l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "    index += [('Land cover', 'forest_height_2020_min',   'm', 'GLCLUC 2000-2020'),\n",
    "              ('Land cover', 'forest_height_2020_mean',  'm', 'GLCLUC 2000-2020'),\n",
    "              ('Land cover', 'forest_height_2020_max',   'm', 'GLCLUC 2000-2020'),\n",
    "              ('Land cover', 'forest_height_2020_std',   'm', 'GLCLUC 2000-2020')]\n",
    "\n",
    "    return l_values, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23a27556-a14b-4134-9fc3-5c0bced5f692",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def attributes_from_glclu2019(geo_folder, dataset, shp_str, l_values, l_index):\n",
    "\n",
    "    '''Calculates percentage occurrence of all classes in GLCLU2019 map'''\n",
    "\n",
    "    tif = geo_folder / dataset / 'raw' / 'glclu2019_map.tif'\n",
    "    zonal_out = zonal_stats(shp_str, tif, categorical=True)\n",
    "    check_scale_and_offset(tif)\n",
    "    l_values,l_index = update_values_list_with_categorical(l_values, l_index, zonal_out, 'GLCLU 2019', prefix='lc1_')\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fdf92013-5644-413a-986b-c2001d0e6a67",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_values_list_with_categorical(l_values, l_index, zonal_out, source, prefix=''):\n",
    "    '''Maps a zonal histogram of categorical classes onto descriptions and adds to lists'''\n",
    "\n",
    "    # Get the category definitions\n",
    "    cat_dict = get_categorical_dict(source)    \n",
    "\n",
    "    # Separately handle lumped and distributed cases\n",
    "    if len(zonal_out) == 1: # lumped case\n",
    "    \n",
    "        # Find the total number of classified pixels\n",
    "        total_pixels = 0\n",
    "        for land_id,count in zonal_out[0].items():\n",
    "            total_pixels += count\n",
    "        \n",
    "        # Loop over all categories and see what we have in this catchment\n",
    "        for land_id,text in cat_dict.items():\n",
    "            land_prct = 0\n",
    "            if land_id in zonal_out[0].keys():\n",
    "                land_prct = zonal_out[0][land_id] / total_pixels\n",
    "            l_values.append(land_prct)\n",
    "            l_index.append(('Land cover', f'{prefix}{text}_fraction', '-', f'{source}'))\n",
    "\n",
    "    else:  # distributed case, multiple polygons\n",
    "\n",
    "        # confirm that l_values has as many nested lists as we have zonal stats outputs\n",
    "        num_nested_lists = sum(1 for item in l_values if isinstance(item, list))\n",
    "        assert num_nested_lists == len(zonal_out), f\"zonal_out length does not match expected list length {num_nested_lists}. zonal_out: {zonal_out}\"\n",
    "\n",
    "        # now loop over the zonal outputs and append to relevant lists\n",
    "        for i in range(0,num_nested_lists):\n",
    "\n",
    "            # Find the total number of classified pixels\n",
    "            total_pixels = 0\n",
    "            for land_id,count in zonal_out[i].items():\n",
    "                total_pixels += count\n",
    "            \n",
    "            # Loop over all categories and see what we have in this catchment\n",
    "            tmp_index = [] # we need this so the index resets on each subbasin iteration, and we need that because we only need the index once\n",
    "            for land_id,text in cat_dict.items():\n",
    "                land_prct = 0\n",
    "                if land_id in zonal_out[i].keys():\n",
    "                    land_prct = zonal_out[i][land_id] / total_pixels\n",
    "                l_values[i].append(land_prct)\n",
    "                tmp_index.append(('Land cover', f'{prefix}{text}_fraction', '-', f'{source}'))\n",
    "\n",
    "        # Add the index values only once\n",
    "        for item in tmp_index:\n",
    "            l_index.append(item)\n",
    "\n",
    "    return l_values,l_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf96c0-f22a-4798-b701-de500fc0525f",
   "metadata": {},
   "source": [
    "#### Fix HydroLAKES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08011357-0dae-489c-8e81-694cb3662d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_values_lakes = [[] for _ in range(len(l_comids_geo))]\n",
    "l_index_lakes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21cf892c-250e-4151-ba88-ece94cd67952",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'hydrolakes'\n",
    "l_values_lakes, l_index_lakes, l_comids_lakes = attributes_from_hydrolakes(geo_folder, dataset, shp, ea_crs, l_values_lakes, l_index_lakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd81c024-06b3-49f6-8476-71c05246eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the hydrolakes attribute dataframe\n",
    "att_df_lakes = pd.DataFrame(data = l_values_lakes, index = l_comids_lakes).transpose()\n",
    "multi_index = pd.MultiIndex.from_tuples(l_index_lakes, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "att_df_lakes.index = multi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "900fabdf-ed6a-43d9-aa73-72317001a2e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def subset_hydrolakes_to_subbasin(lakes, subbasin, ea_crs):\n",
    "\n",
    "    '''lakes: GeoDataframe | subbasin: geometry | ea_crs: string'''\n",
    "\n",
    "    # Clip the lakes polygon to the subbasin\n",
    "    poly_lakes = gpd.clip(lakes,subbasin)\n",
    "\n",
    "    # If we are left with any lake polygons:\n",
    "    if len(poly_lakes) > 0 :\n",
    "    # Update the Lake_area [km2] and Vol_total [million~m^3] values\n",
    "    # This is needed in cases where due to clipping the lake polygon we end up with a partial lake in this subbasin\n",
    "    \n",
    "        # Old values\n",
    "        old_areas = poly_lakes['Lake_area'] # km2\n",
    "        old_volumes = poly_lakes['Vol_total'] # million m3\n",
    "        \n",
    "        # Get new area\n",
    "        new_areas = poly_lakes.to_crs(ea_crs).area / 10**6 # [m2] / 10^6 = [km2]\n",
    "        new_areas_rounded = round(new_areas,2) # this matches the number of significant digits in the test case (CAN_01DJ005)\n",
    "        \n",
    "        # Scale volume by new area - this is a bit simplistic but we have no better way to estimate the volume\n",
    "        new_volumes = old_volumes * (new_areas_rounded / old_areas)\n",
    "        \n",
    "        # Replace values\n",
    "        poly_lakes['Lake_area'] = new_areas_rounded\n",
    "        poly_lakes['Vol_total'] = new_volumes\n",
    "\n",
    "    return poly_lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6e424b7-0f6b-4241-98ca-b9fa3cc1f07e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def attributes_from_hydrolakes(geo_folder, dataset, basin_shp_path, ea_crs, l_values, l_index):\n",
    "    \n",
    "    '''Calculates open water attributes from HydroLAKES'''\n",
    "\n",
    "    # We need some special actions if we're dealing with the distributed case, so check that first\n",
    "    case = 'lumped'\n",
    "    if 'distributed' in basin_shp_path:\n",
    "        case = 'distributed'\n",
    "    \n",
    "    # Define the standard file name\n",
    "    lake_str = str(geo_folder / dataset / 'raw' / 'HydroLAKES_polys_v10_NorthAmerica.shp')\n",
    "\n",
    "    # Now handle the different cases\n",
    "    if case == 'lumped':\n",
    "\n",
    "        # Check if the file exists (some basins won't have lakes)\n",
    "        if os.path.exists(lake_str):\n",
    "            lakes = gpd.read_file(lake_str)\n",
    "            res_mask  = lakes['Lake_type'] == 2 # Lake Type 2 == reservoir; see docs (https://data.hydrosheds.org/file/technical-documentation/HydroLAKES_TechDoc_v10.pdf)\n",
    "            num_lakes = len(lakes)\n",
    "            num_resvr = res_mask.sum()\n",
    "\n",
    "        else: # no lakes shapefile\n",
    "            lakes = None # this tells get_open_water_stats() that we had no lake shapefile\n",
    "            num_lakes = 0\n",
    "            num_resvr = 0\n",
    "\n",
    "        l_values.append(num_lakes)\n",
    "        l_index.append(('Open water', 'open_water_number',  '-', 'HydroLAKES'))\n",
    "        l_values.append(num_resvr)\n",
    "        l_index.append(('Open water', 'known_reservoirs',  '-', 'HydroLAKES'))\n",
    "\n",
    "        # Summary stats\n",
    "        l_values, l_index = get_open_water_stats(lakes, 'Lake_area', 'all', l_values, l_index) # All open water\n",
    "        l_values, l_index = get_open_water_stats(lakes, 'Vol_total', 'all', l_values, l_index)\n",
    "        l_values, l_index = get_open_water_stats(lakes, 'Lake_area', 'reservoir', l_values, l_index) # Reservoirs only\n",
    "        l_values, l_index = get_open_water_stats(lakes, 'Vol_total', 'reservoir', l_values, l_index)\n",
    "\n",
    "        # set the remaing output\n",
    "        l_comids_lakes = None\n",
    "\n",
    "    elif case == 'distributed':\n",
    "\n",
    "        # Load the basin shape \n",
    "        basin = gpd.read_file(basin_shp_path)\n",
    "        \n",
    "        # Load the lake shape if we have one\n",
    "        if os.path.exists(lake_str):\n",
    "            lakes = gpd.read_file(lake_str)\n",
    "        else:\n",
    "            lakes = None # we use this to indicate we had no lakes at all\n",
    "            \n",
    "        # Loop over the individual polygons and create a new mini-HydroLAKES geodataframe for each polygon\n",
    "        num_poly = len(basin)\n",
    "        l_comids_lakes = basin['COMID'].values\n",
    "        for i_poly in range(num_poly):\n",
    "\n",
    "            # Rest the storage lists\n",
    "            tmp_values = []\n",
    "            tmp_index = []\n",
    "\n",
    "            # subset the 'lakes' gdf to each individual polygon if we have 'lakes'           \n",
    "            if lakes is not None:\n",
    "                poly_lakes = subset_hydrolakes_to_subbasin(lakes, basin.iloc[i_poly]['geometry'], ea_crs)\n",
    "\n",
    "                # Now see if we have a lake at all, and act accordingly\n",
    "                if len(poly_lakes) > 0:\n",
    "                    res_mask  = lakes['Lake_type'] == 2 # Lake Type 2 == reservoir; see docs (https://data.hydrosheds.org/file/technical-documentation/HydroLAKES_TechDoc_v10.pdf)\n",
    "                    num_lakes = len(lakes)\n",
    "                    num_resvr = res_mask.sum()\n",
    "        \n",
    "                else: # no lakes in this subbasins\n",
    "                    poly_lakes = None # this tells get_open_water_stats() that we had no lake shapefile\n",
    "                    num_lakes = 0\n",
    "                    num_resvr = 0\n",
    "            elif lakes is None: # this means we didn't have a lakes shapefile for this basin at all\n",
    "                poly_lakes = None # this tells get_open_water_stats() we had nothing\n",
    "                num_lakes = 0\n",
    "                num_resvr = 0\n",
    "\n",
    "            # Stats\n",
    "            tmp_values.append(num_lakes)\n",
    "            tmp_index.append(('Open water', 'open_water_number',  '-', 'HydroLAKES'))\n",
    "            tmp_values.append(num_resvr)\n",
    "            tmp_index.append(('Open water', 'known_reservoirs',  '-', 'HydroLAKES'))\n",
    "            tmp_values, tmp_index = get_open_water_stats(poly_lakes, 'Lake_area', 'all', tmp_values, tmp_index) # All open water\n",
    "            tmp_values, tmp_index = get_open_water_stats(poly_lakes, 'Vol_total', 'all', tmp_values, tmp_index)\n",
    "            tmp_values, tmp_index = get_open_water_stats(poly_lakes, 'Lake_area', 'reservoir', tmp_values, tmp_index) # Reservoirs only\n",
    "            tmp_values, tmp_index = get_open_water_stats(poly_lakes, 'Vol_total', 'reservoir', tmp_values, tmp_index)\n",
    "\n",
    "            # Add the new values for this subbasin into the main l_values list\n",
    "            l_values[i_poly] = tmp_values # This works because we create a unique dataframe just for HydroLAKES results\n",
    "\n",
    "        # End of subbasin loop, add the new index entries only once\n",
    "        l_index = tmp_index # we don't use append() in the distributed case because we'll be storing this in a dedicated HydroLAKES attribute df\n",
    "\n",
    "    return l_values, l_index, l_comids_lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb6dfa0-a8e6-44ed-a92a-2171307a208f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Fix the GLHYMPS case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61af2f04-1a39-4cb7-b2b8-e9f9e37b3f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_values_glhymps = [[] for _ in range(len(l_comids_geo))]\n",
    "l_index_glhymps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d3b0f14-14ea-45b5-a17e-b88e325edf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'glhymps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55d7c507-b18e-4dea-b12f-0dff36acbe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/custom/software/2020/avx2/MPI/gcc9/openmpi4/geo-stack/2022a/lib/python3.10/site-packages/geopandas/io/file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "l_values_glhymps, l_index_glhymps, l_comids_glhymps = attributes_from_glhymps(geo_folder, dataset, shp, l_values_glhymps, l_index_glhymps, equal_area_crs=ea_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8d50f3c-49ed-4ff0-a565-8387c85a30c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the GLHYMPS attribute dataframe\n",
    "att_df_glhymps = pd.DataFrame(data = l_values_glhymps, index = l_comids_glhymps).transpose()\n",
    "multi_index = pd.MultiIndex.from_tuples(l_index_glhymps, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "att_df_glhymps.index = multi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31b3c9f4-21f8-431a-ab27-13d0b0adac28",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def subset_glhymps_to_subbasin(glhymps, subbasin, ea_crs):\n",
    "\n",
    "    poly_glhymps = gpd.clip(glhymps,subbasin)\n",
    "    poly_glhymps['New_area_m2'] = poly_glhymps.to_crs(ea_crs).area\n",
    "\n",
    "    return poly_glhymps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8775aa1-a097-40ab-a450-268a072d05a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def attributes_from_glhymps(geo_folder, dataset, basin_shp_path, l_values, l_index, equal_area_crs='ESRI:102008'):\n",
    "\n",
    "    '''Calculates attributes from GLHYMPS'''\n",
    "\n",
    "    # We need some special actions if we're dealing with the distributed case, so check that first\n",
    "    case = 'lumped'\n",
    "    if 'distributed' in basin_shp_path:\n",
    "        case = 'distributed'\n",
    "    \n",
    "    # Load the geology file\n",
    "    geol_str = str(geo_folder / dataset / 'raw' / 'glhymps.shp')\n",
    "    geol = gpd.read_file(geol_str)\n",
    "\n",
    "    # -- File updates\n",
    "    # Rename the columns, because the shortened ones are not very helpful\n",
    "    if ('Porosity' in geol.columns) and ('Permeabili' in geol.columns) \\\n",
    "    and ('Permeabi_1' in geol.columns) and ('Permeabi_2' in geol.columns):\n",
    "        geol.rename(columns={'Porosity': 'porosity',   'Permeabili': 'logK_Ferr',\n",
    "                             'Permeabi_1': 'logK_Ice', 'Permeabi_2': 'logK_std'}, inplace=True)\n",
    "        geol.to_file(geol_str)\n",
    "\n",
    "    # Ensure we have the correct areas to work with\n",
    "    if 'New_area_m2' not in geol.columns:\n",
    "        geol['New_area_m2'] = geol.to_crs(equal_area_crs).area\n",
    "        #geol.to_file(geol_str)\n",
    "\n",
    "    # Clean up a few unsightly processing errors\n",
    "    geol['Shape_Area'] = geol.to_crs(equal_area_crs).area\n",
    "    geol = geol[['IDENTITY_','Shape_Area','porosity','logK_Ferr','logK_Ice','logK_std','geometry']]\n",
    "    geol.to_file(geol_str)\n",
    "    # -- End file updates\n",
    "\n",
    "    # Now handle the different cases\n",
    "    if case == 'lumped':\n",
    "    \n",
    "        # Create areal averages and standard deviations\n",
    "        # Stdev source: https://stats.stackexchange.com/a/6536\n",
    "        porosity_mean = (geol['porosity']*geol['New_area_m2']).sum()/geol['New_area_m2'].sum()\n",
    "        num_obs_coef = ((geol['New_area_m2'] > 0).sum()-1)/(geol['New_area_m2'] > 0).sum()\n",
    "        porosity_std = np.sqrt((geol['New_area_m2'] * (geol['porosity']-porosity_mean)**2).sum() / geol['New_area_m2'].sum())\n",
    "        \n",
    "        # Porosity\n",
    "        l_values.append( geol['porosity'].min() )\n",
    "        l_index.append(('Geology', 'porosity_min',  '-', 'GLHYMPS'))\n",
    "        l_values.append( porosity_mean ) # areal average\n",
    "        l_index.append(('Geology', 'porosity_mean',  '-', 'GLHYMPS'))\n",
    "        l_values.append( geol['porosity'].max() )\n",
    "        l_index.append(('Geology', 'porosity_max',  '-', 'GLHYMPS'))\n",
    "        l_values.append( porosity_std )\n",
    "        l_index.append(('Geology', 'porosity_std',  '-', 'GLHYMPS'))\n",
    "    \n",
    "        # Create areal averages and standard deviations\n",
    "        # Stdev source: https://stats.stackexchange.com/a/6536\n",
    "        permeability_mean = (geol['logK_Ice']*geol['New_area_m2']).sum()/geol['New_area_m2'].sum()\n",
    "        num_obs_coef = ((geol['New_area_m2'] > 0).sum()-1)/(geol['New_area_m2'] > 0).sum()\n",
    "        permeability_std = np.sqrt((geol['New_area_m2'] * (geol['logK_Ice']-permeability_mean)**2).sum() / geol['New_area_m2'].sum())\n",
    "        \n",
    "        # Permeability\n",
    "        l_values.append( geol['logK_Ice'].min() )\n",
    "        l_index.append(('Geology', 'log_permeability_min',  'm^2', 'GLHYMPS'))\n",
    "        l_values.append( permeability_mean )\n",
    "        l_index.append(('Geology', 'log_permeability_mean',  'm^2', 'GLHYMPS'))\n",
    "        l_values.append( geol['logK_Ice'].max() )\n",
    "        l_index.append(('Geology', 'log_permeability_max',  'm^2', 'GLHYMPS'))\n",
    "        l_values.append( permeability_std )\n",
    "        l_index.append(('Geology', 'log_permeability_std',  'm^2', 'GLHYMPS'))\n",
    "\n",
    "        # Set the remaining output\n",
    "        l_comids_glhymps = None\n",
    "\n",
    "    # Case 2\n",
    "    elif case == 'distributed':\n",
    "    \n",
    "        # Load the basin shape \n",
    "        basin = gpd.read_file(basin_shp_path)\n",
    "\n",
    "        # Loop over the individual polygons and create a new mini-HydroLAKES geodataframe for each polygon\n",
    "        num_poly = len(basin)\n",
    "        l_comids_glhymps = basin['COMID'].values\n",
    "        for i_poly in range(num_poly):\n",
    "\n",
    "            # Rest the storage lists\n",
    "            tmp_values = []\n",
    "            tmp_index = []\n",
    "\n",
    "            # Subset the shape to the subbasin\n",
    "            poly_glhymps = subset_glhymps_to_subbasin(geol, basin.iloc[i_poly]['geometry'], equal_area_crs)\n",
    "\n",
    "            # Create areal averages and standard deviations\n",
    "            # Stdev source: https://stats.stackexchange.com/a/6536\n",
    "            porosity_mean = (poly_glhymps['porosity']*poly_glhymps['New_area_m2']).sum()/poly_glhymps['New_area_m2'].sum()\n",
    "            num_obs_coef = ((poly_glhymps['New_area_m2'] > 0).sum()-1)/(poly_glhymps['New_area_m2'] > 0).sum()\n",
    "            porosity_std = np.sqrt((poly_glhymps['New_area_m2'] * (poly_glhymps['porosity']-porosity_mean)**2).sum() / poly_glhymps['New_area_m2'].sum())\n",
    "            \n",
    "            # Porosity\n",
    "            tmp_values.append( poly_glhymps['porosity'].min() )\n",
    "            tmp_index.append(('Geology', 'porosity_min',  '-', 'GLHYMPS'))\n",
    "            tmp_values.append( porosity_mean ) # areal average\n",
    "            tmp_index.append(('Geology', 'porosity_mean',  '-', 'GLHYMPS'))\n",
    "            tmp_values.append( poly_glhymps['porosity'].max() )\n",
    "            tmp_index.append(('Geology', 'porosity_max',  '-', 'GLHYMPS'))\n",
    "            tmp_values.append( porosity_std )\n",
    "            tmp_index.append(('Geology', 'porosity_std',  '-', 'GLHYMPS'))\n",
    "        \n",
    "            # Create areal averages and standard deviations\n",
    "            # Stdev source: https://stats.stackexchange.com/a/6536\n",
    "            permeability_mean = (poly_glhymps['logK_Ice']*poly_glhymps['New_area_m2']).sum()/poly_glhymps['New_area_m2'].sum()\n",
    "            num_obs_coef = ((poly_glhymps['New_area_m2'] > 0).sum()-1)/(poly_glhymps['New_area_m2'] > 0).sum()\n",
    "            permeability_std = np.sqrt((poly_glhymps['New_area_m2'] * (poly_glhymps['logK_Ice']-permeability_mean)**2).sum() / poly_glhymps['New_area_m2'].sum())\n",
    "            \n",
    "            # Permeability\n",
    "            tmp_values.append( poly_glhymps['logK_Ice'].min() )\n",
    "            tmp_index.append(('Geology', 'log_permeability_min',  'm^2', 'GLHYMPS'))\n",
    "            tmp_values.append( permeability_mean )\n",
    "            tmp_index.append(('Geology', 'log_permeability_mean',  'm^2', 'GLHYMPS'))\n",
    "            tmp_values.append( poly_glhymps['logK_Ice'].max() )\n",
    "            tmp_index.append(('Geology', 'log_permeability_max',  'm^2', 'GLHYMPS'))\n",
    "            tmp_values.append( permeability_std )\n",
    "            tmp_index.append(('Geology', 'log_permeability_std',  'm^2', 'GLHYMPS'))\n",
    "            \n",
    "\n",
    "            # Add the new values for this subbasin into the main l_values list\n",
    "            l_values[i_poly] = tmp_values # This works because we create a unique dataframe just for HydroLAKES results\n",
    "\n",
    "        # End of subbasin loop, add the new index entries only once\n",
    "        l_index = tmp_index # we don't use append() in the distributed case because we'll be storing this in a dedicated HydroLAKES attribute df\n",
    "    \n",
    "    return l_values, l_index, l_comids_glhymps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ea62f4-3caa-42ed-a4af-0edb5528581e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Fix the RDRS distributed case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb29b4c7-b702-4906-bb36-836edd446bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_path = shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4011030-b36b-409d-ad3b-0463414b9ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running distributed case for RDRS data.\n",
      "Running subbasin 0\n",
      "Running subbasin 1\n",
      "Running subbasin 2\n"
     ]
    }
   ],
   "source": [
    "# trial the function\n",
    "l_values_met = []\n",
    "l_index_met = []\n",
    "l_values_met, l_index_met, l_comids_met, _ = attributes_from_rdrs(met_folder, shp_path, 'RDRS', l_values_met, l_index_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05e30407-8081-4abd-a401-de9dd6d8a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the forcing attributes dataframe\n",
    "att_df_met = pd.DataFrame(data = l_values_met, index = l_comids_met).transpose()\n",
    "multi_index = pd.MultiIndex.from_tuples(l_index_met, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "att_df_met.index = multi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27bc64d6-4c2e-42b7-aeb9-3de37bca15dc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_temp_prcp_stats(var, condition, hilo, l_values,l_index,\n",
    "                              dataset='ERA5', units='hours'):\n",
    "    \n",
    "    '''Calculates frequency (mean) and duration (mean, median, skew, kurtosis) \n",
    "        of temperature/precipitation periods'''\n",
    "\n",
    "    # Constants. We want everything in [days] for consistency with original CAMELS\n",
    "    hours_per_day = 24 # [hours day-1]\n",
    "    days_per_year = 365.25 # [days year-1]\n",
    "\n",
    "    # Calculate frequencies\n",
    "    freq = condition.mean(dim='time') * days_per_year # [-] * [days year-1]\n",
    "    l_values.append(freq.values)\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_freq', 'days year^-1', dataset) )\n",
    "    \n",
    "    # Calculate duration statistics\n",
    "    durations = find_durations(condition) # [time steps]\n",
    "    if units == 'hours':\n",
    "        durations = durations / hours_per_day # [days] = [hours] / [hours day-1]\n",
    "    l_values.append(np.mean(durations)) # [days]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_mean', 'days', dataset) ) # Consistency with\n",
    "    l_values.append(np.median(durations)) # [days]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_median', 'days', dataset) )\n",
    "    l_values.append(skew(durations)) # [-]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_skew', '-', dataset) )\n",
    "    l_values.append(kurtosis(durations)) # [-]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_kurtosis', '-', dataset) )\n",
    "\n",
    "    # Calculate timing statistic\n",
    "    condition['season'] = ('time', [get_season(month) for month in condition['time.month'].values]) # add seasons\n",
    "    season_groups = condition.groupby('season')\n",
    "    season_list   = list(season_groups.groups.keys())\n",
    "    max_season_id = int(season_groups.sum().argmax(dim='season').values) # find season with most True values\n",
    "    l_values.append(season_list[max_season_id]) # add season abbrev\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_timing', 'season', dataset) )\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8761ab02-a63a-471e-8ebb-d9ca4e840082",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def attributes_from_rdrs(met_folder, shp_path, dataset, l_values, l_index, use_mfdataset=False):\n",
    "\n",
    "    '''Calculates a variety of metrics from RDRS data'''\n",
    "\n",
    "    # Define file locations, depending on if we are dealing with lumped or distributed cases\n",
    "    if 'lumped' in shp_path:\n",
    "        rdrs_folder = met_folder / 'lumped'\n",
    "        case = 'lumped'\n",
    "    elif 'distributed' in shp_path:\n",
    "        rdrs_folder = met_folder / 'distributed'\n",
    "        case = 'distributed'\n",
    "    rdrs_files = sorted( glob.glob( str(rdrs_folder / 'RDRS_*.nc') ) )\n",
    "    print(f'Running {case} case for RDRS data.')\n",
    "\n",
    "    # Open the data\n",
    "    if use_mfdataset or (case == 'distributed'):\n",
    "        ds = xr.open_mfdataset(rdrs_files, combine=\"by_coords\") # Don't use 'decode_cf=False' > this somehow loses most of the timesteps\n",
    "    else:\n",
    "        ds = xr.merge([xr.open_dataset(f) for f in rdrs_files])\n",
    "        ds = ds.load()\n",
    "        ds = ds.isel(hru=0) # We need this so the lumped and distributed cases have the same dimensions inside the calculation function: (time,nbnds)\n",
    "                            # Without this, the lumped case has an extra 'hru' dimension (length 1) and this complicates value extraction\n",
    "\n",
    "    # --- Act according to case\n",
    "    if case == 'lumped':\n",
    "        # -- Get the precipitation for hydrologic signature calculations later; this avoids having to reload the entire dataset another time\n",
    "        ds_precip = ds['RDRS_v2.1_A_PR0_SFC'].copy()\n",
    "\n",
    "        # --- Calculate the statistics\n",
    "        l_values, l_index = calculate_rdrs_stats_from_ds(ds,l_values,l_index)\n",
    "        return l_values, l_index, ds_precip, ds\n",
    "    \n",
    "    elif case == 'distributed':\n",
    "\n",
    "        comid_order = [] # we need these later, to ensure we line the forcing attributes up with the geospatial attributes correctly\n",
    "\n",
    "        # Loop over the HRUs\n",
    "        num_hru = len(ds['hru'])\n",
    "        for i in range(0,num_hru):\n",
    "\n",
    "            print(f'Running subbasin {i}')\n",
    "\n",
    "            # Load the data for this sub-basin\n",
    "            ds_hru = ds.isel(hru=i) # example\n",
    "            ds_hru.load() # Load data into memory; done in place\n",
    "\n",
    "            # Specifically track the hruID (COMID) so we can ensure correct matches with the rest of the attributes later\n",
    "            assert (ds_hru['hruId'].values == ds_hru['hruId'][0].values).all(), f\"COMIDs not all identical {ds_hru['hruId'].values}\"\n",
    "            comid_order.append(ds_hru['hruId'][0].values)\n",
    "\n",
    "            # calculate the stats\n",
    "            tmp_values = [] # statistics for this subbasin - we need a single empty list so we can properly store in the nested list later\n",
    "            tmp_index  = [] # we also store the index in a tmp variable, so we only get this once instead of appending num_hru times to the main index list\n",
    "            tmp_values, tmp_index = calculate_rdrs_stats_from_ds(ds_hru,tmp_values,tmp_index) \n",
    "            l_values.append(tmp_values)\n",
    "\n",
    "        # prep outputs\n",
    "        comid_order = [arr.tolist() for arr in comid_order] # convert list of arrays to simple list\n",
    "        l_index.append(tmp_index) # need this only once; tmp_index is already a list so this creates a nested list: [['Climate', 'num_years_rdrs', 'years', 'RDRS')]]\n",
    "        return l_values, l_index[0], comid_order, ds # we need l_index[0] to return a non-nested list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "048908bc-cdeb-4b82-ab65-4e59b874c773",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_rdrs_stats_from_ds(ds,l_values,l_index):\n",
    "\n",
    "    # Define various conversion constants\n",
    "    water_density = 1000 # kg m-3\n",
    "    mm_per_m = 1000 # mm m-1\n",
    "    seconds_per_hour = 60*60 # s h-1\n",
    "    seconds_per_day = seconds_per_hour*24 # s d-1\n",
    "    days_per_month = np.array([31, 28.25, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]).reshape(-1, 1).flatten() # d month-1 || NOTE: added the .flatten() for distributed case, not tested for lumped\n",
    "    days_per_year = days_per_month.sum()\n",
    "    flip_sign = -1 # -; used to convert PET from negative (by convention this indicates an upward flux) to positive (ERA5 only)\n",
    "    kelvin_to_celsius = -273.15\n",
    "    pa_per_kpa = 1000 # Pa kPa-1\n",
    "    \n",
    "    # Select whole years only\n",
    "    #   This avoids issues in cases where we have incomplete whole data years\n",
    "    #   (e.g. 2000-06-01 to 2007-12-31) in basins with very seasonal weather\n",
    "    #   (e.g. all precip occurs in Jan, Feb, Mar). By using only full years\n",
    "    #   we avoid accidentally biasing the attributes.\n",
    "    ds = subset_dataset_to_max_full_years(ds)\n",
    "    num_years = len(ds.groupby('time.year'))\n",
    "    l_values.append(num_years)\n",
    "    l_index.append( ('Climate', 'num_years_rdrs', 'years', 'RDRS') )\n",
    "\n",
    "    # --- Annual statistics (P, PET, T, aridity, seasonality, temperature, snow)\n",
    "    # P\n",
    "    yearly_pr0 = ds['RDRS_v2.1_A_PR0_SFC'].resample(time='1Y').mean() * seconds_per_day * days_per_year * mm_per_m / water_density # kg m-2 s-1 > mm yr-1\n",
    "    l_values.append(yearly_pr0.mean().values)\n",
    "    l_index.append(('Climate', 'PR0_SFC_mean', 'mm', 'RDRS'))\n",
    "    l_values.append(yearly_pr0.std().values)\n",
    "    l_index.append(('Climate', 'PR0_SFC_std', 'mm', 'RDRS'))\n",
    "\n",
    "    # PET\n",
    "    yearly_pet = ds['pet'].resample(time='1Y').mean() * seconds_per_day * days_per_year * mm_per_m / water_density # kg m-2 s-1 > mm yr-1\n",
    "    l_values.append(yearly_pet.mean().values)\n",
    "    l_index.append(('Climate', 'pet_mean', 'mm', 'RDRS'))\n",
    "    l_values.append(yearly_pet.std().values)\n",
    "    l_index.append(('Climate', 'pet_std', 'mm', 'RDRS'))\n",
    "\n",
    "    # T\n",
    "    yearly_tt = ds['RDRS_v2.1_P_TT_1.5m'].resample(time='1Y').mean() + kelvin_to_celsius # K > C\n",
    "    l_values.append(yearly_tt.mean().values)\n",
    "    l_index.append(('Climate', 'TT_mean', 'C', 'RDRS'))\n",
    "    l_values.append(yearly_tt.std().values)\n",
    "    l_index.append(('Climate', 'TT_std', 'C', 'RDRS'))\n",
    "\n",
    "    # Aridity\n",
    "    yearly_ari  = yearly_pet / yearly_pr0\n",
    "    l_values.append(yearly_ari.mean().values)\n",
    "    l_index.append(('Climate', 'aridity1_mean', '-', 'RDRS'))\n",
    "    l_values.append(yearly_ari.std().values)\n",
    "    l_index.append(('Climate', 'aridity1_std', '-', 'RDRS'))\n",
    "\n",
    "    # Snow\n",
    "    ds['snow'] = xr.where(ds['RDRS_v2.1_P_TT_1.5m'] < 273.15, ds['RDRS_v2.1_A_PR0_SFC'],0)\n",
    "    yearly_snow = ds['snow'].resample(time='1Y').mean() * seconds_per_day * days_per_year * mm_per_m / water_density\n",
    "    yearly_fs = yearly_snow / yearly_pr0\n",
    "    l_values.append(yearly_fs.mean().values)\n",
    "    l_index.append(('Climate', 'fracsnow1_mean', '-', 'RDRS'))\n",
    "    l_values.append(yearly_fs.std().values)\n",
    "    l_index.append(('Climate', 'fracsnow1_std', '-', 'RDRS'))\n",
    "\n",
    "    # Seasonality\n",
    "    seasonality = find_climate_seasonality_rdrs(ds,use_typical_cycle=False)\n",
    "    l_values.append(seasonality.mean())\n",
    "    l_index.append(('Climate', 'seasonality1_mean', '-', 'RDRS'))\n",
    "    l_values.append(seasonality.std())\n",
    "    l_index.append(('Climate', 'seasonality1_std', '-', 'RDRS'))\n",
    "\n",
    "    # --- Monthly attributes\n",
    "    # Calculate monthly PET in mm\n",
    "    #      kg m-2 s-1 / kg m-3\n",
    "    # mm month-1 = kg m-2 s-1 * kg-1 m3 * s d-1 * d month-1 * mm m-1 * -\n",
    "    monthly_pet = ds['pet'].resample(time='1M').mean().groupby('time.month')\n",
    "    pet_m = monthly_pet.mean() / water_density * seconds_per_day * days_per_month * mm_per_m  # [kg m-2 s-1] to [mm month-1]; negative to indicate upward flux\n",
    "    pet_s = monthly_pet.std() / water_density * seconds_per_day * days_per_month * mm_per_m  # [kg m-2 s-1] to [mm month-1]; negative to indicate upward flux\n",
    "    l_values, l_index = process_monthly_means_to_lists(pet_m, 'mean', l_values, l_index, 'pet', 'mm', source='RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(pet_s, 'std', l_values, l_index, 'pet', 'mm', source='RDRS')\n",
    "\n",
    "    # Same for precipitation: [mm month-1]\n",
    "    monthly_pr0 = ds['RDRS_v2.1_A_PR0_SFC'].resample(time='1M').mean().groupby('time.month')\n",
    "    pr0_m = monthly_pr0.mean() / water_density * seconds_per_day * days_per_month * mm_per_m # [kg m-2 s-1] to [mm month-1]\n",
    "    pr0_s = monthly_pr0.std() / water_density * seconds_per_day * days_per_month * mm_per_m # [kg m-2 s-1] to [mm month-1]\n",
    "    l_values, l_index = process_monthly_means_to_lists(pr0_m, 'mean', l_values, l_index, 'RDRS_v2.1_A_PR0_SFC', 'mm', source='RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(pr0_s, 'std', l_values, l_index, 'RDRS_v2.1_A_PR0_SFC', 'mm', source='RDRS')\n",
    "\n",
    "    # Monthly temperature statistics [C]\n",
    "    monthly_tavg = (ds['RDRS_v2.1_P_TT_1.5m'].resample(time='1D').mean().resample(time='1M').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tavg_m = monthly_tavg.mean()\n",
    "    tavg_s = monthly_tavg.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(tavg_m, 'mean', l_values, l_index, 'tdavg', 'C', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(tavg_s, 'std', l_values, l_index, 'tdavg', 'C', source = 'RDRS')\n",
    "\n",
    "    monthly_tmin = (ds['RDRS_v2.1_P_TT_1.5m'].resample(time='1D').min().resample(time='1M').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tmin_m = monthly_tmin.mean()\n",
    "    tmin_s = monthly_tmin.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(tmin_m, 'mean', l_values, l_index, 'tdmin', 'C', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(tmin_m, 'std', l_values, l_index, 'tdmin', 'C', source = 'RDRS')\n",
    "    \n",
    "    monthly_tmax = (ds['RDRS_v2.1_P_TT_1.5m'].resample(time='1D').max().resample(time='1M').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tmax_m = monthly_tmax.mean()\n",
    "    tmax_s = monthly_tmax.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(tmax_m, 'mean', l_values, l_index, 'tdmax', 'C', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(tmax_s, 'std', l_values, l_index, 'tdmax', 'C', source = 'RDRS')\n",
    "\n",
    "    # Monthly shortwave and longwave [W m-2]\n",
    "    monthly_sw = ds['RDRS_v2.1_P_FB_SFC'].resample(time='1M').mean().groupby('time.month')\n",
    "    sw_m = monthly_sw.mean()\n",
    "    sw_s = monthly_sw.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(sw_m, 'mean', l_values, l_index, 'RDRS_v2.1_P_FB_SFC', 'W m^-2', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(sw_s, 'std', l_values, l_index, 'RDRS_v2.1_P_FB_SFC', 'W m^-2', source = 'RDRS')\n",
    "    \n",
    "    monthly_lw = ds['RDRS_v2.1_P_FI_SFC'].resample(time='1M').mean().groupby('time.month')\n",
    "    lw_m = monthly_lw.mean(dim='time')\n",
    "    lw_s = monthly_lw.std(dim='time')\n",
    "    l_values, l_index = process_monthly_means_to_lists(lw_m, 'mean', l_values, l_index, 'RDRS_v2.1_P_FI_SFC', 'W m^-2', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(lw_s, 'std', l_values, l_index, 'RDRS_v2.1_P_FI_SFC', 'W m^-2', source = 'RDRS')\n",
    "\n",
    "    # Surface pressure [Pa]\n",
    "    monthly_sp = ds['RDRS_v2.1_P_P0_SFC'].resample(time='1M').mean().groupby('time.month')\n",
    "    sp_m = monthly_sp.mean() / pa_per_kpa # [Pa] > [kPa]\n",
    "    sp_s = monthly_sp.std() / pa_per_kpa\n",
    "    l_values, l_index = process_monthly_means_to_lists(sp_m, 'mean', l_values, l_index, 'RDRS_v2.1_P_P0_SFC', 'kPa', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(sp_s, 'std', l_values, l_index, 'RDRS_v2.1_P_P0_SFC', 'kPa', source = 'RDRS')\n",
    "\n",
    "    # Humidity [-]\n",
    "    monthly_q = ds['RDRS_v2.1_P_HU_1.5m'].resample(time='1M').mean().groupby('time.month') # specific\n",
    "    q_m = monthly_q.mean()\n",
    "    q_s = monthly_q.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(q_m, 'mean', l_values, l_index, 'RDRS_v2.1_P_HU_1.5m', 'kg kg^-1', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(q_s, 'std', l_values, l_index, 'RDRS_v2.1_P_HU_1.5m', 'kg kg^-1', source = 'RDRS')\n",
    "    \n",
    "    monthly_rh = ds['RDRS_v2.1_P_HR_1.5m'].resample(time='1M').mean().groupby('time.month') # relative\n",
    "    rh_m = monthly_rh.mean()\n",
    "    rh_s = monthly_rh.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(rh_m, 'mean', l_values, l_index, 'RDRS_v2.1_P_HR_1.5m', 'kPa kPa^-1', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(rh_s, 'std', l_values, l_index, 'RDRS_v2.1_P_HR_1.5m', 'kPa kPa^-1', source = 'RDRS')\n",
    "\n",
    "    # Wind speed [m s-1]\n",
    "    monthly_w = ds['RDRS_v2.1_P_UVC_10m'].resample(time='1M').mean().groupby('time.month')\n",
    "    w_m = monthly_w.mean()\n",
    "    w_s = monthly_w.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(w_m, 'mean', l_values, l_index, 'RDRS_v2.1_P_UVC_10m', 'm s^-1', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(w_s, 'std', l_values, l_index, 'RDRS_v2.1_P_UVC_10m', 'm s^-1', source = 'RDRS')\n",
    "\n",
    "    # Wind direction\n",
    "    monthly_phi = ds['phi'].resample(time='1M').apply(circmean_group).groupby('time.month')\n",
    "    phi_m = monthly_phi.apply(circmean_group)\n",
    "    phi_s = monthly_phi.apply(circstd_group)\n",
    "    l_values, l_index = process_monthly_means_to_lists(phi_m, 'mean', l_values, l_index, 'phi', 'degrees', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(phi_s, 'std', l_values, l_index, 'phi', 'degrees', source = 'RDRS')\n",
    "\n",
    "    # aridity\n",
    "    monthly_pet = ds['pet'].resample(time='1M').mean()\n",
    "    monthly_pr0 = ds['RDRS_v2.1_A_PR0_SFC'].resample(time='1M').mean()\n",
    "    if (monthly_pr0 == 0).any():\n",
    "        print(f'--- WARNING: attributes_from_rdrs(): adding 1 mm to monthly precipitation to avoid divide by zero error in aridity calculation')\n",
    "        monthly_pr0[(monthly_pr0 == 0).sel(hru=0)] = 1 / mm_per_m * water_density / (seconds_per_day * days_per_month.mean()) # [mm month-1] / [mm m-1] * [kg m-3] / ([s d-1] * [d month-1]) = [kg m-2 s-1]\n",
    "    monthly_ari = (monthly_pet / monthly_pr0).groupby('time.month')\n",
    "    ari_m = monthly_ari.mean()\n",
    "    ari_s = monthly_ari.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(ari_m, 'mean', l_values, l_index, 'aridity1', '-', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(ari_s, 'std', l_values, l_index, 'aridity1', '-', source = 'RDRS')\n",
    "\n",
    "    # snow\n",
    "    monthly_snow = ds['snow'].resample(time='1M').mean()\n",
    "    monthly_pr0 = ds['RDRS_v2.1_A_PR0_SFC'].resample(time='1M').mean()\n",
    "    if (monthly_pr0 == 0).any():\n",
    "        print(f'--- WARNING: attributes_from_rdrs(): adding 1 mm to monthly precipitation to avoid divide by zero error in snow calculation. Note that by definition this cannot change the fraction snow result (if there is 0 precip, none of it will fall as snow)')\n",
    "        monthly_pr0[(monthly_pr0 == 0).sel(hru=0)] = 1 / mm_per_m * water_density / (seconds_per_day * days_per_month.mean()) # [mm month-1] / [mm m-1] * [kg m-3] / ([s d-1] * [d month-1]) = [kg m-2 s-1]\n",
    "    monthly_snow = (monthly_snow / monthly_pr0).groupby('time.month')\n",
    "    fsnow_m = monthly_snow.mean()\n",
    "    fsnow_s = monthly_snow.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(fsnow_m, 'mean', l_values, l_index, 'fracsnow1', '-', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(fsnow_s, 'std', l_values, l_index, 'fracsnow1', '-', source = 'RDRS') \n",
    "\n",
    "    # --- High-frequency statistics (high/low duration/timing/magnitude)\n",
    "    #  Everyone does precip. We'll add temperature too as a drought/frost indicator\n",
    "    \n",
    "    # -- LOW TEMPERATURE\n",
    "    variable  = 'RDRS_v2.1_P_TT_1.5m'\n",
    "    low_threshold = 273.15 # K, freezing point\n",
    "    low_condition = ds[variable] < low_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('temp',low_condition,'low',l_values,l_index, dataset='RDRS')\n",
    "\n",
    "    # -- HIGH TEMPERATURE\n",
    "    # WMO defines a heat wave as a 5-day or longer period with maximum daily temperatures 5C above \n",
    "    # \"standard\" daily max temperature (1961-1990; source:\n",
    "    # https://www.ifrc.org/sites/default/files/2021-06/10-HEAT-WAVE-HR.pdf).\n",
    "    # We define a \"hot day\" therefore as a day with a maximum temperature 5 degrees over the \n",
    "    # the long-term mean maximum temperature.\n",
    "    #   Note: we don't have 1961-1990 data for some stations, so we stick with long-term mean.\n",
    "    #   Note: this will in most cases slightly underestimate heat waves compared to WMO definition\n",
    "    \n",
    "    # First, we identify the long-term mean daily maximum temperature in a dedicated function\n",
    "    var = 'RDRS_v2.1_P_TT_1.5m'\n",
    "    high_threshold = create_mean_daily_max_series(ds,var=var)\n",
    "    \n",
    "    # Next, we check if which 't' values are 5 degrees above the long-term mean daily max \n",
    "    #  (\"(ds['t'] > result_array + 5)\"), and resample this to a daily time series \n",
    "    #  (\"resample(time='1D')\") filled with \"True\" if any value in that day was True.\n",
    "    daily_flags = (ds[var] > high_threshold + 5).resample(time='1D').any()\n",
    "    \n",
    "    # Finally, we reindex these daily flags back onto the hourly time series by filling values\n",
    "    high_condition = daily_flags.reindex_like(ds[var], method='ffill')\n",
    "    \n",
    "    # Now calculate stats like before\n",
    "    l_values,l_index = calculate_temp_prcp_stats('temp',high_condition,'high',l_values,l_index, dataset='RDRS')\n",
    "\n",
    "    # -- LOW PRECIPITATION\n",
    "    variable = 'RDRS_v2.1_A_PR0_SFC'\n",
    "    # We'll stick with the original CAMELS definition of low precipitation: < 1 mm day-1\n",
    "    # It may not make too much sense to look at \"dry hours\" so we'll do this analysis at daily step\n",
    "    low_threshold = 1 # [mm d-1]\n",
    "    # Create daily precipitation sum (divided by density, times mm m-1 cancels out)\n",
    "    # [kg m-2 s-1] * [s h-1] / [kg m-3] * [mm m-1] = [mm h-1]\n",
    "    low_condition = (ds[variable] * seconds_per_hour).resample(time='1D').sum() < low_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('prec',low_condition,'low',l_values,l_index,\n",
    "                                             units='days', dataset='RDRS') # this 'units' argument prevents conversion to days inside the functiom\n",
    "    \n",
    "    # -- HIGH PRECIPITATION\n",
    "    # CAMELS: > 5 times mean daily precip\n",
    "    high_threshold = 5 * (ds[variable] * seconds_per_hour).resample(time='1D').sum().mean()\n",
    "    high_condition = (ds[variable] * seconds_per_hour).resample(time='1D').sum() >= high_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('prec',high_condition,'high',l_values,l_index,\n",
    "                                                 units='days', dataset='RDRS')\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb47443-7406-4714-9d78-4052df00a7d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Merge the separate attribute dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb9b9e40-7a63-448e-bd3c-80b4b0919b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check we have the same columns in all dfs\n",
    "geo_columns = att_df_geo.columns.unique().sort_values()\n",
    "met_columns = att_df_met.columns.unique().sort_values()\n",
    "lak_columns = att_df_lakes.columns.unique().sort_values()\n",
    "glh_columns = att_df_glhymps.columns.unique().sort_values()\n",
    "\n",
    "assert (geo_columns == met_columns).all(), f\"COMID mismatches between meteo and geospatial attribute dataframes\"\n",
    "assert (geo_columns == lak_columns).all(), f\"COMID mismatches between hydrolakes and geospatial attribute dataframes\"\n",
    "assert (geo_columns == glh_columns).all(), f\"COMID mismatches between glhymps and geospatial attribute dataframes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "823d94a9-c280-4aef-93ec-d07791335093",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_df = pd.concat([att_df_met,att_df_geo,att_df_lakes,att_df_glhymps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4af438d4-4644-4b44-889f-d8f6cdfa04ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>72048321.0</th>\n",
       "      <th>72049179.0</th>\n",
       "      <th>72049188.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Climate</th>\n",
       "      <th>num_years_rdrs</th>\n",
       "      <th>years</th>\n",
       "      <th>RDRS</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR0_SFC_mean</th>\n",
       "      <th>mm</th>\n",
       "      <th>RDRS</th>\n",
       "      <td>1389.9713642913412</td>\n",
       "      <td>1383.0917654395307</td>\n",
       "      <td>1393.9310812791518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR0_SFC_std</th>\n",
       "      <th>mm</th>\n",
       "      <th>RDRS</th>\n",
       "      <td>153.05992530337002</td>\n",
       "      <td>155.11670134231477</td>\n",
       "      <td>153.5888911052156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pet_mean</th>\n",
       "      <th>mm</th>\n",
       "      <th>RDRS</th>\n",
       "      <td>1026.7960842582397</td>\n",
       "      <td>1022.7861444815062</td>\n",
       "      <td>1024.7584377462772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pet_std</th>\n",
       "      <th>mm</th>\n",
       "      <th>RDRS</th>\n",
       "      <td>50.76377868009653</td>\n",
       "      <td>48.809287944138774</td>\n",
       "      <td>50.45739506616813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Geology</th>\n",
       "      <th>porosity_std</th>\n",
       "      <th>-</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>0.071502</td>\n",
       "      <td>0.049969</td>\n",
       "      <td>0.056043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_permeability_min</th>\n",
       "      <th>m^2</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>-16.5</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_permeability_mean</th>\n",
       "      <th>m^2</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>-14.54584</td>\n",
       "      <td>-14.19256</td>\n",
       "      <td>-14.432358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_permeability_max</th>\n",
       "      <th>m^2</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>-12.5</td>\n",
       "      <td>-14.1</td>\n",
       "      <td>-12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_permeability_std</th>\n",
       "      <th>m^2</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>1.067653</td>\n",
       "      <td>0.305366</td>\n",
       "      <td>0.843112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>747 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      72048321.0  \\\n",
       "Category Attribute             Unit  Source                        \n",
       "Climate  num_years_rdrs        years RDRS                     24   \n",
       "         PR0_SFC_mean          mm    RDRS     1389.9713642913412   \n",
       "         PR0_SFC_std           mm    RDRS     153.05992530337002   \n",
       "         pet_mean              mm    RDRS     1026.7960842582397   \n",
       "         pet_std               mm    RDRS      50.76377868009653   \n",
       "...                                                          ...   \n",
       "Geology  porosity_std          -     GLHYMPS            0.071502   \n",
       "         log_permeability_min  m^2   GLHYMPS               -16.5   \n",
       "         log_permeability_mean m^2   GLHYMPS           -14.54584   \n",
       "         log_permeability_max  m^2   GLHYMPS               -12.5   \n",
       "         log_permeability_std  m^2   GLHYMPS            1.067653   \n",
       "\n",
       "                                                      72049179.0  \\\n",
       "Category Attribute             Unit  Source                        \n",
       "Climate  num_years_rdrs        years RDRS                     24   \n",
       "         PR0_SFC_mean          mm    RDRS     1383.0917654395307   \n",
       "         PR0_SFC_std           mm    RDRS     155.11670134231477   \n",
       "         pet_mean              mm    RDRS     1022.7861444815062   \n",
       "         pet_std               mm    RDRS     48.809287944138774   \n",
       "...                                                          ...   \n",
       "Geology  porosity_std          -     GLHYMPS            0.049969   \n",
       "         log_permeability_min  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_mean m^2   GLHYMPS           -14.19256   \n",
       "         log_permeability_max  m^2   GLHYMPS               -14.1   \n",
       "         log_permeability_std  m^2   GLHYMPS            0.305366   \n",
       "\n",
       "                                                      72049188.0  \n",
       "Category Attribute             Unit  Source                       \n",
       "Climate  num_years_rdrs        years RDRS                     24  \n",
       "         PR0_SFC_mean          mm    RDRS     1393.9310812791518  \n",
       "         PR0_SFC_std           mm    RDRS      153.5888911052156  \n",
       "         pet_mean              mm    RDRS     1024.7584377462772  \n",
       "         pet_std               mm    RDRS      50.45739506616813  \n",
       "...                                                          ...  \n",
       "Geology  porosity_std          -     GLHYMPS            0.056043  \n",
       "         log_permeability_min  m^2   GLHYMPS               -16.5  \n",
       "         log_permeability_mean m^2   GLHYMPS          -14.432358  \n",
       "         log_permeability_max  m^2   GLHYMPS               -12.5  \n",
       "         log_permeability_std  m^2   GLHYMPS            0.843112  \n",
       "\n",
       "[747 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af286f-8d98-4344-8e8b-b6b8300fa1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9b459-8a9f-4210-b7ff-bab3819d9990",
   "metadata": {},
   "source": [
    "#### Full code trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b44331c-4cd6-4b2a-ba99-db3a767f9ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!!! CHECK DEBUGGING STATUS: \n",
      "- Testing 1 file \n",
      "- Testing 1 basin\n",
      "Processing geospatial data into attributes for CAN_02QA002\n",
      " - processing rdrs\n",
      "[]\n",
      " - processing worldclim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/globalhome/wmk934/HPC/camels_spat/camels-spat-env-jupyter/lib/python3.10/site-packages/rasterstats/io.py:328: NodataWarning: Setting nodata to -999; specify nodata explicitly\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - processing hydrology\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "attributes_from_streamflow(): mismatch between precipitation and streamflow final timestamp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_625160/2369865323.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0ml_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributes_from_hydrolakes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeo_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'hydrology'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0ml_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributes_from_streamflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyd_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasin_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_precip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m## SOIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/camels_spat/python_cs_functions/attributes.py\u001b[0m in \u001b[0;36mattributes_from_streamflow\u001b[0;34m(hyd_folder, dataset, basin_id, pre, row, l_values, l_index)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;31m# Signatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0ml_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_signatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ml_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/camels_spat/python_cs_functions/attributes.py\u001b[0m in \u001b[0;36mcalculate_signatures\u001b[0;34m(hyd, pre, source, l_values, l_index)\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[0mhyd_ss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# subset streamflow to precip record length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mhyd_ss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attributes_from_streamflow(): mismatch between precipitation and streamflow start timestamp'\u001b[0m \u001b[0;31m# confirm time periods are the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mhyd_ss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attributes_from_streamflow(): mismatch between precipitation and streamflow final timestamp'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyd_ss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'water_year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'water_year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: attributes_from_streamflow(): mismatch between precipitation and streamflow final timestamp"
     ]
    }
   ],
   "source": [
    "print(debug_message)\n",
    "for ix,row in cs_meta.iterrows():\n",
    "\n",
    "    # DEBUGGING\n",
    "    if ix != 46: continue\n",
    "\n",
    "    # Get the paths\n",
    "    basin_id, shp_lump_path, shp_dist_path, _, _ = prepare_delineation_outputs(cs_meta, ix, basins_path)\n",
    "    geo_folder = basins_path / 'basin_data' / basin_id / 'geospatial'\n",
    "    met_folder = basins_path / 'basin_data' / basin_id / 'forcing'\n",
    "    hyd_folder = basins_path / 'basin_data' / basin_id / 'observations'\n",
    "\n",
    "    # Define the shapefiles\n",
    "    shp = str(shp_dist_path).format('basin') # because zonalstats wants a file path, not a geodataframe\n",
    "    riv = str(shp_dist_path).format('river') # For topographic attributes\n",
    "\n",
    "    # Load the shapefile to get the sub-basin order for geotiffs and shapefiles\n",
    "    gdf = gpd.read_file(shp)\n",
    "\n",
    "    # Data storage\n",
    "    l_comids_geo = gdf['COMID'].to_list() # Store the polygon IDs into a list, we'll later use this as columns in the attribute dataframes\n",
    "    l_values_geo = [[] for _ in range(len(l_comids_geo))] # Initialize an empty list where we'll store this basin's attributes - nested lists for each subbasin\n",
    "    l_index_geo = [] # Initialize an empty list where we'll store the attribute descriptions - this will be our dataframe index\n",
    "\n",
    "    l_values_lakes = [[] for _ in range(len(l_comids_geo))] # HydroLAKES open water bodies shapefile\n",
    "    l_index_lakes = []\n",
    "    l_values_glhymps = [[] for _ in range(len(l_comids_geo))] # GLHYMPS geology shapefile\n",
    "    l_index_glhymps = []\n",
    "\n",
    "    l_values_met = [] # RDRS meteorological netcdf data - creating a nested list with subbasins is done differently for RDRS, so this line is supposed to look different from the other l_values_[x] lists\n",
    "    l_index_met = []\n",
    "\n",
    "    # Data-specific processing\n",
    "    print(f'Processing geospatial data into attributes for {basin_id}')\n",
    "    for dataset in data_subfolders:\n",
    "        print(f' - processing {dataset}')\n",
    "\n",
    "        ## CLIMATE\n",
    "        if dataset == 'rdrs':\n",
    "            #l_values, l_index, ds_precip, ds_rdrs = csa.attributes_from_rdrs(met_folder, shp, dataset, l_values, l_index)\n",
    "            l_values_met, l_index_met, l_comids_met, _ = attributes_from_rdrs(met_folder, shp, dataset, l_values_met, l_index_met)\n",
    "        if dataset == 'worldclim':\n",
    "            csa.oudin_pet_from_worldclim(geo_folder, dataset) # Get an extra PET estimate to sanity check RDRS outcomes\n",
    "            csa.aridity_and_fraction_snow_from_worldclim(geo_folder, dataset) # Get monthly aridity and fraction snow maps\n",
    "            l_values, l_index = csa.attributes_from_worldclim(geo_folder, dataset, shp, l_values, l_index)\n",
    "\n",
    "        ## LAND COVER\n",
    "        if dataset == 'forest_height':\n",
    "            l_values, l_index = csa.attributes_from_forest_height(geo_folder, dataset, shp, l_values, l_index)\n",
    "        if dataset == 'lai':\n",
    "            l_values, l_index = csa.attributes_from_lai(geo_folder, dataset, temp_path, shp, l_values, l_index)\n",
    "        if dataset == 'glclu2019':\n",
    "            l_values, l_index = csa.attributes_from_glclu2019(geo_folder, dataset, shp, l_values, l_index)\n",
    "        if dataset == 'modis_land':\n",
    "            l_values, l_index = csa.attributes_from_modis_land(geo_folder, dataset, shp, l_values, l_index)\n",
    "        if dataset == 'lgrip30':\n",
    "            l_values, l_index = csa.attributes_from_lgrip30(geo_folder, dataset, shp, l_values, l_index)\n",
    "\n",
    "        ## TOPOGRAPHY\n",
    "        if dataset == 'merit':\n",
    "            l_values, l_index = csa.attributes_from_merit(geo_folder, dataset, shp, riv, row, l_values, l_index)\n",
    "\n",
    "        ## OPENWATER\n",
    "        if dataset == 'hydrolakes':\n",
    "            #l_values, l_index = csa.attributes_from_hydrolakes(geo_folder, dataset, l_values, l_index)\n",
    "            l_values_lakes, l_index_lakes, l_comids_lakes = attributes_from_hydrolakes(geo_folder, dataset, shp, ea_crs, \n",
    "                                                                                       l_values_lakes, l_index_lakes)\n",
    "\n",
    "        ## SOIL\n",
    "        if dataset == 'pelletier':\n",
    "            l_values, l_index = csa.attributes_from_pelletier(geo_folder, dataset, shp, l_values, l_index)\n",
    "        if dataset == 'soilgrids':\n",
    "            l_values, l_index = csa.attributes_from_soilgrids(geo_folder, dataset, shp, l_values, l_index)\n",
    "\n",
    "        ## GEOLOGY\n",
    "        if dataset == 'glhymps':\n",
    "            #l_values, l_index = csa.attributes_from_glhymps(geo_folder, dataset, l_values, l_index)\n",
    "            l_values_glhymps, l_index_glhymps, l_comids_glhymps = attributes_from_glhymps(geo_folder, dataset, shp, \n",
    "                                                                                          l_values_glhymps, l_index_glhymps, \n",
    "                                                                                          equal_area_crs=ea_crs)\n",
    "\n",
    "        ## MERGE THE DATAFRAMES\n",
    "        # Make the individual dataframes\n",
    "        # - RDRS\n",
    "        att_df_met = pd.DataFrame(data = l_values_met, index = l_comids_met).transpose()\n",
    "        multi_index = pd.MultiIndex.from_tuples(l_index_met, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "        att_df_met.index = multi_index\n",
    "\n",
    "        # - geotiffs\n",
    "        att_df_geo = pd.DataFrame(data = l_values_geo, index = l_comids_geo).transpose()\n",
    "        multi_index = pd.MultiIndex.from_tuples(l_index_geo, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "        att_df_geo.index = multi_index\n",
    "        \n",
    "        # - HydroLAKES\n",
    "        att_df_lakes = pd.DataFrame(data = l_values_lakes, index = l_comids_lakes).transpose()\n",
    "        multi_index = pd.MultiIndex.from_tuples(l_index_lakes, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "        att_df_lakes.index = multi_index\n",
    "\n",
    "        # - GLHYMPS\n",
    "        att_df_glhymps = pd.DataFrame(data = l_values_glhymps, index = l_comids_glhymps).transpose()\n",
    "        multi_index = pd.MultiIndex.from_tuples(l_index_glhymps, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "        att_df_glhymps.index = multi_index\n",
    "\n",
    "        # Check we have the same columns in all dfs\n",
    "        geo_columns = att_df_geo.columns.unique().sort_values()\n",
    "        met_columns = att_df_met.columns.unique().sort_values()\n",
    "        lak_columns = att_df_lakes.columns.unique().sort_values()\n",
    "        glh_columns = att_df_glhymps.columns.unique().sort_values()\n",
    "        assert (geo_columns == met_columns).all(), f\"COMID mismatches between meteo and geospatial attribute dataframes\"\n",
    "        assert (geo_columns == lak_columns).all(), f\"COMID mismatches between hydrolakes and geospatial attribute dataframes\"\n",
    "        assert (geo_columns == glh_columns).all(), f\"COMID mismatches between glhymps and geospatial attribute dataframes\"\n",
    "\n",
    "        # Merge and save\n",
    "        att_df = pd.concat([att_df_met,att_df_geo,att_df_lakes,att_df_glhymps])\n",
    "            \n",
    "print(debug_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a7fc77-bd53-4181-9079-dd4a7a7df099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3e7fd3-54b2-41a6-9df0-a2785064299b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels-spat-env-jupyter",
   "language": "python",
   "name": "camels-spat-env-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
