{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db3e732-426b-4b22-b055-b9395f29fa09",
   "metadata": {},
   "source": [
    "## Calculate attributes per subbasin\n",
    "Takes prepared geospatial data and computes various attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac7b17d-07d9-4985-bb9c-8508fb2c7554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "from python_cs_functions import config as cs, attributes as csa\n",
    "from python_cs_functions.delineate import prepare_delineation_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a14e3a-4a96-4308-9f4b-4bc1a2eceab8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### DEV only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7931b34c-e133-4a6c-a137-8366898954dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import glob\n",
    "import numpy as np \n",
    "import os\n",
    "import xarray as xr\n",
    "from rasterstats import zonal_stats\n",
    "from python_cs_functions.attributes import read_scale_and_offset, zonal_out_none2nan, subset_dataset_to_max_full_years, process_monthly_means_to_lists\n",
    "from python_cs_functions.attributes import find_climate_seasonality_rdrs, circmean_group, circstd_group, calculate_temp_prcp_stats, find_durations, get_season\n",
    "from python_cs_functions.attributes import create_mean_daily_max_series, get_open_water_stats, get_categorical_dict, check_scale_and_offset\n",
    "from scipy.stats import circmean, circstd, skew, kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bcba8f-d99c-4058-9de9-23ae255c19e9",
   "metadata": {},
   "source": [
    "### Config handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dfdcc03-f734-4df6-8bee-25ae59f5560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where the config file can be found\n",
    "config_file = '../0_config/config.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aa42aa7-cf4d-4c51-8555-05638e2ffc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the required info from the config file\n",
    "data_path            = cs.read_from_config(config_file,'data_path')\n",
    "\n",
    "# CAMELS-spat metadata\n",
    "cs_meta_path = cs.read_from_config(config_file,'cs_basin_path')\n",
    "cs_meta_name = cs.read_from_config(config_file,'cs_meta_name')\n",
    "cs_unusable_name = cs.read_from_config(config_file,'cs_unusable_name')\n",
    "\n",
    "# Basin folder\n",
    "cs_basin_folder = cs.read_from_config(config_file, 'cs_basin_path')\n",
    "basins_path = Path(data_path) / cs_basin_folder\n",
    "\n",
    "# Get the temporary data folder\n",
    "cs_temp_folder = cs.read_from_config(config_file, 'temp_path')\n",
    "temp_path = Path(cs_temp_folder)\n",
    "temp_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Get the attribute folder\n",
    "att_folder = cs.read_from_config(config_file, 'att_path')\n",
    "att_path = basins_path / att_folder / 'distributed'\n",
    "att_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get the CRS we want to do area calculations in (possibly helpful for GLHYMPS and HydroLAKES)\n",
    "ea_crs = cs.read_from_config(config_file, 'equal_area_crs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933dae9c-b5fe-42d2-9cc9-2ee54661e2c7",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "333bd63a-b0a1-417d-b9c6-d28306a0120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAMELS-spat metadata file\n",
    "cs_meta_path = Path(data_path) / cs_meta_path\n",
    "cs_meta = pd.read_csv(cs_meta_path / cs_meta_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a497a0-a9a2-404c-9b2a-8173cbb1436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open list of unusable stations; Enforce reading IDs as string to keep leading 0's\n",
    "cs_unusable = pd.read_csv(cs_meta_path / cs_unusable_name, dtype={'Station_id': object})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87113cc-bfb3-48dd-bae1-4c7f19fd39b3",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3185dc90-2a46-46b2-bdd1-c0032c2c452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_message = f'\\n!!! CHECK DEBUGGING STATUS: \\n- Testing 1 file \\n- Testing 1 basin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c395a8-7e5d-4140-b61d-65581fc05a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed 'hydrology', because we don't have gauges for every subbasin\n",
    "data_subfolders = ['rdrs', 'worldclim', 'lai', 'forest_height', 'glclu2019', 'modis_land', 'lgrip30', 'merit', 'hydrolakes', 'pelletier', 'soilgrids', 'glhymps']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b52ec8-3e62-458b-bdee-ea394b72e304",
   "metadata": {},
   "source": [
    "### DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03391c31-fc73-4824-bfe2-8e45edb48478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAN_01AK006\n"
     ]
    }
   ],
   "source": [
    "# Set up a relatively simple test case (not too big)\n",
    "ix = 9\n",
    "row = cs_meta.iloc[ix]\n",
    "print(f\"{row['Country']}_{row['Station_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3e28b28-c07c-47df-9676-d38b0a607010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the paths\n",
    "basin_id, shp_lump_path, shp_dist_path, _, _ = prepare_delineation_outputs(cs_meta, ix, basins_path)\n",
    "geo_folder = basins_path / 'basin_data' / basin_id / 'geospatial'\n",
    "met_folder = basins_path / 'basin_data' / basin_id / 'forcing'\n",
    "hyd_folder = basins_path / 'basin_data' / basin_id / 'observations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea1ed379-dc7e-4057-b44c-775a77130aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the shapefiles\n",
    "shp = str(shp_dist_path).format('basin') # because zonalstats wants a file path, not a geodataframe\n",
    "riv = str(shp_dist_path).format('river') # For topographic attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331a258-7214-4574-b885-60cd7e68c089",
   "metadata": {},
   "source": [
    "Test the different data products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ca8f2-699b-4302-a081-2a0635243cf5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Fix the distributed case for geotiffs\n",
    "Trial with forest_height (general case), soilgrids (special averaging function), glclu (categorical outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41536796-3f72-4e29-a05f-6dcc3710c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile to get the sub-basin order for geotiffs and shapefiles\n",
    "gdf = gpd.read_file(shp)\n",
    "l_comids_geo = gdf['COMID'].to_list() # Store the polygon IDs into a list, we'll later use this as columns in the attribute dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f97ddb05-525d-48ef-b40d-76c6b6af5e37",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "attributes_from_soilgrids() got an unexpected keyword argument 'case'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2622370/2605712360.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'soilgrids'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'soilgrids'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0ml_values_geo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_index_geo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributes_from_soilgrids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeo_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_values_geo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_index_geo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'distributed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m '''\n\u001b[1;32m     14\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'glclu2019'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: attributes_from_soilgrids() got an unexpected keyword argument 'case'"
     ]
    }
   ],
   "source": [
    "# Initialize storage lists for geotiff and shapefile attributes\n",
    "l_values_geo = [[] for _ in range(len(l_comids_geo))] # Initialize an empty list where we'll store this basin's attributes - nested lists for each subbasin\n",
    "l_index_geo = [] # Initialize an empty list where we'll store the attribute descriptions - this will be our dataframe index\n",
    "\n",
    "# geotiffs\n",
    "dataset = 'forest_height'\n",
    "if dataset == 'forest_height':\n",
    "    l_values_geo, l_index_geo = attributes_from_forest_height(geo_folder, dataset, shp, l_values_geo, l_index_geo, case='distributed')\n",
    "\n",
    "dataset = 'soilgrids'\n",
    "if dataset == 'soilgrids':\n",
    "    l_values_geo, l_index_geo = csa.attributes_from_soilgrids(geo_folder, dataset, shp, l_values_geo, l_index_geo, case='distributed')\n",
    "\n",
    "dataset = 'glclu2019'\n",
    "if dataset == 'glclu2019':\n",
    "    l_values_geo, l_index_geo = csa.attributes_from_glclu2019(geo_folder, dataset, shp, l_values_geo, l_index_geo, case='distributed')\n",
    "\n",
    "dataset = 'merit'\n",
    "l_values_geo, l_index_geo, l_comids_merit = attributes_from_merit(geo_folder, dataset, shp, riv, row, l_values_geo, l_index_geo, equal_area_crs=ea_crs, case='distributed')\n",
    "assert (l_comids_geo == l_comids_merit).all(), f\"mismatch between COMID orders determined before and inside attributes_from_merit()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9c50f681-d2f9-4120-b98e-c33f4a1406b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the geospatial attribute dataframe\n",
    "att_df_geo = pd.DataFrame(data = l_values_geo, index = l_comids_geo).transpose()\n",
    "multi_index = pd.MultiIndex.from_tuples(l_index_geo, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "att_df_geo.index = multi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "100e788c-a330-466b-b57c-952cc70d0d51",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_values_list(l_values, stats, zonal_out, scale, offset, case='lumped'):\n",
    "\n",
    "    # Update scale and offset to usable values\n",
    "    if scale is None: scale = 1 # If scale is undefined that means we simply multiply by 1\n",
    "    if offset is None: offset = 0 # Undefined offset > add 0\n",
    "\n",
    "    # Deal with the occassional None that we get when raster data input to zonal-stats is missing\n",
    "    # This fixes 9 occurrences of missing data in the soilgrids maps\n",
    "    zonal_out = zonal_out_none2nan(zonal_out)\n",
    "    \n",
    "    # We loop through the calculated stats in a pre-determined order:\n",
    "    # 1. min\n",
    "    # 2. mean\n",
    "    # 3. max\n",
    "    # 4. stdev\n",
    "    # 5. ..\n",
    "    if case == 'lumped':\n",
    "        if 'min' in stats:  l_values.append(zonal_out[0]['min']  * scale + offset)\n",
    "        if 'mean' in stats: l_values.append(zonal_out[0]['mean'] * scale + offset)\n",
    "        if 'harmonic_mean'  in stats: l_values.append(zonal_out[0]['harmonic_mean'] * scale + offset) # only here for soilgrids conductivity, in which case we don't have 'mean'\n",
    "        if 'max' in stats:  l_values.append(zonal_out[0]['max']  * scale + offset)\n",
    "        if 'std' in stats:  l_values.append(zonal_out[0]['std']  * scale + offset)\n",
    "\n",
    "    # distributed case, multiple polygons\n",
    "    elif case == 'distributed':\n",
    "        \n",
    "        # confirm that l_values has as many nested lists as we have zonal stats outputs\n",
    "        num_nested_lists = sum(1 for item in l_values if isinstance(item, list))\n",
    "        assert num_nested_lists == len(zonal_out), f\"zonal_out length does not match expected list length {num_nested_lists}. zonal_out: {zonal_out}\"\n",
    "        \n",
    "        # now loop over the zonal outputs and append to relevant lists\n",
    "        for i in range(0,num_nested_lists):\n",
    "            if 'min' in stats:  l_values[i].append(zonal_out[i]['min']  * scale + offset)\n",
    "            if 'mean' in stats: l_values[i].append(zonal_out[i]['mean'] * scale + offset)\n",
    "            if 'harmonic_mean'  in stats: l_values[i].append(zonal_out[i]['harmonic_mean'] * scale + offset) # only here for soilgrids conductivity, in which case we don't have 'mean'\n",
    "            if 'max' in stats:  l_values[i].append(zonal_out[i]['max']  * scale + offset)\n",
    "            if 'std' in stats:  l_values[i].append(zonal_out[i]['std']  * scale + offset)\n",
    "\n",
    "    return l_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53f105a2-3ca8-42fc-a91e-42d2892b61fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def harmonic_mean(x):\n",
    "    return np.ma.count(x) / (1/x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a111a2a-79c6-4178-b239-b2291f5f30c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def attributes_from_soilgrids(geo_folder, dataset, shp_str, l_values, l_index):\n",
    "\n",
    "    '''Calculates attributes from SOILGRIDS maps'''\n",
    "\n",
    "    # File specifiction\n",
    "    sub_folders = ['bdod',     'cfvo',       'clay',    'sand',    'silt',    'soc',      'porosity']\n",
    "    units       = ['cg cm^-3', 'cm^3 dm^-3', 'g kg^-1', 'g kg^-1', 'g kg^-1', 'dg kg^-1', '-']\n",
    "    depths = ['0-5cm', '5-15cm', '15-30cm', '30-60cm', '60-100cm', '100-200cm']\n",
    "    fields = ['mean', 'uncertainty']\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    \n",
    "    # Loop over the files and calculate stats\n",
    "    for sub_folder, unit in zip(sub_folders, units):\n",
    "        for depth in depths:\n",
    "            for field in fields:\n",
    "                tif = str(geo_folder / dataset / 'raw' / f'{sub_folder}' / f'{sub_folder}_{depth}_{field}.tif')\n",
    "                if not os.path.exists(tif): continue # porosity has no uncertainty field\n",
    "                zonal_out = zonal_stats(shp_str, tif, stats=stats)\n",
    "                scale,offset = read_scale_and_offset(tif)\n",
    "                l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "                l_index += [('Soil', f'{sub_folder}_{depth}_{field}_min',  f'{unit}', 'SOILGRIDS'),\n",
    "                            ('Soil', f'{sub_folder}_{depth}_{field}_mean', f'{unit}', 'SOILGRIDS'),\n",
    "                            ('Soil', f'{sub_folder}_{depth}_{field}_max',  f'{unit}', 'SOILGRIDS'),\n",
    "                            ('Soil', f'{sub_folder}_{depth}_{field}_std',  f'{unit}', 'SOILGRIDS')]\n",
    "           \n",
    "    # --- Specifc processing for the conductivity fields\n",
    "    # For conductivity we want to have a harmonic mean because that should be more\n",
    "    # representative as a spatial average. rasterstats doesn't have that, so we \n",
    "    # need a custom function. We're splitting out the processing from the rest for\n",
    "    # clarity. Could also have done this with a bunch of if-statements, but this \n",
    "    # seems cleaner, also because conductivity doesn't have uncertainty maps.\n",
    "    \n",
    "    # Process the data fields\n",
    "    sub_folder = 'conductivity'\n",
    "    unit       = 'cm hr^-1'\n",
    "    depths = ['0-5cm', '5-15cm', '15-30cm', '30-60cm', '60-100cm', '100-200cm']\n",
    "    field  = 'mean' # no uncertainty maps for these\n",
    "    stats = ['min', 'max', 'std']\n",
    "\n",
    "    for depth in depths:\n",
    "        tif = str(geo_folder / dataset / 'raw' / f'{sub_folder}' / f'{sub_folder}_{depth}_{field}.tif')\n",
    "        if not os.path.exists(tif): continue # porosity has no uncertainty field\n",
    "        zonal_out = zonal_stats(shp_str, tif, stats=stats, add_stats={'harmonic_mean': harmonic_mean})\n",
    "        scale,offset = read_scale_and_offset(tif)\n",
    "        l_values = update_values_list(l_values, ['min', 'max', 'std', 'harmonic_mean'], zonal_out, scale, offset)\n",
    "        l_index += [('Soil', f'{sub_folder}_{depth}_{field}_min',  f'{unit}', 'SOILGRIDS'),\n",
    "                    ('Soil', f'{sub_folder}_{depth}_{field}_harmonic_mean', f'{unit}', 'SOILGRIDS'),\n",
    "                    ('Soil', f'{sub_folder}_{depth}_{field}_max',  f'{unit}', 'SOILGRIDS'),\n",
    "                    ('Soil', f'{sub_folder}_{depth}_{field}_std',  f'{unit}', 'SOILGRIDS')]\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f2a28dcf-8f8f-4c27-90f7-5b501dbba25b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def attributes_from_forest_height(geo_folder, dataset, shp_str, l_values, index, case='lumped'):\n",
    "\n",
    "    '''Calculates mean, min, max and stdv for forest height 2000 and 2020 tifs'''\n",
    "\n",
    "    # Year 2000 min, mean, max, stdev\n",
    "    tif = str( geo_folder / dataset / 'raw' / 'forest_height_2000.tif' )\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    zonal_out = zonal_stats(shp_str, tif, stats=stats)\n",
    "    scale,offset = read_scale_and_offset(tif)\n",
    "    l_values = update_values_list(l_values, stats, zonal_out, scale, offset, case=case)\n",
    "    index += [('Land cover', 'forest_height_2000_min',   'm', 'GLCLUC 2000-2020'),\n",
    "              ('Land cover', 'forest_height_2000_mean',  'm', 'GLCLUC 2000-2020'),\n",
    "              ('Land cover', 'forest_height_2000_max',   'm', 'GLCLUC 2000-2020'),\n",
    "              ('Land cover', 'forest_height_2000_std',   'm', 'GLCLUC 2000-2020')]\n",
    "\n",
    "    # Year 2020 mean, stdev\n",
    "    tif = geo_folder / dataset / 'raw' / 'forest_height_2020.tif'\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    zonal_out = zonal_stats(shp_str, tif, stats=stats)\n",
    "    scale,offset = read_scale_and_offset(tif)\n",
    "    l_values = update_values_list(l_values, stats, zonal_out, scale, offset, case=case)\n",
    "    index += [('Land cover', 'forest_height_2020_min',   'm', 'GLCLUC 2000-2020'),\n",
    "              ('Land cover', 'forest_height_2020_mean',  'm', 'GLCLUC 2000-2020'),\n",
    "              ('Land cover', 'forest_height_2020_max',   'm', 'GLCLUC 2000-2020'),\n",
    "              ('Land cover', 'forest_height_2020_std',   'm', 'GLCLUC 2000-2020')]\n",
    "\n",
    "    return l_values, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23a27556-a14b-4134-9fc3-5c0bced5f692",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def attributes_from_glclu2019(geo_folder, dataset, shp_str, l_values, l_index):\n",
    "\n",
    "    '''Calculates percentage occurrence of all classes in GLCLU2019 map'''\n",
    "\n",
    "    tif = geo_folder / dataset / 'raw' / 'glclu2019_map.tif'\n",
    "    zonal_out = zonal_stats(shp_str, tif, categorical=True)\n",
    "    check_scale_and_offset(tif)\n",
    "    l_values,l_index = update_values_list_with_categorical(l_values, l_index, zonal_out, 'GLCLU 2019', prefix='lc1_')\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fdf92013-5644-413a-986b-c2001d0e6a67",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_values_list_with_categorical(l_values, l_index, zonal_out, source, prefix='', case='lumped'):\n",
    "    '''Maps a zonal histogram of categorical classes onto descriptions and adds to lists'''\n",
    "\n",
    "    # Get the category definitions\n",
    "    cat_dict = get_categorical_dict(source)    \n",
    "\n",
    "    # Separately handle lumped and distributed cases\n",
    "    if case == 'lumped':\n",
    "    \n",
    "        # Find the total number of classified pixels\n",
    "        total_pixels = 0\n",
    "        for land_id,count in zonal_out[0].items():\n",
    "            total_pixels += count\n",
    "        \n",
    "        # Loop over all categories and see what we have in this catchment\n",
    "        for land_id,text in cat_dict.items():\n",
    "            land_prct = 0\n",
    "            if land_id in zonal_out[0].keys():\n",
    "                land_prct = zonal_out[0][land_id] / total_pixels\n",
    "            l_values.append(land_prct)\n",
    "            l_index.append(('Land cover', f'{prefix}{text}_fraction', '-', f'{source}'))\n",
    "\n",
    "    # distributed case, multiple polygons\n",
    "    elif case == 'distributed': \n",
    "\n",
    "        # confirm that l_values has as many nested lists as we have zonal stats outputs\n",
    "        num_nested_lists = sum(1 for item in l_values if isinstance(item, list))\n",
    "        assert num_nested_lists == len(zonal_out), f\"zonal_out length does not match expected list length {num_nested_lists}. zonal_out: {zonal_out}\"\n",
    "\n",
    "        # now loop over the zonal outputs and append to relevant lists\n",
    "        for i in range(0,num_nested_lists):\n",
    "\n",
    "            # Find the total number of classified pixels\n",
    "            total_pixels = 0\n",
    "            for land_id,count in zonal_out[i].items():\n",
    "                total_pixels += count\n",
    "            \n",
    "            # Loop over all categories and see what we have in this catchment\n",
    "            tmp_index = [] # we need this so the index resets on each subbasin iteration, and we need that because we only need the index once\n",
    "            for land_id,text in cat_dict.items():\n",
    "                land_prct = 0\n",
    "                if land_id in zonal_out[i].keys():\n",
    "                    land_prct = zonal_out[i][land_id] / total_pixels\n",
    "                l_values[i].append(land_prct)\n",
    "                tmp_index.append(('Land cover', f'{prefix}{text}_fraction', '-', f'{source}'))\n",
    "\n",
    "        # Add the index values only once\n",
    "        for item in tmp_index:\n",
    "            l_index.append(item)\n",
    "\n",
    "    return l_values,l_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2500a85e-7d72-4dbf-83e5-0a1a5a692a28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Fix the WorldClim distributed case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ed2c00e-256e-486a-9fc6-af834f771c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>72048321</th>\n",
       "      <th>72049179</th>\n",
       "      <th>72049188</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">Climate</th>\n",
       "      <th>prec_mean</th>\n",
       "      <th>mm</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>1269.745763</td>\n",
       "      <td>1348.750000</td>\n",
       "      <td>1359.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prec_std</th>\n",
       "      <th>mm</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>51.918970</td>\n",
       "      <td>34.964565</td>\n",
       "      <td>29.905377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pet_mean</th>\n",
       "      <th>mm</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>256.894862</td>\n",
       "      <td>247.144816</td>\n",
       "      <td>248.320435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pet_std</th>\n",
       "      <th>mm</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>4.371759</td>\n",
       "      <td>4.058496</td>\n",
       "      <td>3.083181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tavg_mean</th>\n",
       "      <th>C</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>5.410735</td>\n",
       "      <td>4.851562</td>\n",
       "      <td>4.921528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fracsnow2_std_month_10</th>\n",
       "      <th>-</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fracsnow2_mean_month_11</th>\n",
       "      <th>-</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fracsnow2_std_month_11</th>\n",
       "      <th>-</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fracsnow2_mean_month_12</th>\n",
       "      <th>-</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fracsnow2_std_month_12</th>\n",
       "      <th>-</th>\n",
       "      <th>WorldClim</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    72048321     72049179  \\\n",
       "Category Attribute               Unit Source                                \n",
       "Climate  prec_mean               mm   WorldClim  1269.745763  1348.750000   \n",
       "         prec_std                mm   WorldClim    51.918970    34.964565   \n",
       "         pet_mean                mm   WorldClim   256.894862   247.144816   \n",
       "         pet_std                 mm   WorldClim     4.371759     4.058496   \n",
       "         tavg_mean               C    WorldClim     5.410735     4.851562   \n",
       "...                                                      ...          ...   \n",
       "         fracsnow2_std_month_10  -    WorldClim     0.000000     0.000000   \n",
       "         fracsnow2_mean_month_11 -    WorldClim     0.000000     0.000000   \n",
       "         fracsnow2_std_month_11  -    WorldClim     0.000000     0.000000   \n",
       "         fracsnow2_mean_month_12 -    WorldClim     1.000000     1.000000   \n",
       "         fracsnow2_std_month_12  -    WorldClim     0.000000     0.000000   \n",
       "\n",
       "                                                    72049188  \n",
       "Category Attribute               Unit Source                  \n",
       "Climate  prec_mean               mm   WorldClim  1359.958333  \n",
       "         prec_std                mm   WorldClim    29.905377  \n",
       "         pet_mean                mm   WorldClim   248.320435  \n",
       "         pet_std                 mm   WorldClim     3.083181  \n",
       "         tavg_mean               C    WorldClim     4.921528  \n",
       "...                                                      ...  \n",
       "         fracsnow2_std_month_10  -    WorldClim     0.000000  \n",
       "         fracsnow2_mean_month_11 -    WorldClim     0.000000  \n",
       "         fracsnow2_std_month_11  -    WorldClim     0.000000  \n",
       "         fracsnow2_mean_month_12 -    WorldClim     1.000000  \n",
       "         fracsnow2_std_month_12  -    WorldClim     0.000000  \n",
       "\n",
       "[252 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data storage\n",
    "l_comids_geo = gdf['COMID'].to_list() # Store the polygon IDs into a list, we'll later use this as columns in the attribute dataframes\n",
    "l_values_geo = [[] for _ in range(len(l_comids_geo))] # Initialize an empty list where we'll store this basin's attributes - nested lists for each subbasin\n",
    "l_index_geo = [] # Initialize an empty list where we'll store the attribute descriptions - this will be our dataframe index\n",
    "\n",
    "dataset = 'worldclim'\n",
    "if dataset == 'worldclim':\n",
    "    csa.oudin_pet_from_worldclim(geo_folder, dataset) # Get an extra PET estimate to sanity check RDRS outcomes\n",
    "    csa.aridity_and_fraction_snow_from_worldclim(geo_folder, dataset) # Get monthly aridity and fraction snow maps\n",
    "    l_values, l_index = attributes_from_worldclim(geo_folder, dataset, shp, l_values_geo, l_index_geo)\n",
    "\n",
    "# - geotiffs\n",
    "att_df_geo = pd.DataFrame(data = l_values_geo, index = l_comids_geo).transpose()\n",
    "multi_index = pd.MultiIndex.from_tuples(l_index_geo, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "att_df_geo.index = multi_index\n",
    "att_df_geo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128c85e-490d-4f4d-8b10-94d57963c22e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecea1fe4-7a05-424e-aad1-f465ff153103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from rasterstats import zonal_stats\n",
    "from python_cs_functions.attributes import read_scale_and_offset, update_values_list,zonal_stats_unit_conversion,create_annual_worldclim_map\n",
    "from python_cs_functions.attributes import derive_annual_worldclim_aridity,derive_annual_worldclim_seasonality,derive_annual_worldclim_fracsnow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "056df4da-9b8f-4353-87d6-9f66b0ac22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_worldclim(geo_folder, dataset, shp_str, l_values, l_index):\n",
    "\n",
    "    '''Calculates mean and stdv for tifs of monthly WorldClim values'''\n",
    "\n",
    "    # Define file locations\n",
    "    # Units source: https://www.worldclim.org/data/worldclim21.html\n",
    "    clim_folder = geo_folder / dataset / 'raw'\n",
    "    sub_folders =      ['prec', 'srad',   'tavg', 'tmax', 'tmin', 'vapr', 'wind',   'pet', 'aridity2', 'fracsnow2'] # aridity and fractionsnow have subscript 2 to distinguish them from ERA5 attributes\n",
    "    sub_folder_units = ['mm',   'W m^-2', 'C',    'C',    'C',    'kPa',  'm s^-1', 'mm',  '-',       '-']  # srad original: kJ m^-2 d^-1. Converted below\n",
    "\n",
    "    # Get the annual values\n",
    "    l_values,l_index = get_annual_worldclim_attributes(clim_folder, shp_str, 'prec', 'tavg', 'pet', 'snow2', l_values, l_index)\n",
    "\n",
    "    # Loop over the files and calculate the stats\n",
    "    for sub_folder, sub_folder_unit in zip(sub_folders, sub_folder_units):\n",
    "        month_files = sorted( glob.glob(str(clim_folder / sub_folder / '*.tif')) )\n",
    "        for month_file in month_files:\n",
    "            month_file = clim_folder / sub_folder / month_file # Construct the full path, because listdir() gives only files\n",
    "            stats = ['mean', 'std']\n",
    "            zonal_out = zonal_stats(shp_str, month_file, stats=stats)\n",
    "            \n",
    "            scale, offset = read_scale_and_offset(month_file)\n",
    "            if sub_folder == 'srad':\n",
    "                zonal_out = zonal_stats_unit_conversion(zonal_out,stats,'srad', scale, offset)\n",
    "\n",
    "            l_values = update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "            \n",
    "            month = os.path.basename(month_file).split('_')[3].split('.')[0]\n",
    "            var = os.path.basename(month_file).split('_')[2]\n",
    "            source = 'WorldClim'\n",
    "            if var == 'pet': source = 'WorldClim (derived, Oudin et al., 2005)'\n",
    "            l_index += [('Climate', f'{var}_mean_month_{month}', f'{sub_folder_unit}',  source),\n",
    "                        ('Climate', f'{var}_std_month_{month}', f'{sub_folder_unit}', source)]\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6c16e84-0495-4131-9e1d-aa358f9475ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annual_worldclim_attributes(clim_folder, shp_str, prec_folder, tavg_folder, pet_folder, snow_folder, l_values, l_index):\n",
    "\n",
    "    '''Calculates annual WorldClim statistics'''\n",
    "\n",
    "    # Create the output folder\n",
    "    ann_folder = clim_folder / 'annual'\n",
    "    ann_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # General settings\n",
    "    stats = ['mean', 'std']\n",
    "\n",
    "    # We need some special actions if we're dealing with the distributed case, so check that first\n",
    "    case = 'lumped'\n",
    "    if 'distributed' in shp_str:\n",
    "        case = 'distributed'\n",
    "        num_nested_lists = len(l_values)\n",
    "    \n",
    "    # --- P\n",
    "    prec_files = sorted( glob.glob(str(clim_folder / prec_folder / '*.tif')) )\n",
    "    annual_prec = create_annual_worldclim_map(prec_files, ann_folder, 'prec_sum.tif', 'prec')\n",
    "    zonal_out = zonal_stats(shp_str, annual_prec, stats=stats)\n",
    "\n",
    "    # Handle scale and offset\n",
    "    scale,offset = read_scale_and_offset(annual_prec)\n",
    "    if scale is None: scale = 1 # If scale is undefined that means we simply multiply by 1\n",
    "    if offset is None: offset = 0 # Undefined offset > add 0\n",
    "\n",
    "    # Update lists\n",
    "    for stat in stats:\n",
    "        if case == 'lumped':\n",
    "            l_values.append(zonal_out[0][stat] * scale + offset)\n",
    "        elif case == 'distributed':\n",
    "            for i in range(0,num_nested_lists):\n",
    "                l_values[i].append(zonal_out[i][stat] * scale + offset)\n",
    "        l_index.append(('Climate',f'prec_{stat}','mm', 'WorldClim'))\n",
    "\n",
    "    # --- PET\n",
    "    pet_files = sorted( glob.glob(str(clim_folder / pet_folder / '*.tif')) )\n",
    "    annual_pet = create_annual_worldclim_map(pet_files, ann_folder, 'pet_sum.tif', 'pet')\n",
    "    zonal_out = zonal_stats(shp_str, annual_pet, stats=stats)\n",
    "    \n",
    "    scale,offset = read_scale_and_offset(annual_pet)\n",
    "    if scale is None: scale = 1\n",
    "    if offset is None: offset = 0\n",
    "        \n",
    "    for stat in stats:\n",
    "        if case == 'lumped':\n",
    "            l_values.append(zonal_out[0][stat] * scale + offset)\n",
    "        elif case == 'distributed':\n",
    "            for i in range(0,num_nested_lists):\n",
    "                l_values[i].append(zonal_out[i][stat] * scale + offset)\n",
    "        l_index.append(('Climate',f'pet_{stat}','mm', 'WorldClim'))\n",
    "        \n",
    "    # --- T\n",
    "    tavg_files = sorted( glob.glob(str(clim_folder / tavg_folder / '*.tif')) )\n",
    "    annual_tavg = create_annual_worldclim_map(tavg_files, ann_folder, 't_avg.tif', 'tavg')\n",
    "    zonal_out = zonal_stats(shp_str, annual_tavg, stats=stats)\n",
    "    \n",
    "    scale,offset = read_scale_and_offset(annual_tavg)\n",
    "    if scale is None: scale = 1\n",
    "    if offset is None: offset = 0\n",
    "        \n",
    "    for stat in stats:\n",
    "        if case == 'lumped':\n",
    "            l_values.append(zonal_out[0][stat] * scale + offset)\n",
    "        elif case == 'distributed':\n",
    "            for i in range(0,num_nested_lists):\n",
    "                l_values[i].append(zonal_out[i][stat] * scale + offset)\n",
    "        l_index.append(('Climate',f'tavg_{stat}','C', 'WorldClim'))\n",
    "\n",
    "    # --- Snow\n",
    "    snow_files = sorted( glob.glob(str(clim_folder / snow_folder / '*.tif')) )\n",
    "    annual_snow = create_annual_worldclim_map(snow_files, ann_folder, 'snow_sum.tif', 'snow')\n",
    "\n",
    "    # --- Aridity\n",
    "    annual_ari = derive_annual_worldclim_aridity(annual_prec, annual_pet, ann_folder, 'aridity.tif')\n",
    "    zonal_out = zonal_stats(shp_str, annual_ari, stats=stats)\n",
    "    \n",
    "    scale,offset = read_scale_and_offset(annual_ari)\n",
    "    if scale is None: scale = 1\n",
    "    if offset is None: offset = 0\n",
    "        \n",
    "    for stat in stats:\n",
    "        if case == 'lumped':\n",
    "            l_values.append(zonal_out[0][stat] * scale + offset)\n",
    "        elif case == 'distributed':\n",
    "            for i in range(0,num_nested_lists):\n",
    "                l_values[i].append(zonal_out[i][stat] * scale + offset)\n",
    "        l_index.append(('Climate',f'aridity2_{stat}','-', 'WorldClim'))\n",
    "\n",
    "    # --- Seasonality\n",
    "    annual_seas = derive_annual_worldclim_seasonality(prec_files, tavg_files, ann_folder, 'seasonality.tif')\n",
    "    zonal_out = zonal_stats(shp_str, annual_seas, stats=stats)\n",
    "    \n",
    "    scale,offset = read_scale_and_offset(annual_seas)\n",
    "    if scale is None: scale = 1\n",
    "    if offset is None: offset = 0\n",
    "        \n",
    "    for stat in stats:\n",
    "        if case == 'lumped':\n",
    "            l_values.append(zonal_out[0][stat] * scale + offset)\n",
    "        elif case == 'distributed':\n",
    "            for i in range(0,num_nested_lists):\n",
    "                l_values[i].append(zonal_out[i][stat] * scale + offset)\n",
    "        l_index.append(('Climate',f'seasonality2_{stat}','-', 'WorldClim'))\n",
    "    \n",
    "    # --- Snow fraction\n",
    "    annual_fs = derive_annual_worldclim_fracsnow(annual_prec, annual_snow, ann_folder, 'fracsnow.tif')\n",
    "    zonal_out = zonal_stats(shp_str, annual_fs, stats=stats)\n",
    "    \n",
    "    scale,offset = read_scale_and_offset(annual_fs)\n",
    "    if scale is None: scale = 1\n",
    "    if offset is None: offset = 0\n",
    "        \n",
    "    for stat in stats:\n",
    "        if case == 'lumped':\n",
    "            l_values.append(zonal_out[0][stat] * scale + offset)\n",
    "        elif case == 'distributed':\n",
    "            for i in range(0,num_nested_lists):\n",
    "                l_values[i].append(zonal_out[i][stat] * scale + offset)\n",
    "        l_index.append(('Climate',f'fracsnow2_{stat}','-', 'WorldClim'))\n",
    "    \n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb82fc-393e-4391-8a8d-48d90fb888f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eeaf96c0-f22a-4798-b701-de500fc0525f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Fix HydroLAKES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08011357-0dae-489c-8e81-694cb3662d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_values_lakes = [[] for _ in range(len(l_comids_geo))]\n",
    "l_index_lakes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21cf892c-250e-4151-ba88-ece94cd67952",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'hydrolakes'\n",
    "l_values_lakes, l_index_lakes, l_comids_lakes = attributes_from_hydrolakes(geo_folder, dataset, shp, ea_crs, l_values_lakes, l_index_lakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd81c024-06b3-49f6-8476-71c05246eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the hydrolakes attribute dataframe\n",
    "att_df_lakes = pd.DataFrame(data = l_values_lakes, index = l_comids_lakes).transpose()\n",
    "multi_index = pd.MultiIndex.from_tuples(l_index_lakes, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "att_df_lakes.index = multi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "900fabdf-ed6a-43d9-aa73-72317001a2e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def subset_hydrolakes_to_subbasin(lakes, subbasin, ea_crs):\n",
    "\n",
    "    '''lakes: GeoDataframe | subbasin: geometry | ea_crs: string'''\n",
    "\n",
    "    # Clip the lakes polygon to the subbasin\n",
    "    poly_lakes = gpd.clip(lakes,subbasin)\n",
    "\n",
    "    # If we are left with any lake polygons:\n",
    "    if len(poly_lakes) > 0 :\n",
    "    # Update the Lake_area [km2] and Vol_total [million~m^3] values\n",
    "    # This is needed in cases where due to clipping the lake polygon we end up with a partial lake in this subbasin\n",
    "    \n",
    "        # Old values\n",
    "        old_areas = poly_lakes['Lake_area'] # km2\n",
    "        old_volumes = poly_lakes['Vol_total'] # million m3\n",
    "        \n",
    "        # Get new area\n",
    "        new_areas = poly_lakes.to_crs(ea_crs).area / 10**6 # [m2] / 10^6 = [km2]\n",
    "        new_areas_rounded = round(new_areas,2) # this matches the number of significant digits in the test case (CAN_01DJ005)\n",
    "        \n",
    "        # Scale volume by new area - this is a bit simplistic but we have no better way to estimate the volume\n",
    "        new_volumes = old_volumes * (new_areas_rounded / old_areas)\n",
    "        \n",
    "        # Replace values\n",
    "        poly_lakes['Lake_area'] = new_areas_rounded\n",
    "        poly_lakes['Vol_total'] = new_volumes\n",
    "\n",
    "    return poly_lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6e424b7-0f6b-4241-98ca-b9fa3cc1f07e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def attributes_from_hydrolakes(geo_folder, dataset, basin_shp_path, ea_crs, l_values, l_index):\n",
    "    \n",
    "    '''Calculates open water attributes from HydroLAKES'''\n",
    "\n",
    "    # We need some special actions if we're dealing with the distributed case, so check that first\n",
    "    case = 'lumped'\n",
    "    if 'distributed' in basin_shp_path:\n",
    "        case = 'distributed'\n",
    "    \n",
    "    # Define the standard file name\n",
    "    lake_str = str(geo_folder / dataset / 'raw' / 'HydroLAKES_polys_v10_NorthAmerica.shp')\n",
    "\n",
    "    # Now handle the different cases\n",
    "    if case == 'lumped':\n",
    "\n",
    "        # Check if the file exists (some basins won't have lakes)\n",
    "        if os.path.exists(lake_str):\n",
    "            lakes = gpd.read_file(lake_str)\n",
    "            res_mask  = lakes['Lake_type'] == 2 # Lake Type 2 == reservoir; see docs (https://data.hydrosheds.org/file/technical-documentation/HydroLAKES_TechDoc_v10.pdf)\n",
    "            num_lakes = len(lakes)\n",
    "            num_resvr = res_mask.sum()\n",
    "\n",
    "        else: # no lakes shapefile\n",
    "            lakes = None # this tells get_open_water_stats() that we had no lake shapefile\n",
    "            num_lakes = 0\n",
    "            num_resvr = 0\n",
    "\n",
    "        l_values.append(num_lakes)\n",
    "        l_index.append(('Open water', 'open_water_number',  '-', 'HydroLAKES'))\n",
    "        l_values.append(num_resvr)\n",
    "        l_index.append(('Open water', 'known_reservoirs',  '-', 'HydroLAKES'))\n",
    "\n",
    "        # Summary stats\n",
    "        l_values, l_index = get_open_water_stats(lakes, 'Lake_area', 'all', l_values, l_index) # All open water\n",
    "        l_values, l_index = get_open_water_stats(lakes, 'Vol_total', 'all', l_values, l_index)\n",
    "        l_values, l_index = get_open_water_stats(lakes, 'Lake_area', 'reservoir', l_values, l_index) # Reservoirs only\n",
    "        l_values, l_index = get_open_water_stats(lakes, 'Vol_total', 'reservoir', l_values, l_index)\n",
    "\n",
    "        # set the remaing output\n",
    "        l_comids_lakes = None\n",
    "\n",
    "    elif case == 'distributed':\n",
    "\n",
    "        # Load the basin shape \n",
    "        basin = gpd.read_file(basin_shp_path)\n",
    "        \n",
    "        # Load the lake shape if we have one\n",
    "        if os.path.exists(lake_str):\n",
    "            lakes = gpd.read_file(lake_str)\n",
    "        else:\n",
    "            lakes = None # we use this to indicate we had no lakes at all\n",
    "            \n",
    "        # Loop over the individual polygons and create a new mini-HydroLAKES geodataframe for each polygon\n",
    "        num_poly = len(basin)\n",
    "        l_comids_lakes = basin['COMID'].values\n",
    "        for i_poly in range(num_poly):\n",
    "\n",
    "            # Rest the storage lists\n",
    "            tmp_values = []\n",
    "            tmp_index = []\n",
    "\n",
    "            # subset the 'lakes' gdf to each individual polygon if we have 'lakes'           \n",
    "            if lakes is not None:\n",
    "                poly_lakes = subset_hydrolakes_to_subbasin(lakes, basin.iloc[i_poly]['geometry'], ea_crs)\n",
    "\n",
    "                # Now see if we have a lake at all, and act accordingly\n",
    "                if len(poly_lakes) > 0:\n",
    "                    res_mask  = lakes['Lake_type'] == 2 # Lake Type 2 == reservoir; see docs (https://data.hydrosheds.org/file/technical-documentation/HydroLAKES_TechDoc_v10.pdf)\n",
    "                    num_lakes = len(lakes)\n",
    "                    num_resvr = res_mask.sum()\n",
    "        \n",
    "                else: # no lakes in this subbasins\n",
    "                    poly_lakes = None # this tells get_open_water_stats() that we had no lake shapefile\n",
    "                    num_lakes = 0\n",
    "                    num_resvr = 0\n",
    "            elif lakes is None: # this means we didn't have a lakes shapefile for this basin at all\n",
    "                poly_lakes = None # this tells get_open_water_stats() we had nothing\n",
    "                num_lakes = 0\n",
    "                num_resvr = 0\n",
    "\n",
    "            # Stats\n",
    "            tmp_values.append(num_lakes)\n",
    "            tmp_index.append(('Open water', 'open_water_number',  '-', 'HydroLAKES'))\n",
    "            tmp_values.append(num_resvr)\n",
    "            tmp_index.append(('Open water', 'known_reservoirs',  '-', 'HydroLAKES'))\n",
    "            tmp_values, tmp_index = get_open_water_stats(poly_lakes, 'Lake_area', 'all', tmp_values, tmp_index) # All open water\n",
    "            tmp_values, tmp_index = get_open_water_stats(poly_lakes, 'Vol_total', 'all', tmp_values, tmp_index)\n",
    "            tmp_values, tmp_index = get_open_water_stats(poly_lakes, 'Lake_area', 'reservoir', tmp_values, tmp_index) # Reservoirs only\n",
    "            tmp_values, tmp_index = get_open_water_stats(poly_lakes, 'Vol_total', 'reservoir', tmp_values, tmp_index)\n",
    "\n",
    "            # Add the new values for this subbasin into the main l_values list\n",
    "            l_values[i_poly] = tmp_values # This works because we create a unique dataframe just for HydroLAKES results\n",
    "\n",
    "        # End of subbasin loop, add the new index entries only once\n",
    "        l_index = tmp_index # we don't use append() in the distributed case because we'll be storing this in a dedicated HydroLAKES attribute df\n",
    "\n",
    "    return l_values, l_index, l_comids_lakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb6dfa0-a8e6-44ed-a92a-2171307a208f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Fix the GLHYMPS case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61af2f04-1a39-4cb7-b2b8-e9f9e37b3f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_values_glhymps = [[] for _ in range(len(l_comids_geo))]\n",
    "l_index_glhymps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d3b0f14-14ea-45b5-a17e-b88e325edf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'glhymps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55d7c507-b18e-4dea-b12f-0dff36acbe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/custom/software/2020/avx2/MPI/gcc9/openmpi4/geo-stack/2022a/lib/python3.10/site-packages/geopandas/io/file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "l_values_glhymps, l_index_glhymps, l_comids_glhymps = attributes_from_glhymps(geo_folder, dataset, shp, l_values_glhymps, l_index_glhymps, equal_area_crs=ea_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8d50f3c-49ed-4ff0-a565-8387c85a30c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the GLHYMPS attribute dataframe\n",
    "att_df_glhymps = pd.DataFrame(data = l_values_glhymps, index = l_comids_glhymps).transpose()\n",
    "multi_index = pd.MultiIndex.from_tuples(l_index_glhymps, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "att_df_glhymps.index = multi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31b3c9f4-21f8-431a-ab27-13d0b0adac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_glhymps_to_subbasin(glhymps, subbasin, ea_crs):\n",
    "\n",
    "    poly_glhymps = gpd.clip(glhymps,subbasin)\n",
    "    poly_glhymps['New_area_m2'] = poly_glhymps.to_crs(ea_crs).area\n",
    "\n",
    "    return poly_glhymps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8775aa1-a097-40ab-a450-268a072d05a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_glhymps(geo_folder, dataset, basin_shp_path, l_values, l_index, equal_area_crs='ESRI:102008'):\n",
    "\n",
    "    '''Calculates attributes from GLHYMPS'''\n",
    "\n",
    "    # We need some special actions if we're dealing with the distributed case, so check that first\n",
    "    case = 'lumped'\n",
    "    if 'distributed' in basin_shp_path:\n",
    "        case = 'distributed'\n",
    "    \n",
    "    # Load the geology file\n",
    "    geol_str = str(geo_folder / dataset / 'raw' / 'glhymps.shp')\n",
    "    geol = gpd.read_file(geol_str)\n",
    "\n",
    "    # -- File updates\n",
    "    # Rename the columns, because the shortened ones are not very helpful\n",
    "    if ('Porosity' in geol.columns) and ('Permeabili' in geol.columns) \\\n",
    "    and ('Permeabi_1' in geol.columns) and ('Permeabi_2' in geol.columns):\n",
    "        geol.rename(columns={'Porosity': 'porosity',   'Permeabili': 'logK_Ferr',\n",
    "                             'Permeabi_1': 'logK_Ice', 'Permeabi_2': 'logK_std'}, inplace=True)\n",
    "        geol.to_file(geol_str)\n",
    "\n",
    "    # Ensure we have the correct areas to work with\n",
    "    if 'New_area_m2' not in geol.columns:\n",
    "        geol['New_area_m2'] = geol.to_crs(equal_area_crs).area\n",
    "        #geol.to_file(geol_str)\n",
    "\n",
    "    # Clean up a few unsightly processing errors\n",
    "    geol['Shape_Area'] = geol.to_crs(equal_area_crs).area\n",
    "    geol = geol[['IDENTITY_','Shape_Area','porosity','logK_Ferr','logK_Ice','logK_std','geometry']]\n",
    "    geol.to_file(geol_str)\n",
    "    # -- End file updates\n",
    "\n",
    "    # Now handle the different cases\n",
    "    if case == 'lumped':\n",
    "    \n",
    "        # Create areal averages and standard deviations\n",
    "        # Stdev source: https://stats.stackexchange.com/a/6536\n",
    "        porosity_mean = (geol['porosity']*geol['New_area_m2']).sum()/geol['New_area_m2'].sum()\n",
    "        num_obs_coef = ((geol['New_area_m2'] > 0).sum()-1)/(geol['New_area_m2'] > 0).sum()\n",
    "        porosity_std = np.sqrt((geol['New_area_m2'] * (geol['porosity']-porosity_mean)**2).sum() / geol['New_area_m2'].sum())\n",
    "        \n",
    "        # Porosity\n",
    "        l_values.append( geol['porosity'].min() )\n",
    "        l_index.append(('Geology', 'porosity_min',  '-', 'GLHYMPS'))\n",
    "        l_values.append( porosity_mean ) # areal average\n",
    "        l_index.append(('Geology', 'porosity_mean',  '-', 'GLHYMPS'))\n",
    "        l_values.append( geol['porosity'].max() )\n",
    "        l_index.append(('Geology', 'porosity_max',  '-', 'GLHYMPS'))\n",
    "        l_values.append( porosity_std )\n",
    "        l_index.append(('Geology', 'porosity_std',  '-', 'GLHYMPS'))\n",
    "    \n",
    "        # Create areal averages and standard deviations\n",
    "        # Stdev source: https://stats.stackexchange.com/a/6536\n",
    "        permeability_mean = (geol['logK_Ice']*geol['New_area_m2']).sum()/geol['New_area_m2'].sum()\n",
    "        num_obs_coef = ((geol['New_area_m2'] > 0).sum()-1)/(geol['New_area_m2'] > 0).sum()\n",
    "        permeability_std = np.sqrt((geol['New_area_m2'] * (geol['logK_Ice']-permeability_mean)**2).sum() / geol['New_area_m2'].sum())\n",
    "        \n",
    "        # Permeability\n",
    "        l_values.append( geol['logK_Ice'].min() )\n",
    "        l_index.append(('Geology', 'log_permeability_min',  'm^2', 'GLHYMPS'))\n",
    "        l_values.append( permeability_mean )\n",
    "        l_index.append(('Geology', 'log_permeability_mean',  'm^2', 'GLHYMPS'))\n",
    "        l_values.append( geol['logK_Ice'].max() )\n",
    "        l_index.append(('Geology', 'log_permeability_max',  'm^2', 'GLHYMPS'))\n",
    "        l_values.append( permeability_std )\n",
    "        l_index.append(('Geology', 'log_permeability_std',  'm^2', 'GLHYMPS'))\n",
    "\n",
    "        # Set the remaining output\n",
    "        l_comids_glhymps = None\n",
    "\n",
    "    # Case 2\n",
    "    elif case == 'distributed':\n",
    "    \n",
    "        # Load the basin shape \n",
    "        basin = gpd.read_file(basin_shp_path)\n",
    "\n",
    "        # Loop over the individual polygons and create a new mini-HydroLAKES geodataframe for each polygon\n",
    "        num_poly = len(basin)\n",
    "        l_comids_glhymps = basin['COMID'].values\n",
    "        for i_poly in range(num_poly):\n",
    "\n",
    "            # Rest the storage lists\n",
    "            tmp_values = []\n",
    "            tmp_index = []\n",
    "\n",
    "            # Subset the shape to the subbasin\n",
    "            poly_glhymps = subset_glhymps_to_subbasin(geol, basin.iloc[i_poly]['geometry'], equal_area_crs)\n",
    "\n",
    "            # Create areal averages and standard deviations\n",
    "            # Stdev source: https://stats.stackexchange.com/a/6536\n",
    "            porosity_mean = (poly_glhymps['porosity']*poly_glhymps['New_area_m2']).sum()/poly_glhymps['New_area_m2'].sum()\n",
    "            num_obs_coef = ((poly_glhymps['New_area_m2'] > 0).sum()-1)/(poly_glhymps['New_area_m2'] > 0).sum()\n",
    "            porosity_std = np.sqrt((poly_glhymps['New_area_m2'] * (poly_glhymps['porosity']-porosity_mean)**2).sum() / poly_glhymps['New_area_m2'].sum())\n",
    "            \n",
    "            # Porosity\n",
    "            tmp_values.append( poly_glhymps['porosity'].min() )\n",
    "            tmp_index.append(('Geology', 'porosity_min',  '-', 'GLHYMPS'))\n",
    "            tmp_values.append( porosity_mean ) # areal average\n",
    "            tmp_index.append(('Geology', 'porosity_mean',  '-', 'GLHYMPS'))\n",
    "            tmp_values.append( poly_glhymps['porosity'].max() )\n",
    "            tmp_index.append(('Geology', 'porosity_max',  '-', 'GLHYMPS'))\n",
    "            tmp_values.append( porosity_std )\n",
    "            tmp_index.append(('Geology', 'porosity_std',  '-', 'GLHYMPS'))\n",
    "        \n",
    "            # Create areal averages and standard deviations\n",
    "            # Stdev source: https://stats.stackexchange.com/a/6536\n",
    "            permeability_mean = (poly_glhymps['logK_Ice']*poly_glhymps['New_area_m2']).sum()/poly_glhymps['New_area_m2'].sum()\n",
    "            num_obs_coef = ((poly_glhymps['New_area_m2'] > 0).sum()-1)/(poly_glhymps['New_area_m2'] > 0).sum()\n",
    "            permeability_std = np.sqrt((poly_glhymps['New_area_m2'] * (poly_glhymps['logK_Ice']-permeability_mean)**2).sum() / poly_glhymps['New_area_m2'].sum())\n",
    "            \n",
    "            # Permeability\n",
    "            tmp_values.append( poly_glhymps['logK_Ice'].min() )\n",
    "            tmp_index.append(('Geology', 'log_permeability_min',  'm^2', 'GLHYMPS'))\n",
    "            tmp_values.append( permeability_mean )\n",
    "            tmp_index.append(('Geology', 'log_permeability_mean',  'm^2', 'GLHYMPS'))\n",
    "            tmp_values.append( poly_glhymps['logK_Ice'].max() )\n",
    "            tmp_index.append(('Geology', 'log_permeability_max',  'm^2', 'GLHYMPS'))\n",
    "            tmp_values.append( permeability_std )\n",
    "            tmp_index.append(('Geology', 'log_permeability_std',  'm^2', 'GLHYMPS'))\n",
    "            \n",
    "\n",
    "            # Add the new values for this subbasin into the main l_values list\n",
    "            l_values[i_poly] = tmp_values # This works because we create a unique dataframe just for HydroLAKES results\n",
    "\n",
    "        # End of subbasin loop, add the new index entries only once\n",
    "        l_index = tmp_index # we don't use append() in the distributed case because we'll be storing this in a dedicated HydroLAKES attribute df\n",
    "    \n",
    "    return l_values, l_index, l_comids_glhymps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ea62f4-3caa-42ed-a4af-0edb5528581e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Fix the RDRS distributed case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb29b4c7-b702-4906-bb36-836edd446bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_path = shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4011030-b36b-409d-ad3b-0463414b9ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running distributed case for RDRS data.\n",
      "Running subbasin 0\n",
      "Running subbasin 1\n",
      "Running subbasin 2\n"
     ]
    }
   ],
   "source": [
    "# trial the function\n",
    "l_values_met = []\n",
    "l_index_met = []\n",
    "l_values_met, l_index_met, l_comids_met, _ = attributes_from_rdrs(met_folder, shp_path, 'RDRS', l_values_met, l_index_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05e30407-8081-4abd-a401-de9dd6d8a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the forcing attributes dataframe\n",
    "att_df_met = pd.DataFrame(data = l_values_met, index = l_comids_met).transpose()\n",
    "multi_index = pd.MultiIndex.from_tuples(l_index_met, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "att_df_met.index = multi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27bc64d6-4c2e-42b7-aeb9-3de37bca15dc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_temp_prcp_stats(var, condition, hilo, l_values,l_index,\n",
    "                              dataset='ERA5', units='hours'):\n",
    "    \n",
    "    '''Calculates frequency (mean) and duration (mean, median, skew, kurtosis) \n",
    "        of temperature/precipitation periods'''\n",
    "\n",
    "    # Constants. We want everything in [days] for consistency with original CAMELS\n",
    "    hours_per_day = 24 # [hours day-1]\n",
    "    days_per_year = 365.25 # [days year-1]\n",
    "\n",
    "    # Calculate frequencies\n",
    "    freq = condition.mean(dim='time') * days_per_year # [-] * [days year-1]\n",
    "    l_values.append(freq.values)\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_freq', 'days year^-1', dataset) )\n",
    "    \n",
    "    # Calculate duration statistics\n",
    "    durations = find_durations(condition) # [time steps]\n",
    "    if units == 'hours':\n",
    "        durations = durations / hours_per_day # [days] = [hours] / [hours day-1]\n",
    "    l_values.append(np.mean(durations)) # [days]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_mean', 'days', dataset) ) # Consistency with\n",
    "    l_values.append(np.median(durations)) # [days]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_median', 'days', dataset) )\n",
    "    l_values.append(skew(durations)) # [-]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_skew', '-', dataset) )\n",
    "    l_values.append(kurtosis(durations)) # [-]\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_dur_kurtosis', '-', dataset) )\n",
    "\n",
    "    # Calculate timing statistic\n",
    "    condition['season'] = ('time', [get_season(month) for month in condition['time.month'].values]) # add seasons\n",
    "    season_groups = condition.groupby('season')\n",
    "    season_list   = list(season_groups.groups.keys())\n",
    "    max_season_id = int(season_groups.sum().argmax(dim='season').values) # find season with most True values\n",
    "    l_values.append(season_list[max_season_id]) # add season abbrev\n",
    "    l_index.append( ('Climate', f'{hilo}_{var}_timing', 'season', dataset) )\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8761ab02-a63a-471e-8ebb-d9ca4e840082",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def attributes_from_rdrs(met_folder, shp_path, dataset, l_values, l_index, use_mfdataset=False):\n",
    "\n",
    "    '''Calculates a variety of metrics from RDRS data'''\n",
    "\n",
    "    # Define file locations, depending on if we are dealing with lumped or distributed cases\n",
    "    if 'lumped' in shp_path:\n",
    "        rdrs_folder = met_folder / 'lumped'\n",
    "        case = 'lumped'\n",
    "    elif 'distributed' in shp_path:\n",
    "        rdrs_folder = met_folder / 'distributed'\n",
    "        case = 'distributed'\n",
    "    rdrs_files = sorted( glob.glob( str(rdrs_folder / 'RDRS_*.nc') ) )\n",
    "    print(f'Running {case} case for RDRS data.')\n",
    "\n",
    "    # Open the data\n",
    "    if use_mfdataset or (case == 'distributed'):\n",
    "        ds = xr.open_mfdataset(rdrs_files, combine=\"by_coords\") # Don't use 'decode_cf=False' > this somehow loses most of the timesteps\n",
    "    else:\n",
    "        ds = xr.merge([xr.open_dataset(f) for f in rdrs_files])\n",
    "        ds = ds.load()\n",
    "        ds = ds.isel(hru=0) # We need this so the lumped and distributed cases have the same dimensions inside the calculation function: (time,nbnds)\n",
    "                            # Without this, the lumped case has an extra 'hru' dimension (length 1) and this complicates value extraction\n",
    "\n",
    "    # --- Act according to case\n",
    "    if case == 'lumped':\n",
    "        # -- Get the precipitation for hydrologic signature calculations later; this avoids having to reload the entire dataset another time\n",
    "        ds_precip = ds['RDRS_v2.1_A_PR0_SFC'].copy()\n",
    "\n",
    "        # --- Calculate the statistics\n",
    "        l_values, l_index = calculate_rdrs_stats_from_ds(ds,l_values,l_index)\n",
    "        return l_values, l_index, ds_precip, ds\n",
    "    \n",
    "    elif case == 'distributed':\n",
    "\n",
    "        comid_order = [] # we need these later, to ensure we line the forcing attributes up with the geospatial attributes correctly\n",
    "\n",
    "        # Loop over the HRUs\n",
    "        num_hru = len(ds['hru'])\n",
    "        for i in range(0,num_hru):\n",
    "\n",
    "            print(f'Running subbasin {i}')\n",
    "\n",
    "            # Load the data for this sub-basin\n",
    "            ds_hru = ds.isel(hru=i) # example\n",
    "            ds_hru.load() # Load data into memory; done in place\n",
    "\n",
    "            # Specifically track the hruID (COMID) so we can ensure correct matches with the rest of the attributes later\n",
    "            assert (ds_hru['hruId'].values == ds_hru['hruId'][0].values).all(), f\"COMIDs not all identical {ds_hru['hruId'].values}\"\n",
    "            comid_order.append(ds_hru['hruId'][0].values)\n",
    "\n",
    "            # calculate the stats\n",
    "            tmp_values = [] # statistics for this subbasin - we need a single empty list so we can properly store in the nested list later\n",
    "            tmp_index  = [] # we also store the index in a tmp variable, so we only get this once instead of appending num_hru times to the main index list\n",
    "            tmp_values, tmp_index = calculate_rdrs_stats_from_ds(ds_hru,tmp_values,tmp_index) \n",
    "            l_values.append(tmp_values)\n",
    "\n",
    "        # prep outputs\n",
    "        comid_order = [arr.tolist() for arr in comid_order] # convert list of arrays to simple list\n",
    "        l_index.append(tmp_index) # need this only once; tmp_index is already a list so this creates a nested list: [['Climate', 'num_years_rdrs', 'years', 'RDRS')]]\n",
    "        return l_values, l_index[0], comid_order, ds # we need l_index[0] to return a non-nested list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "048908bc-cdeb-4b82-ab65-4e59b874c773",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_rdrs_stats_from_ds(ds,l_values,l_index):\n",
    "\n",
    "    # Define various conversion constants\n",
    "    water_density = 1000 # kg m-3\n",
    "    mm_per_m = 1000 # mm m-1\n",
    "    seconds_per_hour = 60*60 # s h-1\n",
    "    seconds_per_day = seconds_per_hour*24 # s d-1\n",
    "    days_per_month = np.array([31, 28.25, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]).reshape(-1, 1).flatten() # d month-1 || NOTE: added the .flatten() for distributed case, not tested for lumped\n",
    "    days_per_year = days_per_month.sum()\n",
    "    flip_sign = -1 # -; used to convert PET from negative (by convention this indicates an upward flux) to positive (ERA5 only)\n",
    "    kelvin_to_celsius = -273.15\n",
    "    pa_per_kpa = 1000 # Pa kPa-1\n",
    "    \n",
    "    # Select whole years only\n",
    "    #   This avoids issues in cases where we have incomplete whole data years\n",
    "    #   (e.g. 2000-06-01 to 2007-12-31) in basins with very seasonal weather\n",
    "    #   (e.g. all precip occurs in Jan, Feb, Mar). By using only full years\n",
    "    #   we avoid accidentally biasing the attributes.\n",
    "    ds = subset_dataset_to_max_full_years(ds)\n",
    "    num_years = len(ds.groupby('time.year'))\n",
    "    l_values.append(num_years)\n",
    "    l_index.append( ('Climate', 'num_years_rdrs', 'years', 'RDRS') )\n",
    "\n",
    "    # --- Annual statistics (P, PET, T, aridity, seasonality, temperature, snow)\n",
    "    # P\n",
    "    yearly_pr0 = ds['RDRS_v2.1_A_PR0_SFC'].resample(time='1Y').mean() * seconds_per_day * days_per_year * mm_per_m / water_density # kg m-2 s-1 > mm yr-1\n",
    "    l_values.append(yearly_pr0.mean().values)\n",
    "    l_index.append(('Climate', 'PR0_SFC_mean', 'mm', 'RDRS'))\n",
    "    l_values.append(yearly_pr0.std().values)\n",
    "    l_index.append(('Climate', 'PR0_SFC_std', 'mm', 'RDRS'))\n",
    "\n",
    "    # PET\n",
    "    yearly_pet = ds['pet'].resample(time='1Y').mean() * seconds_per_day * days_per_year * mm_per_m / water_density # kg m-2 s-1 > mm yr-1\n",
    "    l_values.append(yearly_pet.mean().values)\n",
    "    l_index.append(('Climate', 'pet_mean', 'mm', 'RDRS'))\n",
    "    l_values.append(yearly_pet.std().values)\n",
    "    l_index.append(('Climate', 'pet_std', 'mm', 'RDRS'))\n",
    "\n",
    "    # T\n",
    "    yearly_tt = ds['RDRS_v2.1_P_TT_1.5m'].resample(time='1Y').mean() + kelvin_to_celsius # K > C\n",
    "    l_values.append(yearly_tt.mean().values)\n",
    "    l_index.append(('Climate', 'TT_mean', 'C', 'RDRS'))\n",
    "    l_values.append(yearly_tt.std().values)\n",
    "    l_index.append(('Climate', 'TT_std', 'C', 'RDRS'))\n",
    "\n",
    "    # Aridity\n",
    "    yearly_ari  = yearly_pet / yearly_pr0\n",
    "    l_values.append(yearly_ari.mean().values)\n",
    "    l_index.append(('Climate', 'aridity1_mean', '-', 'RDRS'))\n",
    "    l_values.append(yearly_ari.std().values)\n",
    "    l_index.append(('Climate', 'aridity1_std', '-', 'RDRS'))\n",
    "\n",
    "    # Snow\n",
    "    ds['snow'] = xr.where(ds['RDRS_v2.1_P_TT_1.5m'] < 273.15, ds['RDRS_v2.1_A_PR0_SFC'],0)\n",
    "    yearly_snow = ds['snow'].resample(time='1Y').mean() * seconds_per_day * days_per_year * mm_per_m / water_density\n",
    "    yearly_fs = yearly_snow / yearly_pr0\n",
    "    l_values.append(yearly_fs.mean().values)\n",
    "    l_index.append(('Climate', 'fracsnow1_mean', '-', 'RDRS'))\n",
    "    l_values.append(yearly_fs.std().values)\n",
    "    l_index.append(('Climate', 'fracsnow1_std', '-', 'RDRS'))\n",
    "\n",
    "    # Seasonality\n",
    "    seasonality = find_climate_seasonality_rdrs(ds,use_typical_cycle=False)\n",
    "    l_values.append(seasonality.mean())\n",
    "    l_index.append(('Climate', 'seasonality1_mean', '-', 'RDRS'))\n",
    "    l_values.append(seasonality.std())\n",
    "    l_index.append(('Climate', 'seasonality1_std', '-', 'RDRS'))\n",
    "\n",
    "    # --- Monthly attributes\n",
    "    # Calculate monthly PET in mm\n",
    "    #      kg m-2 s-1 / kg m-3\n",
    "    # mm month-1 = kg m-2 s-1 * kg-1 m3 * s d-1 * d month-1 * mm m-1 * -\n",
    "    monthly_pet = ds['pet'].resample(time='1M').mean().groupby('time.month')\n",
    "    pet_m = monthly_pet.mean() / water_density * seconds_per_day * days_per_month * mm_per_m  # [kg m-2 s-1] to [mm month-1]; negative to indicate upward flux\n",
    "    pet_s = monthly_pet.std() / water_density * seconds_per_day * days_per_month * mm_per_m  # [kg m-2 s-1] to [mm month-1]; negative to indicate upward flux\n",
    "    l_values, l_index = process_monthly_means_to_lists(pet_m, 'mean', l_values, l_index, 'pet', 'mm', source='RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(pet_s, 'std', l_values, l_index, 'pet', 'mm', source='RDRS')\n",
    "\n",
    "    # Same for precipitation: [mm month-1]\n",
    "    monthly_pr0 = ds['RDRS_v2.1_A_PR0_SFC'].resample(time='1M').mean().groupby('time.month')\n",
    "    pr0_m = monthly_pr0.mean() / water_density * seconds_per_day * days_per_month * mm_per_m # [kg m-2 s-1] to [mm month-1]\n",
    "    pr0_s = monthly_pr0.std() / water_density * seconds_per_day * days_per_month * mm_per_m # [kg m-2 s-1] to [mm month-1]\n",
    "    l_values, l_index = process_monthly_means_to_lists(pr0_m, 'mean', l_values, l_index, 'RDRS_v2.1_A_PR0_SFC', 'mm', source='RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(pr0_s, 'std', l_values, l_index, 'RDRS_v2.1_A_PR0_SFC', 'mm', source='RDRS')\n",
    "\n",
    "    # Monthly temperature statistics [C]\n",
    "    monthly_tavg = (ds['RDRS_v2.1_P_TT_1.5m'].resample(time='1D').mean().resample(time='1M').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tavg_m = monthly_tavg.mean()\n",
    "    tavg_s = monthly_tavg.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(tavg_m, 'mean', l_values, l_index, 'tdavg', 'C', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(tavg_s, 'std', l_values, l_index, 'tdavg', 'C', source = 'RDRS')\n",
    "\n",
    "    monthly_tmin = (ds['RDRS_v2.1_P_TT_1.5m'].resample(time='1D').min().resample(time='1M').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tmin_m = monthly_tmin.mean()\n",
    "    tmin_s = monthly_tmin.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(tmin_m, 'mean', l_values, l_index, 'tdmin', 'C', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(tmin_m, 'std', l_values, l_index, 'tdmin', 'C', source = 'RDRS')\n",
    "    \n",
    "    monthly_tmax = (ds['RDRS_v2.1_P_TT_1.5m'].resample(time='1D').max().resample(time='1M').mean() + kelvin_to_celsius).groupby('time.month')\n",
    "    tmax_m = monthly_tmax.mean()\n",
    "    tmax_s = monthly_tmax.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(tmax_m, 'mean', l_values, l_index, 'tdmax', 'C', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(tmax_s, 'std', l_values, l_index, 'tdmax', 'C', source = 'RDRS')\n",
    "\n",
    "    # Monthly shortwave and longwave [W m-2]\n",
    "    monthly_sw = ds['RDRS_v2.1_P_FB_SFC'].resample(time='1M').mean().groupby('time.month')\n",
    "    sw_m = monthly_sw.mean()\n",
    "    sw_s = monthly_sw.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(sw_m, 'mean', l_values, l_index, 'RDRS_v2.1_P_FB_SFC', 'W m^-2', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(sw_s, 'std', l_values, l_index, 'RDRS_v2.1_P_FB_SFC', 'W m^-2', source = 'RDRS')\n",
    "    \n",
    "    monthly_lw = ds['RDRS_v2.1_P_FI_SFC'].resample(time='1M').mean().groupby('time.month')\n",
    "    lw_m = monthly_lw.mean(dim='time')\n",
    "    lw_s = monthly_lw.std(dim='time')\n",
    "    l_values, l_index = process_monthly_means_to_lists(lw_m, 'mean', l_values, l_index, 'RDRS_v2.1_P_FI_SFC', 'W m^-2', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(lw_s, 'std', l_values, l_index, 'RDRS_v2.1_P_FI_SFC', 'W m^-2', source = 'RDRS')\n",
    "\n",
    "    # Surface pressure [Pa]\n",
    "    monthly_sp = ds['RDRS_v2.1_P_P0_SFC'].resample(time='1M').mean().groupby('time.month')\n",
    "    sp_m = monthly_sp.mean() / pa_per_kpa # [Pa] > [kPa]\n",
    "    sp_s = monthly_sp.std() / pa_per_kpa\n",
    "    l_values, l_index = process_monthly_means_to_lists(sp_m, 'mean', l_values, l_index, 'RDRS_v2.1_P_P0_SFC', 'kPa', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(sp_s, 'std', l_values, l_index, 'RDRS_v2.1_P_P0_SFC', 'kPa', source = 'RDRS')\n",
    "\n",
    "    # Humidity [-]\n",
    "    monthly_q = ds['RDRS_v2.1_P_HU_1.5m'].resample(time='1M').mean().groupby('time.month') # specific\n",
    "    q_m = monthly_q.mean()\n",
    "    q_s = monthly_q.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(q_m, 'mean', l_values, l_index, 'RDRS_v2.1_P_HU_1.5m', 'kg kg^-1', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(q_s, 'std', l_values, l_index, 'RDRS_v2.1_P_HU_1.5m', 'kg kg^-1', source = 'RDRS')\n",
    "    \n",
    "    monthly_rh = ds['RDRS_v2.1_P_HR_1.5m'].resample(time='1M').mean().groupby('time.month') # relative\n",
    "    rh_m = monthly_rh.mean()\n",
    "    rh_s = monthly_rh.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(rh_m, 'mean', l_values, l_index, 'RDRS_v2.1_P_HR_1.5m', 'kPa kPa^-1', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(rh_s, 'std', l_values, l_index, 'RDRS_v2.1_P_HR_1.5m', 'kPa kPa^-1', source = 'RDRS')\n",
    "\n",
    "    # Wind speed [m s-1]\n",
    "    monthly_w = ds['RDRS_v2.1_P_UVC_10m'].resample(time='1M').mean().groupby('time.month')\n",
    "    w_m = monthly_w.mean()\n",
    "    w_s = monthly_w.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(w_m, 'mean', l_values, l_index, 'RDRS_v2.1_P_UVC_10m', 'm s^-1', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(w_s, 'std', l_values, l_index, 'RDRS_v2.1_P_UVC_10m', 'm s^-1', source = 'RDRS')\n",
    "\n",
    "    # Wind direction\n",
    "    monthly_phi = ds['phi'].resample(time='1M').apply(circmean_group).groupby('time.month')\n",
    "    phi_m = monthly_phi.apply(circmean_group)\n",
    "    phi_s = monthly_phi.apply(circstd_group)\n",
    "    l_values, l_index = process_monthly_means_to_lists(phi_m, 'mean', l_values, l_index, 'phi', 'degrees', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(phi_s, 'std', l_values, l_index, 'phi', 'degrees', source = 'RDRS')\n",
    "\n",
    "    # aridity\n",
    "    monthly_pet = ds['pet'].resample(time='1M').mean()\n",
    "    monthly_pr0 = ds['RDRS_v2.1_A_PR0_SFC'].resample(time='1M').mean()\n",
    "    if (monthly_pr0 == 0).any():\n",
    "        print(f'--- WARNING: attributes_from_rdrs(): adding 1 mm to monthly precipitation to avoid divide by zero error in aridity calculation')\n",
    "        monthly_pr0[(monthly_pr0 == 0).sel(hru=0)] = 1 / mm_per_m * water_density / (seconds_per_day * days_per_month.mean()) # [mm month-1] / [mm m-1] * [kg m-3] / ([s d-1] * [d month-1]) = [kg m-2 s-1]\n",
    "    monthly_ari = (monthly_pet / monthly_pr0).groupby('time.month')\n",
    "    ari_m = monthly_ari.mean()\n",
    "    ari_s = monthly_ari.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(ari_m, 'mean', l_values, l_index, 'aridity1', '-', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(ari_s, 'std', l_values, l_index, 'aridity1', '-', source = 'RDRS')\n",
    "\n",
    "    # snow\n",
    "    monthly_snow = ds['snow'].resample(time='1M').mean()\n",
    "    monthly_pr0 = ds['RDRS_v2.1_A_PR0_SFC'].resample(time='1M').mean()\n",
    "    if (monthly_pr0 == 0).any():\n",
    "        print(f'--- WARNING: attributes_from_rdrs(): adding 1 mm to monthly precipitation to avoid divide by zero error in snow calculation. Note that by definition this cannot change the fraction snow result (if there is 0 precip, none of it will fall as snow)')\n",
    "        monthly_pr0[(monthly_pr0 == 0).sel(hru=0)] = 1 / mm_per_m * water_density / (seconds_per_day * days_per_month.mean()) # [mm month-1] / [mm m-1] * [kg m-3] / ([s d-1] * [d month-1]) = [kg m-2 s-1]\n",
    "    monthly_snow = (monthly_snow / monthly_pr0).groupby('time.month')\n",
    "    fsnow_m = monthly_snow.mean()\n",
    "    fsnow_s = monthly_snow.std()\n",
    "    l_values, l_index = process_monthly_means_to_lists(fsnow_m, 'mean', l_values, l_index, 'fracsnow1', '-', source = 'RDRS')\n",
    "    l_values, l_index = process_monthly_means_to_lists(fsnow_s, 'std', l_values, l_index, 'fracsnow1', '-', source = 'RDRS') \n",
    "\n",
    "    # --- High-frequency statistics (high/low duration/timing/magnitude)\n",
    "    #  Everyone does precip. We'll add temperature too as a drought/frost indicator\n",
    "    \n",
    "    # -- LOW TEMPERATURE\n",
    "    variable  = 'RDRS_v2.1_P_TT_1.5m'\n",
    "    low_threshold = 273.15 # K, freezing point\n",
    "    low_condition = ds[variable] < low_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('temp',low_condition,'low',l_values,l_index, dataset='RDRS')\n",
    "\n",
    "    # -- HIGH TEMPERATURE\n",
    "    # WMO defines a heat wave as a 5-day or longer period with maximum daily temperatures 5C above \n",
    "    # \"standard\" daily max temperature (1961-1990; source:\n",
    "    # https://www.ifrc.org/sites/default/files/2021-06/10-HEAT-WAVE-HR.pdf).\n",
    "    # We define a \"hot day\" therefore as a day with a maximum temperature 5 degrees over the \n",
    "    # the long-term mean maximum temperature.\n",
    "    #   Note: we don't have 1961-1990 data for some stations, so we stick with long-term mean.\n",
    "    #   Note: this will in most cases slightly underestimate heat waves compared to WMO definition\n",
    "    \n",
    "    # First, we identify the long-term mean daily maximum temperature in a dedicated function\n",
    "    var = 'RDRS_v2.1_P_TT_1.5m'\n",
    "    high_threshold = create_mean_daily_max_series(ds,var=var)\n",
    "    \n",
    "    # Next, we check if which 't' values are 5 degrees above the long-term mean daily max \n",
    "    #  (\"(ds['t'] > result_array + 5)\"), and resample this to a daily time series \n",
    "    #  (\"resample(time='1D')\") filled with \"True\" if any value in that day was True.\n",
    "    daily_flags = (ds[var] > high_threshold + 5).resample(time='1D').any()\n",
    "    \n",
    "    # Finally, we reindex these daily flags back onto the hourly time series by filling values\n",
    "    high_condition = daily_flags.reindex_like(ds[var], method='ffill')\n",
    "    \n",
    "    # Now calculate stats like before\n",
    "    l_values,l_index = calculate_temp_prcp_stats('temp',high_condition,'high',l_values,l_index, dataset='RDRS')\n",
    "\n",
    "    # -- LOW PRECIPITATION\n",
    "    variable = 'RDRS_v2.1_A_PR0_SFC'\n",
    "    # We'll stick with the original CAMELS definition of low precipitation: < 1 mm day-1\n",
    "    # It may not make too much sense to look at \"dry hours\" so we'll do this analysis at daily step\n",
    "    low_threshold = 1 # [mm d-1]\n",
    "    # Create daily precipitation sum (divided by density, times mm m-1 cancels out)\n",
    "    # [kg m-2 s-1] * [s h-1] / [kg m-3] * [mm m-1] = [mm h-1]\n",
    "    low_condition = (ds[variable] * seconds_per_hour).resample(time='1D').sum() < low_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('prec',low_condition,'low',l_values,l_index,\n",
    "                                             units='days', dataset='RDRS') # this 'units' argument prevents conversion to days inside the functiom\n",
    "    \n",
    "    # -- HIGH PRECIPITATION\n",
    "    # CAMELS: > 5 times mean daily precip\n",
    "    high_threshold = 5 * (ds[variable] * seconds_per_hour).resample(time='1D').sum().mean()\n",
    "    high_condition = (ds[variable] * seconds_per_hour).resample(time='1D').sum() >= high_threshold\n",
    "    l_values,l_index = calculate_temp_prcp_stats('prec',high_condition,'high',l_values,l_index,\n",
    "                                                 units='days', dataset='RDRS')\n",
    "\n",
    "    return l_values, l_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005b8823-774c-4276-ad3f-86c73747f2b9",
   "metadata": {},
   "source": [
    "#### Fix the MERIT distributed case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53453f01-66af-4aca-9c99-46a9926755ea",
   "metadata": {},
   "source": [
    "First, check if just the raster part completes OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b014b83-e5a8-4762-bafe-5a9c28a523d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import circmean, circstd\n",
    "from rasterstats import zonal_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b194aa53-027e-4d28-80d0-23f219de1211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attributes_from_merit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2633276/3258638970.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mriv_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mriv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m l_values_geo, l_index_geo,_ = attributes_from_merit(geo_folder, 'merit', \n\u001b[0m\u001b[1;32m      9\u001b[0m                                                   \u001b[0mshp_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mriv_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                   l_values, l_index, equal_area_crs=ea_crs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attributes_from_merit' is not defined"
     ]
    }
   ],
   "source": [
    "# -- lumped case for trial\n",
    "l_values = []\n",
    "l_index = []\n",
    "basin = 'CAN_01DJ005'\n",
    "shp_str = str(shp_lump_path)\n",
    "riv_str = str(riv)\n",
    "\n",
    "l_values_geo, l_index_geo,_ = attributes_from_merit(geo_folder, 'merit', \n",
    "                                                  shp_str, riv_str, row, \n",
    "                                                  l_values, l_index, equal_area_crs=ea_crs)\n",
    "\n",
    "# Test with a fake second station\n",
    "l_gauges = [basin,'dummy']\n",
    "\n",
    "# Make the dataframe\n",
    "input_dict = dict(zip(l_gauges, [l_values,l_values]))\n",
    "df = pd.DataFrame(input_dict)\n",
    "\n",
    "# Set the index\n",
    "multi_index = pd.MultiIndex.from_tuples(l_index, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "df.index = multi_index\n",
    "\n",
    "# Drop the fake extra column\n",
    "df = df.drop(columns=['dummy'], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5e8650dc-844d-453a-b4b7-9824018c6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile to get the sub-basin order for geotiffs and shapefiles\n",
    "gdf = gpd.read_file(shp)\n",
    "l_comids_geo = gdf['COMID'].to_list() # Store the polygon IDs into a list, we'll later use this as columns in the attribute dataframes\n",
    "\n",
    "# Initialize storage lists for geotiff and shapefile attributes\n",
    "l_values_geo = [[] for _ in range(len(l_comids_geo))] # Initialize an empty list where we'll store this basin's attributes - nested lists for each subbasin\n",
    "l_index_geo = [] # Initialize an empty list where we'll store the attribute descriptions - this will be our dataframe index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "89dad811-ba31-49d9-abf0-85c1dd6c5399",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'merit'\n",
    "shp_str = shp\n",
    "riv_str = riv\n",
    "l_values_geo, l_index_geo, l_comids_merit = attributes_from_merit(geo_folder, dataset, shp_str, riv_str, row, l_values_geo, l_index_geo, equal_area_crs=ea_crs)\n",
    "assert (l_comids_geo == l_comids_merit).all(), f\"mismatch between COMID orders determined before and inside attributes_from_merit()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e78daeae-c0c4-4e1a-b1ce-8dced71da63e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>72048321</th>\n",
       "      <th>72049179</th>\n",
       "      <th>72049188</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"35\" valign=\"top\">Topography</th>\n",
       "      <th>centroid_lat</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>45.470122</td>\n",
       "      <td>45.529858</td>\n",
       "      <td>45.515807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>centroid_lon</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>-63.619328</td>\n",
       "      <td>-63.600849</td>\n",
       "      <td>-63.644994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basin_area</th>\n",
       "      <th>km^2</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>35.054325</td>\n",
       "      <td>29.332096</td>\n",
       "      <td>29.080010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elev_min</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>25.400000</td>\n",
       "      <td>59.900002</td>\n",
       "      <td>59.799999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elev_mean</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>143.622503</td>\n",
       "      <td>267.559147</td>\n",
       "      <td>255.254822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elev_max</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>314.200012</td>\n",
       "      <td>362.700012</td>\n",
       "      <td>363.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elev_std</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>62.706802</td>\n",
       "      <td>63.034259</td>\n",
       "      <td>49.120840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slope_min</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slope_mean</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>4.318055</td>\n",
       "      <td>4.923624</td>\n",
       "      <td>5.113632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slope_max</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>24.456249</td>\n",
       "      <td>26.541697</td>\n",
       "      <td>26.779970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slope_std</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>2.860660</td>\n",
       "      <td>3.437215</td>\n",
       "      <td>3.281258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aspect_min</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aspect_mean</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>159.095487</td>\n",
       "      <td>182.547068</td>\n",
       "      <td>160.570070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aspect_max</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>359.999847</td>\n",
       "      <td>359.999817</td>\n",
       "      <td>359.999878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aspect_std</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>85.905128</td>\n",
       "      <td>106.256507</td>\n",
       "      <td>105.153998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>centroid_lat</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>45.470122</td>\n",
       "      <td>45.529858</td>\n",
       "      <td>45.515807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>centroid_lon</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>-63.619328</td>\n",
       "      <td>-63.600849</td>\n",
       "      <td>-63.644994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basin_area</th>\n",
       "      <th>km^2</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>35.054325</td>\n",
       "      <td>29.332096</td>\n",
       "      <td>29.080010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elev_min</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>25.400000</td>\n",
       "      <td>59.900002</td>\n",
       "      <td>59.799999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elev_mean</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>143.622503</td>\n",
       "      <td>267.559147</td>\n",
       "      <td>255.254822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elev_max</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>314.200012</td>\n",
       "      <td>362.700012</td>\n",
       "      <td>363.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elev_std</th>\n",
       "      <th>m.a.s.l.</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>62.706802</td>\n",
       "      <td>63.034259</td>\n",
       "      <td>49.120840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slope_min</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slope_mean</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>4.318055</td>\n",
       "      <td>4.923624</td>\n",
       "      <td>5.113632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slope_max</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>24.456249</td>\n",
       "      <td>26.541697</td>\n",
       "      <td>26.779970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slope_std</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>2.860660</td>\n",
       "      <td>3.437215</td>\n",
       "      <td>3.281258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aspect_min</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aspect_mean</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>159.095487</td>\n",
       "      <td>182.547068</td>\n",
       "      <td>160.570070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aspect_max</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>359.999847</td>\n",
       "      <td>359.999817</td>\n",
       "      <td>359.999878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aspect_std</th>\n",
       "      <th>degrees</th>\n",
       "      <th>MERIT Hydro</th>\n",
       "      <td>85.905128</td>\n",
       "      <td>106.256507</td>\n",
       "      <td>105.153998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stream_length</th>\n",
       "      <th>km</th>\n",
       "      <th>MERIT Hydro Basins</th>\n",
       "      <td>5.192127</td>\n",
       "      <td>3.122425</td>\n",
       "      <td>3.421978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stream_slope</th>\n",
       "      <th>m m^-1</th>\n",
       "      <th>MERIT Hydro Basins</th>\n",
       "      <td>0.004996</td>\n",
       "      <td>0.025531</td>\n",
       "      <td>0.025761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upstream_area</th>\n",
       "      <th>km^2</th>\n",
       "      <th>MERIT Hydro Basins</th>\n",
       "      <td>115.071409</td>\n",
       "      <td>29.332096</td>\n",
       "      <td>29.080009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stream_density</th>\n",
       "      <th>km^-1</th>\n",
       "      <th>MERIT Hydro, MERIT Hydro Basins</th>\n",
       "      <td>0.148117</td>\n",
       "      <td>0.089074</td>\n",
       "      <td>0.097619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elongation_ratio</th>\n",
       "      <th>-</th>\n",
       "      <th>MERIT Hydro, MERIT Hydro Basins</th>\n",
       "      <td>1.286710</td>\n",
       "      <td>2.139606</td>\n",
       "      <td>1.952309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        72048321  \\\n",
       "Category   Attribute        Unit     Source                                        \n",
       "Topography centroid_lat     degrees  MERIT Hydro                       45.470122   \n",
       "           centroid_lon     degrees  MERIT Hydro                      -63.619328   \n",
       "           basin_area       km^2     MERIT Hydro                       35.054325   \n",
       "           elev_min         m.a.s.l. MERIT Hydro                       25.400000   \n",
       "           elev_mean        m.a.s.l. MERIT Hydro                      143.622503   \n",
       "           elev_max         m.a.s.l. MERIT Hydro                      314.200012   \n",
       "           elev_std         m.a.s.l. MERIT Hydro                       62.706802   \n",
       "           slope_min        degrees  MERIT Hydro                        0.000000   \n",
       "           slope_mean       degrees  MERIT Hydro                        4.318055   \n",
       "           slope_max        degrees  MERIT Hydro                       24.456249   \n",
       "           slope_std        degrees  MERIT Hydro                        2.860660   \n",
       "           aspect_min       degrees  MERIT Hydro                        0.000000   \n",
       "           aspect_mean      degrees  MERIT Hydro                      159.095487   \n",
       "           aspect_max       degrees  MERIT Hydro                      359.999847   \n",
       "           aspect_std       degrees  MERIT Hydro                       85.905128   \n",
       "           centroid_lat     degrees  MERIT Hydro                       45.470122   \n",
       "           centroid_lon     degrees  MERIT Hydro                      -63.619328   \n",
       "           basin_area       km^2     MERIT Hydro                       35.054325   \n",
       "           elev_min         m.a.s.l. MERIT Hydro                       25.400000   \n",
       "           elev_mean        m.a.s.l. MERIT Hydro                      143.622503   \n",
       "           elev_max         m.a.s.l. MERIT Hydro                      314.200012   \n",
       "           elev_std         m.a.s.l. MERIT Hydro                       62.706802   \n",
       "           slope_min        degrees  MERIT Hydro                        0.000000   \n",
       "           slope_mean       degrees  MERIT Hydro                        4.318055   \n",
       "           slope_max        degrees  MERIT Hydro                       24.456249   \n",
       "           slope_std        degrees  MERIT Hydro                        2.860660   \n",
       "           aspect_min       degrees  MERIT Hydro                        0.000000   \n",
       "           aspect_mean      degrees  MERIT Hydro                      159.095487   \n",
       "           aspect_max       degrees  MERIT Hydro                      359.999847   \n",
       "           aspect_std       degrees  MERIT Hydro                       85.905128   \n",
       "           stream_length    km       MERIT Hydro Basins                 5.192127   \n",
       "           stream_slope     m m^-1   MERIT Hydro Basins                 0.004996   \n",
       "           upstream_area    km^2     MERIT Hydro Basins               115.071409   \n",
       "           stream_density   km^-1    MERIT Hydro, MERIT Hydro Basins    0.148117   \n",
       "           elongation_ratio -        MERIT Hydro, MERIT Hydro Basins    1.286710   \n",
       "\n",
       "                                                                        72049179  \\\n",
       "Category   Attribute        Unit     Source                                        \n",
       "Topography centroid_lat     degrees  MERIT Hydro                       45.529858   \n",
       "           centroid_lon     degrees  MERIT Hydro                      -63.600849   \n",
       "           basin_area       km^2     MERIT Hydro                       29.332096   \n",
       "           elev_min         m.a.s.l. MERIT Hydro                       59.900002   \n",
       "           elev_mean        m.a.s.l. MERIT Hydro                      267.559147   \n",
       "           elev_max         m.a.s.l. MERIT Hydro                      362.700012   \n",
       "           elev_std         m.a.s.l. MERIT Hydro                       63.034259   \n",
       "           slope_min        degrees  MERIT Hydro                        0.000000   \n",
       "           slope_mean       degrees  MERIT Hydro                        4.923624   \n",
       "           slope_max        degrees  MERIT Hydro                       26.541697   \n",
       "           slope_std        degrees  MERIT Hydro                        3.437215   \n",
       "           aspect_min       degrees  MERIT Hydro                        0.000000   \n",
       "           aspect_mean      degrees  MERIT Hydro                      182.547068   \n",
       "           aspect_max       degrees  MERIT Hydro                      359.999817   \n",
       "           aspect_std       degrees  MERIT Hydro                      106.256507   \n",
       "           centroid_lat     degrees  MERIT Hydro                       45.529858   \n",
       "           centroid_lon     degrees  MERIT Hydro                      -63.600849   \n",
       "           basin_area       km^2     MERIT Hydro                       29.332096   \n",
       "           elev_min         m.a.s.l. MERIT Hydro                       59.900002   \n",
       "           elev_mean        m.a.s.l. MERIT Hydro                      267.559147   \n",
       "           elev_max         m.a.s.l. MERIT Hydro                      362.700012   \n",
       "           elev_std         m.a.s.l. MERIT Hydro                       63.034259   \n",
       "           slope_min        degrees  MERIT Hydro                        0.000000   \n",
       "           slope_mean       degrees  MERIT Hydro                        4.923624   \n",
       "           slope_max        degrees  MERIT Hydro                       26.541697   \n",
       "           slope_std        degrees  MERIT Hydro                        3.437215   \n",
       "           aspect_min       degrees  MERIT Hydro                        0.000000   \n",
       "           aspect_mean      degrees  MERIT Hydro                      182.547068   \n",
       "           aspect_max       degrees  MERIT Hydro                      359.999817   \n",
       "           aspect_std       degrees  MERIT Hydro                      106.256507   \n",
       "           stream_length    km       MERIT Hydro Basins                 3.122425   \n",
       "           stream_slope     m m^-1   MERIT Hydro Basins                 0.025531   \n",
       "           upstream_area    km^2     MERIT Hydro Basins                29.332096   \n",
       "           stream_density   km^-1    MERIT Hydro, MERIT Hydro Basins    0.089074   \n",
       "           elongation_ratio -        MERIT Hydro, MERIT Hydro Basins    2.139606   \n",
       "\n",
       "                                                                        72049188  \n",
       "Category   Attribute        Unit     Source                                       \n",
       "Topography centroid_lat     degrees  MERIT Hydro                       45.515807  \n",
       "           centroid_lon     degrees  MERIT Hydro                      -63.644994  \n",
       "           basin_area       km^2     MERIT Hydro                       29.080010  \n",
       "           elev_min         m.a.s.l. MERIT Hydro                       59.799999  \n",
       "           elev_mean        m.a.s.l. MERIT Hydro                      255.254822  \n",
       "           elev_max         m.a.s.l. MERIT Hydro                      363.000000  \n",
       "           elev_std         m.a.s.l. MERIT Hydro                       49.120840  \n",
       "           slope_min        degrees  MERIT Hydro                        0.000000  \n",
       "           slope_mean       degrees  MERIT Hydro                        5.113632  \n",
       "           slope_max        degrees  MERIT Hydro                       26.779970  \n",
       "           slope_std        degrees  MERIT Hydro                        3.281258  \n",
       "           aspect_min       degrees  MERIT Hydro                        0.000000  \n",
       "           aspect_mean      degrees  MERIT Hydro                      160.570070  \n",
       "           aspect_max       degrees  MERIT Hydro                      359.999878  \n",
       "           aspect_std       degrees  MERIT Hydro                      105.153998  \n",
       "           centroid_lat     degrees  MERIT Hydro                       45.515807  \n",
       "           centroid_lon     degrees  MERIT Hydro                      -63.644994  \n",
       "           basin_area       km^2     MERIT Hydro                       29.080010  \n",
       "           elev_min         m.a.s.l. MERIT Hydro                       59.799999  \n",
       "           elev_mean        m.a.s.l. MERIT Hydro                      255.254822  \n",
       "           elev_max         m.a.s.l. MERIT Hydro                      363.000000  \n",
       "           elev_std         m.a.s.l. MERIT Hydro                       49.120840  \n",
       "           slope_min        degrees  MERIT Hydro                        0.000000  \n",
       "           slope_mean       degrees  MERIT Hydro                        5.113632  \n",
       "           slope_max        degrees  MERIT Hydro                       26.779970  \n",
       "           slope_std        degrees  MERIT Hydro                        3.281258  \n",
       "           aspect_min       degrees  MERIT Hydro                        0.000000  \n",
       "           aspect_mean      degrees  MERIT Hydro                      160.570070  \n",
       "           aspect_max       degrees  MERIT Hydro                      359.999878  \n",
       "           aspect_std       degrees  MERIT Hydro                      105.153998  \n",
       "           stream_length    km       MERIT Hydro Basins                 3.421978  \n",
       "           stream_slope     m m^-1   MERIT Hydro Basins                 0.025761  \n",
       "           upstream_area    km^2     MERIT Hydro Basins                29.080009  \n",
       "           stream_density   km^-1    MERIT Hydro, MERIT Hydro Basins    0.097619  \n",
       "           elongation_ratio -        MERIT Hydro, MERIT Hydro Basins    1.952309  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the geospatial attribute dataframe\n",
    "att_df_geo = pd.DataFrame(data = l_values_geo, index = l_comids_geo).transpose()\n",
    "multi_index = pd.MultiIndex.from_tuples(l_index_geo, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "att_df_geo.index = multi_index\n",
    "att_df_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bcd14aa-ba35-4e33-a36c-e6f53bfd930f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_river_attributes(riv_str, bas_str, l_values, l_index, area, equal_area_crs='ESRI:102008'):\n",
    "    \n",
    "    '''Calculates topographic attributes from a MERIT Hydro Basins river polygon'''\n",
    "\n",
    "    # We need some special actions if we're dealing with the distributed case, so check that first\n",
    "    case = 'lumped'\n",
    "    if 'distributed' in bas_str:\n",
    "        case = 'distributed'\n",
    "\n",
    "    # Handle the cases\n",
    "    if case == 'lumped': \n",
    "    \n",
    "        # Initialize NaN for cases where we have no shapefile or an empty one\n",
    "        stream_total = np.nan\n",
    "        min_length = np.nan\n",
    "        mean_length = np.nan\n",
    "        max_length = np.nan\n",
    "        std_length = np.nan\n",
    "        riv_order = np.nan\n",
    "        density = np.nan\n",
    "        elongation = np.nan\n",
    "        \n",
    "        # Check if the file exists (for headwaters we won't have a river polygon)\n",
    "        if os.path.exists(riv_str):\n",
    "            \n",
    "            # Load shapefiles\n",
    "            river = gpd.read_file(riv_str)\n",
    "            river = river.set_index('COMID')\n",
    "            river = river[~river.index.duplicated(keep='first')] # Removes any duplicate river segments\n",
    "    \n",
    "            # Check if we actually have a river segment (empty shapefile is not the same as no shapefile)\n",
    "            if len(river) > 0:\n",
    "                # Raw data\n",
    "                stream_lengths = []\n",
    "                headwaters = river[(river['maxup'] == 0) | (river['maxup'].isna())] # identify reaches with no upstream\n",
    "                for COMID in headwaters.index:\n",
    "                    stream_length = 0\n",
    "                    while COMID in river.index:\n",
    "                        stream_length += river.loc[COMID]['new_len_km'] # Add the length of the current segment\n",
    "                        COMID = river.loc[COMID]['NextDownID'] # Get the downstream reach\n",
    "                    stream_lengths.append(stream_length) # If we get here we ran out of downstream IDs\n",
    "            \n",
    "                # Stats\n",
    "                stream_total = river['new_len_km'].sum()\n",
    "                stream_lengths = np.array(stream_lengths)\n",
    "                min_length = stream_lengths.min()\n",
    "                mean_length = stream_lengths.mean()\n",
    "                max_length = stream_lengths.max()\n",
    "                std_length = stream_lengths.std()\n",
    "                riv_order = river['order'].max()\n",
    "                density = stream_total/area\n",
    "                elongation = 2*np.sqrt(area/np.pi)/max_length\n",
    "    \n",
    "        # Update lists\n",
    "        l_values.append(min_length)\n",
    "        l_index.append(('Topography', 'stream_length_min',  'km', 'MERIT Hydro Basins'))\n",
    "        l_values.append(mean_length)\n",
    "        l_index.append(('Topography', 'stream_length_mean', 'km', 'MERIT Hydro Basins'))\n",
    "        l_values.append(max_length)\n",
    "        l_index.append(('Topography', 'stream_length_max',  'km', 'MERIT Hydro Basins'))\n",
    "        l_values.append(std_length)\n",
    "        l_index.append(('Topography', 'stream_length_std',  'km', 'MERIT Hydro Basins'))\n",
    "        l_values.append(stream_total)\n",
    "        l_index.append(('Topography', 'segment_length_total', 'km', 'MERIT Hydro Basins'))\n",
    "        \n",
    "        # Order\n",
    "        l_values.append(riv_order)\n",
    "        l_index.append(('Topography', 'stream_order_max',  '-', 'MERIT Hydro Basins'))\n",
    "    \n",
    "        # Derived\n",
    "        l_values.append(density)\n",
    "        l_index.append(('Topography', 'stream_density',  'km^-1', 'MERIT Hydro, MERIT Hydro Basins'))\n",
    "        l_values.append(elongation)\n",
    "        l_index.append(('Topography', 'elongation_ratio','-', 'MERIT Hydro, MERIT Hydro Basins'))\n",
    "\n",
    "        # Final empty output\n",
    "        merit_comids = None\n",
    "\n",
    "    # distributed case\n",
    "    elif case == 'distributed':\n",
    "\n",
    "        # Get the basin shape and figure out what we're dealing with\n",
    "        basin = gpd.read_file(bas_str)\n",
    "        num_poly = len(basin)\n",
    "        merit_comids = basin['COMID'].values\n",
    "\n",
    "        # Check if the file exists (for headwaters we won't have a river polygon)\n",
    "        if os.path.exists(riv_str):\n",
    "            \n",
    "            # Load shapefiles\n",
    "            river = gpd.read_file(riv_str)\n",
    "            river = river.set_index('COMID')\n",
    "            river = river[~river.index.duplicated(keep='first')] # Removes any duplicate river segments\n",
    "\n",
    "            # Loop over the basins and process\n",
    "            for i_poly in range(num_poly):\n",
    "\n",
    "                # Find the river segment, if any, and get values for the statistics\n",
    "                comid = basin.iloc[i_poly]['COMID']\n",
    "                if comid in river.index:\n",
    "                    riv_poly = river.loc[comid]\n",
    "                    stream_length = riv_poly['new_len_km']\n",
    "                    slope = riv_poly['slope']\n",
    "                    upstream_area = riv_poly['uparea']\n",
    "                    basin_area = basin.to_crs('ESRI:102008').area.iloc[0] / 10**6 # area in km2\n",
    "                    density = stream_length/basin_area\n",
    "                    elongation = 2*np.sqrt(basin_area/np.pi)/stream_length\n",
    "                else:\n",
    "                    stream_length = np.nan # no delineated river here, so no stats\n",
    "                    slope = np.nan\n",
    "                    upstream_area = np.nan\n",
    "                    density = np.nan\n",
    "                    elongation = np.nan\n",
    "                \n",
    "                # Append statistics to list\n",
    "                l_values[i_poly].append(stream_length)\n",
    "                l_values[i_poly].append(slope)\n",
    "                l_values[i_poly].append(upstream_area)\n",
    "                l_values[i_poly].append(density)\n",
    "                l_values[i_poly].append(elongation)\n",
    "                    \n",
    "        else: # no river shapefile\n",
    "            \n",
    "            # just add nans everywhere\n",
    "            stream_length = np.nan # no delineated river here, so no stats\n",
    "            slope = np.nan\n",
    "            upstream_area = np.nan\n",
    "            density = np.nan\n",
    "            elongation = np.nan\n",
    "    \n",
    "            for i_poly in range(num_poly):\n",
    "                l_values[i_poly].append(stream_length)\n",
    "                l_values[i_poly].append(slope)\n",
    "                l_values[i_poly].append(upstream_area)\n",
    "                l_values[i_poly].append(density)\n",
    "                l_values[i_poly].append(elongation)\n",
    "\n",
    "        # Update the index list\n",
    "        l_index += [('Topography', 'stream_length',    'km',     'MERIT Hydro Basins'),\n",
    "                    ('Topography', 'stream_slope',     'm m^-1', 'MERIT Hydro Basins'),\n",
    "                    ('Topography', 'upstream_area',    'km^2',   'MERIT Hydro Basins'),\n",
    "                    ('Topography', 'stream_density',   'km^-1',  'MERIT Hydro, MERIT Hydro Basins'),\n",
    "                    ('Topography', 'elongation_ratio', '-',      'MERIT Hydro, MERIT Hydro Basins')]\n",
    "    \n",
    "    return l_values,l_index,merit_comids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d55b5aa-edd3-4637-8a7a-1186c17f2b3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_circmean(x):\n",
    "    return circmean(x.compressed(), high=360) # need compressed() because scipy < 1.12.0 circmean/std don't work well with masked arrays. This flattens the array and keeps values only\n",
    "\n",
    "def calc_circstd(x):\n",
    "    return circstd(x.compressed(), high=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09c87c40-74f9-4c32-9a44-96b51271e360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_from_merit(geo_folder, dataset, shp_str, riv_str, row, l_values, l_index, equal_area_crs='ESRI:102008'):\n",
    "    \n",
    "    '''Calculates topographic attributes from MERIT data'''\n",
    "\n",
    "    # We need some special actions if we're dealing with the distributed case, so check that first\n",
    "    case = 'lumped'\n",
    "    if 'distributed' in shp_str:\n",
    "        case = 'distributed'\n",
    "\n",
    "    # Load the shapefile\n",
    "    basin = gpd.read_file(shp_str)\n",
    "\n",
    "    ## Known values\n",
    "    if case == 'lumped':\n",
    "        clon = basin.to_crs(ea_crs).centroid.to_crs('EPSG:4326').x.iloc[0] # lon\n",
    "        clat = basin.to_crs(ea_crs).centroid.to_crs('EPSG:4326').y.iloc[0] # lat       \n",
    "        area = row['Basin_area_km2']\n",
    "        slat = row['Station_lat']\n",
    "        slon = row['Station_lon']\n",
    "        src  = row['Station_source']\n",
    "        l_values.append(clat)\n",
    "        l_index.append(('Topography', 'centroid_lat',  'degrees', 'MERIT Hydro'))\n",
    "        l_values.append(clon)\n",
    "        l_index.append(('Topography', 'centroid_lon',  'degrees', 'MERIT Hydro'))\n",
    "        l_values.append(area)\n",
    "        l_index.append(('Topography', 'basin_area', 'km^2', 'MERIT Hydro'))\n",
    "        l_values.append(slat)\n",
    "        l_index.append(('Topography', 'gauge_lat',  'degrees', 'USGS/WSC'))\n",
    "        l_values.append(slon)\n",
    "        l_index.append(('Topography', 'gauge_lon',  'degrees', 'USGS/WSC'))\n",
    "    \n",
    "    elif case == 'distributed':\n",
    "        num_poly = len(basin)\n",
    "        for i_poly in range(num_poly):\n",
    "            clon = basin.to_crs(ea_crs).centroid.to_crs('EPSG:4326').x.iloc[i_poly] # lon\n",
    "            clat = basin.to_crs(ea_crs).centroid.to_crs('EPSG:4326').y.iloc[i_poly] # lat       \n",
    "            area = (basin.to_crs(ea_crs).area / 10**6).iloc[i_poly]\n",
    "            l_values[i_poly].append(clat)\n",
    "            l_values[i_poly].append(clon)\n",
    "            l_values[i_poly].append(area)    \n",
    "        l_index.append(('Topography', 'centroid_lat',  'degrees', 'MERIT Hydro')) # these are outside the subbasin loop because we need them only once\n",
    "        l_index.append(('Topography', 'centroid_lon',  'degrees', 'MERIT Hydro'))\n",
    "        l_index.append(('Topography', 'basin_area', 'km^2', 'MERIT Hydro'))    \n",
    "\n",
    "    ## RASTERS\n",
    "    # Slope and elevation can use zonal stats built-ins\n",
    "    files = [str(geo_folder / dataset / 'raw'    / 'merit_hydro_elv.tif'),\n",
    "             str(geo_folder / dataset / 'slope'  / 'merit_hydro_slope.tif')]\n",
    "    attrs = ['elev',     'slope']\n",
    "    units = ['m.a.s.l.', 'degrees']\n",
    "    stats = ['mean', 'min', 'max', 'std']\n",
    "    for tif,att,unit in zip(files,attrs,units):\n",
    "        zonal_out = zonal_stats(shp_str, tif, stats=stats)\n",
    "        scale,offset = csa.read_scale_and_offset(tif)\n",
    "        l_values = csa.update_values_list(l_values, stats, zonal_out, scale, offset)\n",
    "        l_index += [('Topography', f'{att}_min',  f'{unit}', 'MERIT Hydro'),\n",
    "                    ('Topography', f'{att}_mean', f'{unit}', 'MERIT Hydro'),\n",
    "                    ('Topography', f'{att}_max',  f'{unit}', 'MERIT Hydro'),\n",
    "                    ('Topography', f'{att}_std',  f'{unit}', 'MERIT Hydro')]\n",
    "\n",
    "    # Aspect needs circular stats as custom input\n",
    "    stats = ['min', 'max']\n",
    "    tif = str(geo_folder / dataset / 'aspect' / 'merit_hydro_aspect.tif')\n",
    "    zonal_out = zonal_stats(shp_str, tif, stats=stats, add_stats={'circ_mean': calc_circmean,\n",
    "                                                                  'circ_std':  calc_circstd})\n",
    "    scale,offset = csa.read_scale_and_offset(tif)\n",
    "    l_values = csa.update_values_list(l_values, ['min', 'max', 'circ_mean', 'circ_std'], zonal_out, scale, offset)\n",
    "    l_index += [('Topography', 'aspect_min',   'degrees', 'MERIT Hydro'),\n",
    "                ('Topography', 'aspect_mean',  'degrees', 'MERIT Hydro'),\n",
    "                ('Topography', 'aspect_max',   'degrees', 'MERIT Hydro'),\n",
    "                ('Topography', 'aspect_std',   'degrees', 'MERIT Hydro')]\n",
    "    #l_values, l_index = csa.get_aspect_attributes(tif,l_values,l_index)\n",
    "    \n",
    "    ## VECTOR\n",
    "    l_values, l_index, merit_comids = get_river_attributes(riv_str, shp_str, l_values, l_index, area, equal_area_crs=equal_area_crs)\n",
    "    \n",
    "    return l_values, l_index, merit_comids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb47443-7406-4714-9d78-4052df00a7d5",
   "metadata": {},
   "source": [
    "#### Merge the separate attribute dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb9b9e40-7a63-448e-bd3c-80b4b0919b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check we have the same columns in all dfs\n",
    "geo_columns = att_df_geo.columns.unique().sort_values()\n",
    "met_columns = att_df_met.columns.unique().sort_values()\n",
    "lak_columns = att_df_lakes.columns.unique().sort_values()\n",
    "glh_columns = att_df_glhymps.columns.unique().sort_values()\n",
    "\n",
    "assert (geo_columns == met_columns).all(), f\"COMID mismatches between meteo and geospatial attribute dataframes\"\n",
    "assert (geo_columns == lak_columns).all(), f\"COMID mismatches between hydrolakes and geospatial attribute dataframes\"\n",
    "assert (geo_columns == glh_columns).all(), f\"COMID mismatches between glhymps and geospatial attribute dataframes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "823d94a9-c280-4aef-93ec-d07791335093",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_df = pd.concat([att_df_met,att_df_geo,att_df_lakes,att_df_glhymps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4af438d4-4644-4b44-889f-d8f6cdfa04ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>72048321.0</th>\n",
       "      <th>72049179.0</th>\n",
       "      <th>72049188.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Climate</th>\n",
       "      <th>num_years_rdrs</th>\n",
       "      <th>years</th>\n",
       "      <th>RDRS</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR0_SFC_mean</th>\n",
       "      <th>mm</th>\n",
       "      <th>RDRS</th>\n",
       "      <td>1389.9713642913412</td>\n",
       "      <td>1383.0917654395307</td>\n",
       "      <td>1393.9310812791518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR0_SFC_std</th>\n",
       "      <th>mm</th>\n",
       "      <th>RDRS</th>\n",
       "      <td>153.05992530337002</td>\n",
       "      <td>155.11670134231477</td>\n",
       "      <td>153.5888911052156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pet_mean</th>\n",
       "      <th>mm</th>\n",
       "      <th>RDRS</th>\n",
       "      <td>1026.7960842582397</td>\n",
       "      <td>1022.7861444815062</td>\n",
       "      <td>1024.7584377462772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pet_std</th>\n",
       "      <th>mm</th>\n",
       "      <th>RDRS</th>\n",
       "      <td>50.76377868009653</td>\n",
       "      <td>48.809287944138774</td>\n",
       "      <td>50.45739506616813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Geology</th>\n",
       "      <th>porosity_std</th>\n",
       "      <th>-</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>0.071502</td>\n",
       "      <td>0.049969</td>\n",
       "      <td>0.056043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_permeability_min</th>\n",
       "      <th>m^2</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>-16.5</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_permeability_mean</th>\n",
       "      <th>m^2</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>-14.54584</td>\n",
       "      <td>-14.19256</td>\n",
       "      <td>-14.432358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_permeability_max</th>\n",
       "      <th>m^2</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>-12.5</td>\n",
       "      <td>-14.1</td>\n",
       "      <td>-12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_permeability_std</th>\n",
       "      <th>m^2</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>1.067653</td>\n",
       "      <td>0.305366</td>\n",
       "      <td>0.843112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>747 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      72048321.0  \\\n",
       "Category Attribute             Unit  Source                        \n",
       "Climate  num_years_rdrs        years RDRS                     24   \n",
       "         PR0_SFC_mean          mm    RDRS     1389.9713642913412   \n",
       "         PR0_SFC_std           mm    RDRS     153.05992530337002   \n",
       "         pet_mean              mm    RDRS     1026.7960842582397   \n",
       "         pet_std               mm    RDRS      50.76377868009653   \n",
       "...                                                          ...   \n",
       "Geology  porosity_std          -     GLHYMPS            0.071502   \n",
       "         log_permeability_min  m^2   GLHYMPS               -16.5   \n",
       "         log_permeability_mean m^2   GLHYMPS           -14.54584   \n",
       "         log_permeability_max  m^2   GLHYMPS               -12.5   \n",
       "         log_permeability_std  m^2   GLHYMPS            1.067653   \n",
       "\n",
       "                                                      72049179.0  \\\n",
       "Category Attribute             Unit  Source                        \n",
       "Climate  num_years_rdrs        years RDRS                     24   \n",
       "         PR0_SFC_mean          mm    RDRS     1383.0917654395307   \n",
       "         PR0_SFC_std           mm    RDRS     155.11670134231477   \n",
       "         pet_mean              mm    RDRS     1022.7861444815062   \n",
       "         pet_std               mm    RDRS     48.809287944138774   \n",
       "...                                                          ...   \n",
       "Geology  porosity_std          -     GLHYMPS            0.049969   \n",
       "         log_permeability_min  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_mean m^2   GLHYMPS           -14.19256   \n",
       "         log_permeability_max  m^2   GLHYMPS               -14.1   \n",
       "         log_permeability_std  m^2   GLHYMPS            0.305366   \n",
       "\n",
       "                                                      72049188.0  \n",
       "Category Attribute             Unit  Source                       \n",
       "Climate  num_years_rdrs        years RDRS                     24  \n",
       "         PR0_SFC_mean          mm    RDRS     1393.9310812791518  \n",
       "         PR0_SFC_std           mm    RDRS      153.5888911052156  \n",
       "         pet_mean              mm    RDRS     1024.7584377462772  \n",
       "         pet_std               mm    RDRS      50.45739506616813  \n",
       "...                                                          ...  \n",
       "Geology  porosity_std          -     GLHYMPS            0.056043  \n",
       "         log_permeability_min  m^2   GLHYMPS               -16.5  \n",
       "         log_permeability_mean m^2   GLHYMPS          -14.432358  \n",
       "         log_permeability_max  m^2   GLHYMPS               -12.5  \n",
       "         log_permeability_std  m^2   GLHYMPS            0.843112  \n",
       "\n",
       "[747 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af286f-8d98-4344-8e8b-b6b8300fa1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb9b459-8a9f-4210-b7ff-bab3819d9990",
   "metadata": {},
   "source": [
    "### Full code trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6611decc-555d-4932-af6b-f4a5e0434efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interesting ix's to try:\n",
    "9  # 1 basin\n",
    "13 # 10 basins (1 really small)\n",
    "46 # 3 basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b44331c-4cd6-4b2a-ba99-db3a767f9ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!!! CHECK DEBUGGING STATUS: \n",
      "- Testing 1 file \n",
      "- Testing 1 basin\n",
      "Processing geospatial data into attributes for CAN_01AM001\n",
      " - processing rdrs\n",
      "Running distributed case for RDRS data.\n",
      "Running subbasin 0\n",
      "Running subbasin 1\n",
      "Running subbasin 2\n",
      "Running subbasin 3\n",
      "Running subbasin 4\n",
      "Running subbasin 5\n",
      "Running subbasin 6\n",
      "Running subbasin 7\n",
      "Running subbasin 8\n",
      "Running subbasin 9\n",
      "Running subbasin 10\n",
      " - processing worldclim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/globalhome/wmk934/HPC/camels_spat/camels-spat-env-jupyter/lib/python3.10/site-packages/rasterstats/io.py:328: NodataWarning: Setting nodata to -999; specify nodata explicitly\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - processing lai\n",
      " - processing forest_height\n",
      " - processing glclu2019\n",
      " - processing modis_land\n",
      " - processing lgrip30\n",
      " - processing merit\n",
      " - processing hydrolakes\n",
      " - processing pelletier\n",
      " - processing soilgrids\n",
      " - processing glhymps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/custom/software/2020/avx2/MPI/gcc9/openmpi4/geo-stack/2022a/lib/python3.10/site-packages/geopandas/io/file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!!! CHECK DEBUGGING STATUS: \n",
      "- Testing 1 file \n",
      "- Testing 1 basin\n"
     ]
    }
   ],
   "source": [
    "print(debug_message)\n",
    "for ix,row in cs_meta.iterrows():\n",
    "\n",
    "    # DEBUGGING\n",
    "    if ix != 13: continue\n",
    "\n",
    "    # Get the paths\n",
    "    basin_id, _, shp_dist_path, _, _ = prepare_delineation_outputs(cs_meta, ix, basins_path)\n",
    "    geo_folder = basins_path / 'basin_data' / basin_id / 'geospatial'\n",
    "    met_folder = basins_path / 'basin_data' / basin_id / 'forcing'\n",
    "    hyd_folder = basins_path / 'basin_data' / basin_id / 'observations'\n",
    "\n",
    "    # Make a temporary folder for storage\n",
    "    temp_path = temp_path / basin_id \n",
    "    temp_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Define the shapefiles\n",
    "    shp = str(shp_dist_path).format('basin') # because zonalstats wants a file path, not a geodataframe\n",
    "    riv = str(shp_dist_path).format('river') # For topographic attributes\n",
    "\n",
    "    # Specifically set the case\n",
    "    case = 'distributed'\n",
    "\n",
    "    # Load the shapefile to get the sub-basin order for geotiffs and shapefiles\n",
    "    gdf = gpd.read_file(shp)\n",
    "\n",
    "    # Data storage\n",
    "    l_comids_geo = gdf['COMID'].to_list() # Store the polygon IDs into a list, we'll later use this as columns in the attribute dataframes\n",
    "    l_values_geo = [[] for _ in range(len(l_comids_geo))] # Initialize an empty list where we'll store this basin's attributes - nested lists for each subbasin\n",
    "    l_index_geo = [] # Initialize an empty list where we'll store the attribute descriptions - this will be our dataframe index\n",
    "\n",
    "    l_values_lakes = [[] for _ in range(len(l_comids_geo))] # HydroLAKES open water bodies shapefile\n",
    "    l_index_lakes = []\n",
    "    l_values_glhymps = [[] for _ in range(len(l_comids_geo))] # GLHYMPS geology shapefile\n",
    "    l_index_glhymps = []\n",
    "\n",
    "    l_values_met = [] # RDRS meteorological netcdf data - creating a nested list with subbasins is done differently for RDRS, so this line is supposed to look different from the other l_values_[x] lists\n",
    "    l_index_met = []\n",
    "\n",
    "    # Data-specific processing\n",
    "    print(f'Processing geospatial data into attributes for {basin_id}')\n",
    "    for dataset in data_subfolders:\n",
    "        print(f' - processing {dataset}')\n",
    "\n",
    "        ## CLIMATE\n",
    "        if dataset == 'rdrs':\n",
    "            #l_values, l_index, ds_precip, ds_rdrs = csa.attributes_from_rdrs(met_folder, shp, dataset, l_values, l_index)\n",
    "            l_values_met, l_index_met, l_comids_met, _ = csa.attributes_from_rdrs(met_folder, shp, dataset, l_values_met, l_index_met)\n",
    "        if dataset == 'worldclim':\n",
    "            csa.oudin_pet_from_worldclim(geo_folder, dataset) # Get an extra PET estimate to sanity check RDRS outcomes\n",
    "            csa.aridity_and_fraction_snow_from_worldclim(geo_folder, dataset) # Get monthly aridity and fraction snow maps\n",
    "            l_values_geo, l_index_geo = csa.attributes_from_worldclim(geo_folder, dataset, shp, l_values_geo, l_index_geo, case=case)\n",
    "\n",
    "        ## LAND COVER\n",
    "        if dataset == 'forest_height':\n",
    "            l_values_geo, l_index_geo = csa.attributes_from_forest_height(geo_folder, dataset, shp, l_values_geo, l_index_geo, case=case)\n",
    "        if dataset == 'lai':\n",
    "            l_values_geo, l_index_geo = csa.attributes_from_lai(geo_folder, dataset, temp_path, shp, l_values_geo, l_index_geo, case=case)\n",
    "        if dataset == 'glclu2019':\n",
    "            l_values_geo, l_index_geo = csa.attributes_from_glclu2019(geo_folder, dataset, shp, l_values_geo, l_index_geo, case=case)\n",
    "        if dataset == 'modis_land':\n",
    "            l_values_geo, l_index_geo = csa.attributes_from_modis_land(geo_folder, dataset, shp, l_values_geo, l_index_geo, case=case)\n",
    "        if dataset == 'lgrip30':\n",
    "            l_values_geo, l_index_geo = csa.attributes_from_lgrip30(geo_folder, dataset, shp, l_values_geo, l_index_geo, case=case)\n",
    "\n",
    "        ## TOPOGRAPHY\n",
    "        if dataset == 'merit':\n",
    "            l_values_geo, l_index_geo, l_comids_merit = csa.attributes_from_merit(geo_folder, dataset, shp, riv, row, l_values_geo, l_index_geo, equal_area_crs=ea_crs, case=case)\n",
    "            assert (l_comids_geo == l_comids_merit).all(), f\"mismatch between COMID orders determined before and inside attributes_from_merit()\"\n",
    "\n",
    "        ## OPENWATER\n",
    "        if dataset == 'hydrolakes':\n",
    "            #l_values, l_index = csa.attributes_from_hydrolakes(geo_folder, dataset, l_values, l_index)\n",
    "            l_values_lakes, l_index_lakes, l_comids_lakes = csa.attributes_from_hydrolakes(geo_folder, dataset, shp, ea_crs, \n",
    "                                                                                           l_values_lakes, l_index_lakes)\n",
    "\n",
    "        ## SOIL\n",
    "        if dataset == 'pelletier':\n",
    "            l_values_geo, l_index_geo = csa.attributes_from_pelletier(geo_folder, dataset, shp, l_values_geo, l_index_geo, case=case)\n",
    "        if dataset == 'soilgrids':\n",
    "            l_values_geo, l_index_geo = csa.attributes_from_soilgrids(geo_folder, dataset, shp, l_values_geo, l_index_geo, case=case)\n",
    "\n",
    "        ## GEOLOGY\n",
    "        if dataset == 'glhymps':\n",
    "            #l_values, l_index = csa.attributes_from_glhymps(geo_folder, dataset, l_values, l_index)\n",
    "            l_values_glhymps, l_index_glhymps, l_comids_glhymps = csa.attributes_from_glhymps(geo_folder, dataset, shp, \n",
    "                                                                                              l_values_glhymps, l_index_glhymps, \n",
    "                                                                                              equal_area_crs=ea_crs)\n",
    "\n",
    "    ## MERGE THE DATAFRAMES\n",
    "    # Make the individual dataframes\n",
    "    # - RDRS\n",
    "    att_df_met = pd.DataFrame(data = l_values_met, index = l_comids_met).transpose()\n",
    "    multi_index = pd.MultiIndex.from_tuples(l_index_met, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "    att_df_met.index = multi_index\n",
    "\n",
    "    # - geotiffs\n",
    "    att_df_geo = pd.DataFrame(data = l_values_geo, index = l_comids_geo).transpose()\n",
    "    multi_index = pd.MultiIndex.from_tuples(l_index_geo, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "    att_df_geo.index = multi_index\n",
    "    \n",
    "    # - HydroLAKES\n",
    "    att_df_lakes = pd.DataFrame(data = l_values_lakes, index = l_comids_lakes).transpose()\n",
    "    multi_index = pd.MultiIndex.from_tuples(l_index_lakes, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "    att_df_lakes.index = multi_index\n",
    "\n",
    "    # - GLHYMPS\n",
    "    att_df_glhymps = pd.DataFrame(data = l_values_glhymps, index = l_comids_glhymps).transpose()\n",
    "    multi_index = pd.MultiIndex.from_tuples(l_index_glhymps, names=['Category', 'Attribute', 'Unit', 'Source'])\n",
    "    att_df_glhymps.index = multi_index\n",
    "\n",
    "    # Check we have the same columns in all dfs\n",
    "    geo_columns = att_df_geo.columns.unique().sort_values()\n",
    "    met_columns = att_df_met.columns.unique().sort_values()\n",
    "    lak_columns = att_df_lakes.columns.unique().sort_values()\n",
    "    glh_columns = att_df_glhymps.columns.unique().sort_values()\n",
    "    assert (geo_columns == met_columns).all(), f\"COMID mismatches between meteo and geospatial attribute dataframes\"\n",
    "    assert (geo_columns == lak_columns).all(), f\"COMID mismatches between hydrolakes and geospatial attribute dataframes\"\n",
    "    assert (geo_columns == glh_columns).all(), f\"COMID mismatches between glhymps and geospatial attribute dataframes\"\n",
    "\n",
    "    # Merge and save\n",
    "    att_df = pd.concat([att_df_met,att_df_geo,att_df_lakes,att_df_glhymps])\n",
    "\n",
    "print(debug_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eabb6d0f-13c7-4bfa-8891-bc037c01c5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>72047261.0</th>\n",
       "      <th>72047339.0</th>\n",
       "      <th>72047482.0</th>\n",
       "      <th>72047855.0</th>\n",
       "      <th>72048203.0</th>\n",
       "      <th>72049318.0</th>\n",
       "      <th>72049527.0</th>\n",
       "      <th>72049550.0</th>\n",
       "      <th>72049585.0</th>\n",
       "      <th>72049656.0</th>\n",
       "      <th>72049661.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th>Attribute</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Climate</th>\n",
       "      <th>num_years_rdrs</th>\n",
       "      <th>years</th>\n",
       "      <th>RDRS</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR0_SFC_mean</th>\n",
       "      <th>mm</th>\n",
       "      <th>RDRS</th>\n",
       "      <td>1178.1281495076287</td>\n",
       "      <td>1190.216598959602</td>\n",
       "      <td>1195.2213394437126</td>\n",
       "      <td>1187.1226192793001</td>\n",
       "      <td>1211.7013558289948</td>\n",
       "      <td>1219.8812825363398</td>\n",
       "      <td>1202.5363248568406</td>\n",
       "      <td>1192.0612796708845</td>\n",
       "      <td>1175.3464061380255</td>\n",
       "      <td>1175.4479721074883</td>\n",
       "      <td>1184.2964444827048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PR0_SFC_std</th>\n",
       "      <th>mm</th>\n",
       "      <th>RDRS</th>\n",
       "      <td>149.1360432992738</td>\n",
       "      <td>152.52595093657615</td>\n",
       "      <td>156.2346445042371</td>\n",
       "      <td>155.78782205101646</td>\n",
       "      <td>164.28329058163752</td>\n",
       "      <td>171.50740602588183</td>\n",
       "      <td>167.55966567935928</td>\n",
       "      <td>161.8045194529511</td>\n",
       "      <td>149.9404219614061</td>\n",
       "      <td>152.5628750115797</td>\n",
       "      <td>157.53336880889864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pet_mean</th>\n",
       "      <th>mm</th>\n",
       "      <th>RDRS</th>\n",
       "      <td>1135.972213843205</td>\n",
       "      <td>1126.404816187396</td>\n",
       "      <td>1120.884753996585</td>\n",
       "      <td>1120.0700821635012</td>\n",
       "      <td>1087.9457710392621</td>\n",
       "      <td>1057.4404989777454</td>\n",
       "      <td>1069.5087224262377</td>\n",
       "      <td>1105.0474224789887</td>\n",
       "      <td>1126.5045297484708</td>\n",
       "      <td>1114.3554333146706</td>\n",
       "      <td>1110.5237126009022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pet_std</th>\n",
       "      <th>mm</th>\n",
       "      <th>RDRS</th>\n",
       "      <td>51.072779601267264</td>\n",
       "      <td>50.55141125513</td>\n",
       "      <td>51.576821505826686</td>\n",
       "      <td>51.85721375592091</td>\n",
       "      <td>47.389855566281405</td>\n",
       "      <td>45.102862341729626</td>\n",
       "      <td>45.74977898619349</td>\n",
       "      <td>49.93600383948944</td>\n",
       "      <td>50.55739013029467</td>\n",
       "      <td>50.33665002125363</td>\n",
       "      <td>51.2586680502192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Geology</th>\n",
       "      <th>porosity_std</th>\n",
       "      <th>-</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042234</td>\n",
       "      <td>0.033908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_permeability_min</th>\n",
       "      <th>m^2</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_permeability_mean</th>\n",
       "      <th>m^2</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-14.572608</td>\n",
       "      <td>-14.842139</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_permeability_max</th>\n",
       "      <th>m^2</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-12.5</td>\n",
       "      <td>-12.5</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "      <td>-15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_permeability_std</th>\n",
       "      <th>m^2</th>\n",
       "      <th>GLHYMPS</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.140323</td>\n",
       "      <td>0.915511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1101 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      72047261.0  \\\n",
       "Category Attribute             Unit  Source                        \n",
       "Climate  num_years_rdrs        years RDRS                     37   \n",
       "         PR0_SFC_mean          mm    RDRS     1178.1281495076287   \n",
       "         PR0_SFC_std           mm    RDRS      149.1360432992738   \n",
       "         pet_mean              mm    RDRS      1135.972213843205   \n",
       "         pet_std               mm    RDRS     51.072779601267264   \n",
       "...                                                          ...   \n",
       "Geology  porosity_std          -     GLHYMPS                 0.0   \n",
       "         log_permeability_min  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_mean m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_max  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_std  m^2   GLHYMPS                 0.0   \n",
       "\n",
       "                                                      72047339.0  \\\n",
       "Category Attribute             Unit  Source                        \n",
       "Climate  num_years_rdrs        years RDRS                     37   \n",
       "         PR0_SFC_mean          mm    RDRS      1190.216598959602   \n",
       "         PR0_SFC_std           mm    RDRS     152.52595093657615   \n",
       "         pet_mean              mm    RDRS      1126.404816187396   \n",
       "         pet_std               mm    RDRS         50.55141125513   \n",
       "...                                                          ...   \n",
       "Geology  porosity_std          -     GLHYMPS                 0.0   \n",
       "         log_permeability_min  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_mean m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_max  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_std  m^2   GLHYMPS                 0.0   \n",
       "\n",
       "                                                      72047482.0  \\\n",
       "Category Attribute             Unit  Source                        \n",
       "Climate  num_years_rdrs        years RDRS                     37   \n",
       "         PR0_SFC_mean          mm    RDRS     1195.2213394437126   \n",
       "         PR0_SFC_std           mm    RDRS      156.2346445042371   \n",
       "         pet_mean              mm    RDRS      1120.884753996585   \n",
       "         pet_std               mm    RDRS     51.576821505826686   \n",
       "...                                                          ...   \n",
       "Geology  porosity_std          -     GLHYMPS                 0.0   \n",
       "         log_permeability_min  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_mean m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_max  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_std  m^2   GLHYMPS                 0.0   \n",
       "\n",
       "                                                      72047855.0  \\\n",
       "Category Attribute             Unit  Source                        \n",
       "Climate  num_years_rdrs        years RDRS                     37   \n",
       "         PR0_SFC_mean          mm    RDRS     1187.1226192793001   \n",
       "         PR0_SFC_std           mm    RDRS     155.78782205101646   \n",
       "         pet_mean              mm    RDRS     1120.0700821635012   \n",
       "         pet_std               mm    RDRS      51.85721375592091   \n",
       "...                                                          ...   \n",
       "Geology  porosity_std          -     GLHYMPS                 0.0   \n",
       "         log_permeability_min  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_mean m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_max  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_std  m^2   GLHYMPS                 0.0   \n",
       "\n",
       "                                                      72048203.0  \\\n",
       "Category Attribute             Unit  Source                        \n",
       "Climate  num_years_rdrs        years RDRS                     37   \n",
       "         PR0_SFC_mean          mm    RDRS     1211.7013558289948   \n",
       "         PR0_SFC_std           mm    RDRS     164.28329058163752   \n",
       "         pet_mean              mm    RDRS     1087.9457710392621   \n",
       "         pet_std               mm    RDRS     47.389855566281405   \n",
       "...                                                          ...   \n",
       "Geology  porosity_std          -     GLHYMPS                 0.0   \n",
       "         log_permeability_min  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_mean m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_max  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_std  m^2   GLHYMPS                 0.0   \n",
       "\n",
       "                                                      72049318.0  \\\n",
       "Category Attribute             Unit  Source                        \n",
       "Climate  num_years_rdrs        years RDRS                     37   \n",
       "         PR0_SFC_mean          mm    RDRS     1219.8812825363398   \n",
       "         PR0_SFC_std           mm    RDRS     171.50740602588183   \n",
       "         pet_mean              mm    RDRS     1057.4404989777454   \n",
       "         pet_std               mm    RDRS     45.102862341729626   \n",
       "...                                                          ...   \n",
       "Geology  porosity_std          -     GLHYMPS                 0.0   \n",
       "         log_permeability_min  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_mean m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_max  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_std  m^2   GLHYMPS                 0.0   \n",
       "\n",
       "                                                      72049527.0  \\\n",
       "Category Attribute             Unit  Source                        \n",
       "Climate  num_years_rdrs        years RDRS                     37   \n",
       "         PR0_SFC_mean          mm    RDRS     1202.5363248568406   \n",
       "         PR0_SFC_std           mm    RDRS     167.55966567935928   \n",
       "         pet_mean              mm    RDRS     1069.5087224262377   \n",
       "         pet_std               mm    RDRS      45.74977898619349   \n",
       "...                                                          ...   \n",
       "Geology  porosity_std          -     GLHYMPS            0.042234   \n",
       "         log_permeability_min  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_mean m^2   GLHYMPS          -14.572608   \n",
       "         log_permeability_max  m^2   GLHYMPS               -12.5   \n",
       "         log_permeability_std  m^2   GLHYMPS            1.140323   \n",
       "\n",
       "                                                      72049550.0  \\\n",
       "Category Attribute             Unit  Source                        \n",
       "Climate  num_years_rdrs        years RDRS                     37   \n",
       "         PR0_SFC_mean          mm    RDRS     1192.0612796708845   \n",
       "         PR0_SFC_std           mm    RDRS      161.8045194529511   \n",
       "         pet_mean              mm    RDRS     1105.0474224789887   \n",
       "         pet_std               mm    RDRS      49.93600383948944   \n",
       "...                                                          ...   \n",
       "Geology  porosity_std          -     GLHYMPS            0.033908   \n",
       "         log_permeability_min  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_mean m^2   GLHYMPS          -14.842139   \n",
       "         log_permeability_max  m^2   GLHYMPS               -12.5   \n",
       "         log_permeability_std  m^2   GLHYMPS            0.915511   \n",
       "\n",
       "                                                      72049585.0  \\\n",
       "Category Attribute             Unit  Source                        \n",
       "Climate  num_years_rdrs        years RDRS                     37   \n",
       "         PR0_SFC_mean          mm    RDRS     1175.3464061380255   \n",
       "         PR0_SFC_std           mm    RDRS      149.9404219614061   \n",
       "         pet_mean              mm    RDRS     1126.5045297484708   \n",
       "         pet_std               mm    RDRS      50.55739013029467   \n",
       "...                                                          ...   \n",
       "Geology  porosity_std          -     GLHYMPS                 0.0   \n",
       "         log_permeability_min  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_mean m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_max  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_std  m^2   GLHYMPS                 0.0   \n",
       "\n",
       "                                                      72049656.0  \\\n",
       "Category Attribute             Unit  Source                        \n",
       "Climate  num_years_rdrs        years RDRS                     37   \n",
       "         PR0_SFC_mean          mm    RDRS     1175.4479721074883   \n",
       "         PR0_SFC_std           mm    RDRS      152.5628750115797   \n",
       "         pet_mean              mm    RDRS     1114.3554333146706   \n",
       "         pet_std               mm    RDRS      50.33665002125363   \n",
       "...                                                          ...   \n",
       "Geology  porosity_std          -     GLHYMPS                 0.0   \n",
       "         log_permeability_min  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_mean m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_max  m^2   GLHYMPS               -15.2   \n",
       "         log_permeability_std  m^2   GLHYMPS                 0.0   \n",
       "\n",
       "                                                      72049661.0  \n",
       "Category Attribute             Unit  Source                       \n",
       "Climate  num_years_rdrs        years RDRS                     37  \n",
       "         PR0_SFC_mean          mm    RDRS     1184.2964444827048  \n",
       "         PR0_SFC_std           mm    RDRS     157.53336880889864  \n",
       "         pet_mean              mm    RDRS     1110.5237126009022  \n",
       "         pet_std               mm    RDRS       51.2586680502192  \n",
       "...                                                          ...  \n",
       "Geology  porosity_std          -     GLHYMPS                 0.0  \n",
       "         log_permeability_min  m^2   GLHYMPS               -15.2  \n",
       "         log_permeability_mean m^2   GLHYMPS               -15.2  \n",
       "         log_permeability_max  m^2   GLHYMPS               -15.2  \n",
       "         log_permeability_std  m^2   GLHYMPS                 0.0  \n",
       "\n",
       "[1101 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af8a8532-6bcb-4c2f-b727-8ddd507a7f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "att_file = f'attributes_{basin_id}.csv'\n",
    "att_df.to_csv(att_path/att_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45bb65-2e48-4af2-b95f-02da3318aebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels-spat-env-jupyter",
   "language": "python",
   "name": "camels-spat-env-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
