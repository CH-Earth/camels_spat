{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f237099-02ac-43bc-a7a9-3df7cc56692a",
   "metadata": {},
   "source": [
    "# Restructure data\n",
    "This takes the work-in-progress files and moves them into the final data structure we want to upload.\n",
    "\n",
    "Almost all of the data is catchment-based so we can put this into a parallel run. Things to remember:\n",
    "- Do not redistribute the raw WorldClim data - this is not allowed.\n",
    "- Remember to put the main attribute file into the resulting attributes folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "55141ae2-8d41-40a3-931f-28ad636c2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "import python_cs_functions as cs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c2ca7-63a6-45e7-b1ce-ec3813e8adf8",
   "metadata": {},
   "source": [
    "## Config handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fc94ed62-7e0b-418c-9526-37cc9597beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify where the config file can be found\n",
    "config_file = '../0_config/config.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1b357931-4d44-4457-a9b1-700c5c6a50e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the required info from the config file\n",
    "data_path            = cs.read_from_config(config_file,'data_path')\n",
    "\n",
    "# CAMELS-spat metadata\n",
    "cs_meta_path = cs.read_from_config(config_file,'cs_basin_path')\n",
    "cs_meta_name = cs.read_from_config(config_file,'cs_meta_name')\n",
    "cs_unusable_name = cs.read_from_config(config_file,'cs_unusable_name')\n",
    "\n",
    "# Basin folder\n",
    "cs_basin_folder = cs.read_from_config(config_file, 'cs_basin_path')\n",
    "basins_path = Path(data_path) / cs_basin_folder\n",
    "\n",
    "# Attributes folder\n",
    "cs_att_folder = cs.read_from_config(config_file, 'att_path')\n",
    "att_path  = basins_path / 'camels_spat_attributes.csv'\n",
    "\n",
    "# Destination folder\n",
    "final_fold = cs.read_from_config(config_file, 'final_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda6d28-5f18-4fd4-81fa-eb97a530257f",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dc20c1b9-12f1-4b70-8776-8d1dfc1877b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAMELS-spat metadata file\n",
    "cs_meta_path = Path(data_path) / cs_meta_path\n",
    "cs_meta = pd.read_csv(cs_meta_path / cs_meta_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "28d24e61-3176-4bf1-95d4-19890fd60d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open list of unusable stations; Enforce reading IDs as string to keep leading 0's\n",
    "cs_unusable = pd.read_csv(cs_meta_path / cs_unusable_name, dtype={'Station_id': object})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aa1d68-86eb-4d92-8636-6913833d8675",
   "metadata": {},
   "source": [
    "## General processing\n",
    "Steps:\n",
    "1. Set up the folder structure\n",
    "2. Subset and move the meta-data file to only the 1426 basins we want to keep\n",
    "3. Subset and move the main attributes file\n",
    "4. Add a `readme`, `citations`, and `known issues` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6b1e661f-bad1-410c-85be-87fd68225ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the top level path\n",
    "dest_root = Path(final_fold) / 'camels-spat-upload'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5f0586dc-95d0-497d-a44c-82e060ad7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the folder structure\n",
    "standard_subfolders = ['headwater','meso-scale','macro-scale']\n",
    "forcing_subfolders =  ['gridded','lumped','distributed']\n",
    "specific_subfolders = {\n",
    "    'attributes':   [], \n",
    "    'forcing':      ['daymet','em-earth','era5','rdrs'],\n",
    "    'geospatial':   ['forest-height','glclu2019','glhymps','hydrolakes','lai','lgrip30','merit','modis-land','pelletier','soilgrids','worldclim-derived'],\n",
    "    'observations': ['obs-daily','obs-hourly'],\n",
    "    'shapefiles':   ['delineation-outcomes','shapes-distributed','shapes-forcing','shapes-lumped','shapes-reference']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5a5dbdcb-a995-42d1-9043-bb7d0e43e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for main_folder, sub_folders in specific_subfolders.items():\n",
    "    for scale in standard_subfolders:\n",
    "        if main_folder == 'geospatial': # add dedicated metadata folder\n",
    "            Path(f\"{dest_root}/{main_folder}/_metadata\").mkdir(parents=True, exist_ok=True)\n",
    "        if len(sub_folders) == 0: # attributes\n",
    "            Path(f\"{dest_root}/{main_folder}/{scale}\").mkdir(parents=True, exist_ok=True)\n",
    "        else: # everything else\n",
    "            for sub_folder in sub_folders:\n",
    "                if main_folder == 'forcing':\n",
    "                    for aggregation in forcing_subfolders:\n",
    "                        Path(f\"{dest_root}/{main_folder}/{scale}/{sub_folder}/{sub_folder}-{aggregation}\").mkdir(parents=True, exist_ok=True)\n",
    "                else: # not forcing\n",
    "                    Path(f\"{dest_root}/{main_folder}/{scale}/{sub_folder}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create an ERA5-invariant folder\n",
    "for scale in standard_subfolders:\n",
    "    Path(f\"{dest_root}/forcing/{scale}/era5/era5-invariants\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f7fc1dcb-8b3b-4c1f-92c1-8101fec406c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Meta-data\n",
    "cs_meta_upload = cs_meta[~cs_meta.set_index(['Country', 'Station_id']).index.isin(cs_unusable.set_index(['Country', 'Station_id']).index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fe3532a6-0392-449d-82c7-844e80f96407",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_meta_upload.to_csv(dest_root/'camels-spat-metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c35c73ad-fa9d-4efa-90b4-be99dc93f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Attributes\n",
    "cs_att = pd.read_csv(att_path, low_memory=False)\n",
    "drop_these = cs_unusable['Country'] + \"_\" + cs_unusable['Station_id']\n",
    "cs_att_upload = cs_att.drop(columns=drop_these, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b4a40c2c-a0e2-4a34-a973-2727e0a59d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_att_upload.to_csv(dest_root/'attributes'/'attributes-lumped.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a1b13fb6-bbf1-48a1-8141-58a4cda54ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. readme, citations, known issues\n",
    "# We'll add these manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1f7f2a4c-6c05-4562-adc1-a94873cb7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Basic check(s)\n",
    "# 5.1 Ensure we have the same basins in meta and attributes\n",
    "meta_basins = (cs_meta_upload['Country'] + '_' + cs_meta_upload['Station_id']).values\n",
    "attr_basins = cs_att_upload.columns.to_list()\n",
    "attr_basins.remove('Category')\n",
    "attr_basins.remove('Attribute')\n",
    "attr_basins.remove('Unit')\n",
    "attr_basins.remove('Source')\n",
    "assert (meta_basins == attr_basins).all(), \"Basins not in same order and/or mismatches\"\n",
    "\n",
    "# 5.2 Ensure we have the expected number of basins (1426)\n",
    "assert len(meta_basins) == 1426, \"Number of basins not 1426\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c0281-c440-40fa-bd92-fc23cf87f9db",
   "metadata": {},
   "source": [
    "## Per-basin processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45db4238-54f7-4a86-939c-14685b0b8e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a515f066-5b05-4628-bd46-55787f1724cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mimic function inputs\n",
    "ix = 1116\n",
    "row = cs_meta_upload.iloc[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7516c5ea-9f58-4f5c-a0d7-24991aa23807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the spatial category\n",
    "category = row['subset_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9829de34-0307-4cc3-95c9-ee7936fbaa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the location of the source files\n",
    "basin_id   = row['Country'] + '_' + row['Station_id']\n",
    "geo_folder = basins_path / 'basin_data' / basin_id / 'geospatial'\n",
    "met_folder = basins_path / 'basin_data' / basin_id / 'forcing'\n",
    "hyd_folder = basins_path / 'basin_data' / basin_id / 'observations'\n",
    "shp_folder = basins_path / 'basin_data' / basin_id / 'shapefiles'\n",
    "att_folder = basins_path / 'attributes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1af6516-bfa7-43fa-9da5-17933c78757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track the main files we expect\n",
    "copied_files = []\n",
    "expected_files = [\n",
    "    # shapefiles excluding reference files because we know we don't have them for all cases\n",
    "    'shapefiles-delineation-plot',\n",
    "    'shapefiles-dist-basin',\n",
    "    'shapefiles-dist-river',\n",
    "    'shapefiles-forc-daymet',\n",
    "    'shapefiles-forc-emearth',\n",
    "    'shapefiles-forc-era5',\n",
    "    'shapefiles-forc-rdrs',\n",
    "    'shapefiles-lump-basin',\n",
    "    # observations\n",
    "    'observations-daily', # we know these exist because if not the shutil.copy() will fail\n",
    "    'observations-hourly',# must exist or copy error\n",
    "    # forcing\n",
    "    'forcing-daym-grid',\n",
    "    'forcing-daym-lump', \n",
    "    'forcing-daym-dist',\n",
    "    'forcing-emea-grid',\n",
    "    'forcing-emea-lump',\n",
    "    'forcing-emea-dist',\n",
    "    'forcing-era5-grid',\n",
    "    'forcing-era5-lump',\n",
    "    'forcing-era5-dist',\n",
    "    'forcing-era5-inva',\n",
    "    'forcing-rdrs-grid',\n",
    "    'forcing-rdrs-lump',\n",
    "    'forcing-rdrs-dist',\n",
    "    # attributes\n",
    "    'attributes-dist', # must exist or copy error\n",
    "    # geospatial\n",
    "    'geospatial-forest-height-2000', # must exist or copy error\n",
    "    'geospatial-forest-height-2020', # must exist or copy error\n",
    "    'geospatial-glclu2019-map', # must exist or copy error\n",
    "    'geospatial-glclu2019-strata', # must exist or copy error\n",
    "    'geospatial-glhymps',\n",
    "    'geospatial-hydrolakes',\n",
    "    'geospatial-lai',\n",
    "    'geospatial-lgrip30', # must exist or copy error\n",
    "    'geospatial-merit-aspect', # must exist or copy error\n",
    "    'geospatial-merit-dem', # must exist or copy error\n",
    "    'geospatial-merit-slope', # must exist or copy error\n",
    "    'geospatial-modis-land-mode', # must exist or copy error\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d853adb0-99b0-47f9-8fcc-dbd57894765e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34d6ba61-5b54-49a6-87d3-e94baa68cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Shapefiles\n",
    "# Source:      /basin_data/[basin_id]/shapesfiles/distributed\n",
    "#                                                /forcing_grids\n",
    "#                                                /lumped\n",
    "#                                                /reference\n",
    "#                                                /[basin_id]_delineation_results.png\n",
    "#\n",
    "# Destination: /camels-spat-upload/shapefiles/[category]/delineation-outcomes\n",
    "#                                                       /shapes-distributed\n",
    "#                                                       /shapes-forcing\n",
    "#                                                       /shapes-lumped\n",
    "#                                                       /shapes-reference\n",
    "#\n",
    "# Too keep things clean we'll need to create a dedicated basin folder inside each \n",
    "# of the 'shapes' subfolders in destination.\n",
    "\n",
    "# 1a. Delineation plot\n",
    "# This will error automatically if the source file does not exist as specified\n",
    "file = f\"{basin_id}_delineation_results.png\"\n",
    "src = shp_folder / file\n",
    "dst = dest_root / 'shapefiles' / category / 'delineation-outcomes' / file\n",
    "shutil.copy(src, dst)\n",
    "copied_files.append('shapefiles-delineation-plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b057bf81-6191-493a-ba4d-66cb3374d3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/custom/software/2020/avx2/MPI/gcc9/openmpi4/geo-stack/2022a/lib/python3.10/site-packages/geopandas/io/file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "# 1b. Distributed shapefiles\n",
    "dst = dest_root / 'shapefiles' / category / 'shapes-distributed' / basin_id\n",
    "dst.mkdir(exist_ok=True)\n",
    "\n",
    "src_files = glob.glob(str(shp_folder / 'distributed' / f\"{basin_id}_distributed_*\"))\n",
    "for src_file in src_files:\n",
    "    file = os.path.basename(src_file)\n",
    "    dst_file = dst / file\n",
    "    shutil.copy(src_file, dst_file)\n",
    "    if file == f\"{basin_id}_distributed_basin.shp\": copied_files.append('shapefiles-dist-basin')\n",
    "    if file == f\"{basin_id}_distributed_river.shp\": copied_files.append('shapefiles-dist-river')\n",
    "\n",
    "# Run a quick fix on the distributed river shapefile, to drop the existing 'lengthkm' column\n",
    "river_file = dst / f\"{basin_id}_distributed_river.shp\"\n",
    "gdf = gpd.read_file(river_file)\n",
    "gdf = gdf.drop(columns=['lengthkm'])\n",
    "if len(gdf) == 0: # empty shapefile, and this will interfere with saving\n",
    "    empty_row = pd.DataFrame([{col: np.nan if col != 'geometry' else None for col in gdf.columns}]) # add Nan/None (this may also help users)\n",
    "    gdf = pd.concat([gdf, empty_row], ignore_index=True)\n",
    "gdf.to_file(river_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb2834cd-304c-44ba-9598-dc19a9363816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1c. Forcing shapefiles\n",
    "dst = dest_root / 'shapefiles' / category / 'shapes-forcing' / basin_id\n",
    "dst.mkdir(exist_ok=True)\n",
    "\n",
    "src_files = glob.glob(str(shp_folder / 'forcing_grids' / f\"*_grid_{basin_id}.*\"))\n",
    "for src_file in src_files:\n",
    "    file = os.path.basename(src_file)\n",
    "    file = file.replace(f\"_{basin_id}\",\"\")     # swap the basin id to the front\n",
    "    dst_file = f\"{basin_id}_{file}\"\n",
    "    dst_file = dst / dst_file\n",
    "    shutil.copy(src_file, dst_file)\n",
    "    if os.path.basename(dst_file) == f\"{basin_id}_Daymet_grid.shp\": copied_files.append('shapefiles-forc-daymet')\n",
    "    if os.path.basename(dst_file) == f\"{basin_id}_EM_Earth_grid.shp\": copied_files.append('shapefiles-forc-emearth')\n",
    "    if os.path.basename(dst_file) == f\"{basin_id}_ERA5_grid.shp\": copied_files.append('shapefiles-forc-era5')\n",
    "    if os.path.basename(dst_file) == f\"{basin_id}_RDRS_grid.shp\": copied_files.append('shapefiles-forc-rdrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b66bd576-c55a-4388-97df-d7be2172576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1d. Lumped shapefiles\n",
    "dst = dest_root / 'shapefiles' / category / 'shapes-lumped' / basin_id\n",
    "dst.mkdir(exist_ok=True)\n",
    "src_files = glob.glob(str(shp_folder / 'lumped' / f\"{basin_id}_lumped.*\"))\n",
    "for src_file in src_files:\n",
    "    file = os.path.basename(src_file)\n",
    "    dst_file = dst / file\n",
    "    shutil.copy(src_file, dst_file)\n",
    "    if file == f\"{basin_id}_lumped.shp\": copied_files.append('shapefiles-lump-basin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26355c3f-b3f3-4065-a8e5-81c4c6176cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1e. Reference shapefiles\n",
    "dst = dest_root / 'shapefiles' / category / 'shapes-reference' / basin_id\n",
    "dst.mkdir(exist_ok=True)\n",
    "src_files = glob.glob(str(shp_folder / 'reference' / f\"{basin_id}_reference.*\"))\n",
    "if len(src_files) > 0:\n",
    "    for src_file in src_files:\n",
    "        file = os.path.basename(src_file)\n",
    "        dst_file = dst / file\n",
    "        shutil.copy(src_file, dst_file)\n",
    "else:\n",
    "    with open(str(dst/f\"{basin_id}.txt\"), 'w') as f:\n",
    "        f.write(f\"No reference file available for basin {basin_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a14dee-97d9-4125-b84b-a79079b372f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71b251e9-e31b-47af-b315-7b9e597db3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Observations\n",
    "# Source:      /basin_data/[basin_id]/observations/[basin_id]_daily_flow_observations.nc\n",
    "#                                                 /[basin_id]_hourly_flow_observations.nc\n",
    "#\n",
    "# Destination: /camels-spat-upload/observations/[category]/obs-daily/[basin_id]_daily_flow_observations.nc\n",
    "#                                                         /obs-hourly/[basin_id]_hourly_flow_observations.nc\n",
    "\n",
    "# 2a. Daily\n",
    "file = f\"{basin_id}_daily_flow_observations.nc\"\n",
    "src = hyd_folder / file\n",
    "dst = dest_root / 'observations' / category / 'obs-daily' / file\n",
    "shutil.copy(src, dst)\n",
    "copied_files.append('observations-daily')\n",
    "\n",
    "# 2b. Hourly\n",
    "file = f\"{basin_id}_hourly_flow_observations.nc\"\n",
    "src = hyd_folder / file\n",
    "dst = dest_root / 'observations' / category / 'obs-hourly' / file\n",
    "shutil.copy(src, dst)\n",
    "copied_files.append('observations-hourly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfe0000-7fee-430b-ab5a-5de0dd9efdc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a8f7fe32-0c27-449f-8577-894970168516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_netcdf_compatibility(file1, file2, file3, expected_vars, expected_time_steps):\n",
    "    \"\"\"\n",
    "    Opens three NetCDF files and checks if they:\n",
    "    - Cover the same time period.\n",
    "    - Contain the expected variables (also ensures variables have a 'time' dimension).\n",
    "    - Have the expected number of time steps.\n",
    "\n",
    "    Parameters:\n",
    "    - file1, file2, file3 (str): File paths to the NetCDF files.\n",
    "    - expected_vars (list): List of expected variables that should have a \"time\" dimension.\n",
    "    - expected_time_steps (int): Expected number of time steps in each file.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary with results for time period matching, variable existence, and time step count.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the NetCDF files\n",
    "    ds1 = xr.open_dataset(file1)\n",
    "    ds2 = xr.open_dataset(file2)\n",
    "    ds3 = xr.open_dataset(file3)\n",
    "\n",
    "    # Extract time ranges (if time exists in dataset)\n",
    "    time_range1 = (ds1[\"time\"].min().values, ds1[\"time\"].max().values) if \"time\" in ds1 else None\n",
    "    time_range2 = (ds2[\"time\"].min().values, ds2[\"time\"].max().values) if \"time\" in ds2 else None\n",
    "    time_range3 = (ds3[\"time\"].min().values, ds3[\"time\"].max().values) if \"time\" in ds3 else None\n",
    "\n",
    "    # Check if time periods match\n",
    "    time_match = (time_range1 == time_range2 == time_range3)\n",
    "\n",
    "    # Extract the number of time steps in each dataset\n",
    "    time_steps1 = len(ds1[\"time\"]) if \"time\" in ds1 else None\n",
    "    time_steps2 = len(ds2[\"time\"]) if \"time\" in ds2 else None\n",
    "    time_steps3 = len(ds3[\"time\"]) if \"time\" in ds3 else None\n",
    "\n",
    "    # Check if time step counts match the expected value\n",
    "    time_step_match = (time_steps1 == time_steps2 == time_steps3 == expected_time_steps)\n",
    "\n",
    "    # Check if expected variables exist in each dataset\n",
    "    var_existence = {\n",
    "        var: {\n",
    "            \"file1\": var in ds1.data_vars and \"time\" in ds1[var].dims,\n",
    "            \"file2\": var in ds2.data_vars and \"time\" in ds2[var].dims,\n",
    "            \"file3\": var in ds3.data_vars and \"time\" in ds3[var].dims,\n",
    "        }\n",
    "        for var in expected_vars\n",
    "    }\n",
    "\n",
    "    # Find any missing variables\n",
    "    missing_vars = {\n",
    "        var: [file for file in [\"file1\", \"file2\", \"file3\"] if not var_existence[var][file]]\n",
    "        for var in expected_vars\n",
    "        if not all(var_existence[var].values())  # Only keep variables that are missing in at least one file\n",
    "    }\n",
    "\n",
    "    # Find any files with incorrect time step count\n",
    "    incorrect_time_steps = {\n",
    "        file: actual_steps\n",
    "        for file, actual_steps in {\n",
    "            \"file1\": time_steps1,\n",
    "            \"file2\": time_steps2,\n",
    "            \"file3\": time_steps3,\n",
    "        }.items()\n",
    "        if actual_steps != expected_time_steps\n",
    "    }\n",
    "\n",
    "    # Raise assertion error if any expected variable is missing in any file\n",
    "    assert not missing_vars, f\"Missing expected variables in files: {missing_vars}\"\n",
    "\n",
    "    # Raise assertion error if any file has an incorrect number of time steps\n",
    "    assert not incorrect_time_steps, f\"Incorrect time steps in files: {incorrect_time_steps}. Expected {expected_time_steps}.\"\n",
    "\n",
    "    # Close datasets to free memory\n",
    "    ds1.close()\n",
    "    ds2.close()\n",
    "    ds3.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "56663dfc-cd3e-4146-93be-1ef4d13fbb7a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_encoding(nc, dim_order, time_chunk=100):\n",
    "    \"\"\"\n",
    "    Generates an encoding dictionary for NetCDF compression and chunking.\n",
    "\n",
    "    Parameters:\n",
    "    - nc (xr.Dataset): Xarray dataset containing the variables and dimensions.\n",
    "    - dim_order (tuple): Tuple specifying the expected dimension order (e.g., (\"time\", \"rlat\", \"rlon\")).\n",
    "    - time_chunk (int): Chunk size for the \"time\" dimension.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Encoding dictionary for use with `to_netcdf()`, ensuring 'source' and 'coordinates' are removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create chunk_sizes where \"time\" has \"time_chunk\" size, and other dimensions keep their full length\n",
    "    chunk_sizes = tuple(time_chunk if dim == \"time\" else nc.dims[dim] for dim in dim_order)\n",
    "\n",
    "    # Find variables that match the specified dimension order\n",
    "    matching_vars = [var for var in nc.data_vars if nc[var].dims == dim_order]\n",
    "\n",
    "    # Construct encoding dictionary, preserving existing settings\n",
    "    encoding = {}\n",
    "    for var in matching_vars:\n",
    "        # Copy existing encoding or start with an empty dict\n",
    "        existing_encoding = nc[var].encoding.copy()\n",
    "\n",
    "        # Update only the necessary keys\n",
    "        updated_encoding = {\n",
    "            **existing_encoding,  # Retain existing encoding\n",
    "            \"zlib\": True,\n",
    "            \"complevel\": 4,\n",
    "            \"chunksizes\": chunk_sizes,\n",
    "        }\n",
    "\n",
    "        # Remove unwanted encoding keys\n",
    "        for key in [\"source\", \"coordinates\"]:\n",
    "            updated_encoding.pop(key, None)  # Remove if exists\n",
    "\n",
    "        # Assign updated encoding back to dictionary\n",
    "        encoding[var] = updated_encoding\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3c9b0e5e-1da0-41c1-b527-4da416d843e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fix_nc_encoding(ds, variables=None, missing_values=-999.0):\n",
    "    \"\"\"\n",
    "    Replaces specified missing_values in an xarray Dataset with NaN and updates encoding.\n",
    "\n",
    "    Parameters:\n",
    "    - ds (xr.Dataset): The xarray Dataset to modify.\n",
    "    - variables (list, optional): List of variable names to process. If None, all variables are processed.\n",
    "    - missing_values (float, int, or list): Single missing value or a list of values matching the variables.\n",
    "\n",
    "    Returns:\n",
    "    - xr.Dataset: The updated Dataset with NaNs and modified history.\n",
    "    \"\"\"\n",
    "\n",
    "    # If no variable list is provided, process all data variables\n",
    "    if variables is None:\n",
    "        variables = list(ds.data_vars)\n",
    "\n",
    "    # Ensure missing_values is a list matching the variables list\n",
    "    if not isinstance(missing_values, list):\n",
    "        missing_values = [missing_values] * len(variables)  # Expand single value to list\n",
    "\n",
    "    if len(missing_values) != len(variables):\n",
    "        raise ValueError(\"Length of missing_values must match the number of variables.\")\n",
    "\n",
    "    modified_vars = []  # Track which variables were updated\n",
    "\n",
    "    # Loop over specified variables and their corresponding missing values\n",
    "    for var, missing_value in zip(variables, missing_values):\n",
    "        if var in ds:\n",
    "            # Preserve existing encoding while removing/updating _FillValue and missing_value\n",
    "            updated_encoding = ds[var].encoding.copy()  # Copy the existing encoding\n",
    "                        \n",
    "            # Replace missing_value with NaN\n",
    "            ds[var] = ds[var].where(ds[var] != missing_value, np.nan)\n",
    "            \n",
    "            # Update encoding\n",
    "            updated_encoding[\"_FillValue\"] = np.nan\n",
    "            updated_encoding[\"missing_value\"] = np.nan\n",
    "\n",
    "            # Apply the updated encoding back\n",
    "            ds[var].encoding = updated_encoding\n",
    "\n",
    "            # Track changes\n",
    "            modified_vars.append(f\"{var} (Replaced {missing_value} with NaN)\")\n",
    "\n",
    "    # Update global history attribute (handling case insensitivity)\n",
    "    if modified_vars:\n",
    "        timestamp = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "        change_log = f\"{timestamp}: Replaced FillValues with NaN in: {', '.join(modified_vars)}.\"\n",
    "\n",
    "        # Find existing history key (case-insensitive search)\n",
    "        history_key = next((key for key in ds.attrs if key.lower() == \"history\"), \"History\")\n",
    "\n",
    "        # Append to existing history\n",
    "        existing_history = ds.attrs.get(history_key, \"\")\n",
    "        ds.attrs[history_key] = f\"{existing_history} {change_log}\".strip()\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "93e2268b-a3a7-4815-99d4-84ca11879298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Forcing\n",
    "# Source:      /basin_data/[basin_id]/forcing/distributed/[lots of individual files]\n",
    "#                                            /lumped/[lots of individual files]\n",
    "#                                            /raw/[lots of individual files]\n",
    "#\n",
    "# Destination: /camels-spat-upload/forcing/[category]/daymet/daymet-distributed\n",
    "#                                                    /daymet/daymet-gridded\n",
    "#                                                    /daymet/daymet-lumped\n",
    "#                                                    /em-earth\n",
    "#                                                    /era5\n",
    "#                                                    /rdrs\n",
    "#\n",
    "# We need to merge the individual files for each basin into a single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fc425b5b-474c-4e5d-8df7-6acb3ddf6bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a. DAYMET - \n",
    "# Special treatment: we'll load these without decoding times (decode_times=False) to avoid:\n",
    "# UserWarning: Variable 'time' has datetime type and a bounds variable but time.encoding \n",
    "# does not have units specified. The units encodings for 'time' and 'time_bnds' will be \n",
    "# determined independently and may not be equal, counter to CF-conventions. If this is a \n",
    "# concern, specify a units encoding for 'time' before writing to a file.\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "    # gridded\n",
    "    dst1 = dest_root / 'forcing' / category / 'daymet' / 'daymet-gridded' / f\"{basin_id}_daymet_gridded.nc\"\n",
    "    src_files = glob.glob(str(met_folder / 'raw' / f\"*aymet_*.nc\"))\n",
    "    nc = xr.open_mfdataset(src_files, combine=\"by_coords\")#, decode_times=False)\n",
    "    expected_length = len(nc['time'])\n",
    "    encoding = generate_encoding(nc, (\"time\",\"x\",\"y\"))\n",
    "    nc.to_netcdf(dst1, encoding=encoding)\n",
    "    nc.close()\n",
    "    \n",
    "    # lumped\n",
    "    dst2 = dest_root / 'forcing' / category / 'daymet' / 'daymet-lumped' / f\"{basin_id}_daymet_lumped.nc\"\n",
    "    src_files = glob.glob(str(met_folder / 'lumped' / f\"*aymet_*.nc\"))\n",
    "    nc = xr.open_mfdataset(src_files, combine=\"by_coords\")#, decode_times=False)\n",
    "    encoding = generate_encoding(nc, (\"time\",\"hru\"))\n",
    "    nc.to_netcdf(dst2, encoding=encoding)\n",
    "    nc.close()\n",
    "    \n",
    "    # distributed\n",
    "    dst3 = dest_root / 'forcing' / category / 'daymet' / 'daymet-distributed' / f\"{basin_id}_daymet_distributed.nc\"\n",
    "    src_files = glob.glob(str(met_folder / 'distributed' / f\"*aymet_*.nc\"))\n",
    "    nc = xr.open_mfdataset(src_files, combine=\"by_coords\")#, decode_times=False)\n",
    "    encoding = generate_encoding(nc, (\"time\",\"hru\"))\n",
    "    nc.to_netcdf(dst3, encoding=encoding)\n",
    "    nc.close()\n",
    "\n",
    "    # Check\n",
    "    expected_vars = ['dayl','time_bnds','pet','prcp','srad','tmax','tmin','vp']\n",
    "    check_netcdf_compatibility(dst1, dst2, dst3, expected_vars, expected_length)\n",
    "\n",
    "    copied_files.append('forcing-daym-grid')\n",
    "    copied_files.append('forcing-daym-lump')\n",
    "    copied_files.append('forcing-daym-dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f70bc167-7d5b-4519-93e7-893699c7636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b. EM-Earth - \n",
    "# Special treatment: we can't use decode_times=False, and combine=\"by_coords\" at the same time\n",
    "# because every monthly EM-Earth file has a different time encoding: they are all \"hours since\n",
    "# start of month\" and thus, when decoded, are all 0..744 (or something, depending on month).\n",
    "# When combining, this simply overwrites everything. It is probably better to use \n",
    "# combine=\"by_coords\" to ensure we retain the correct temporal order.\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "    # gridded\n",
    "    dst1 = dest_root / 'forcing' / category / 'em-earth' / 'em-earth-gridded' / f\"{basin_id}_em_earth_gridded.nc\"\n",
    "    src_files = glob.glob(str(met_folder / 'raw' / f\"EM*.nc\"))\n",
    "    nc = xr.open_mfdataset(src_files, combine=\"by_coords\")\n",
    "    expected_length = len(nc['time']) # used later to check\n",
    "    nc = fix_nc_encoding(nc, variables=['tmean','prcp'],missing_values=[\n",
    "            nc['tmean'].encoding['missing_value'],nc['prcp'].encoding['missing_value']])\n",
    "    nc = fix_nc_encoding(nc, variables=['tmean','prcp'],missing_values=-9999.0) # just to be sure\n",
    "    encoding = generate_encoding(nc, (\"time\",\"latitude\",\"longitude\"))\n",
    "    nc.to_netcdf(dst1, encoding=encoding)\n",
    "    nc.close()\n",
    "    \n",
    "    # lumped\n",
    "    dst2 = dest_root / 'forcing' / category / 'em-earth' / 'em-earth-lumped' / f\"{basin_id}_em_earth_lumped.nc\"\n",
    "    src_files = glob.glob(str(met_folder / 'lumped' / f\"EM*.nc\"))\n",
    "    nc = xr.open_mfdataset(src_files, combine=\"by_coords\")\n",
    "    encoding = generate_encoding(nc, (\"time\",\"hru\"))\n",
    "    nc.to_netcdf(dst2, encoding=encoding)\n",
    "    nc.close()\n",
    "    \n",
    "    # distributed\n",
    "    dst3 = dest_root / 'forcing' / category / 'em-earth' / 'em-earth-distributed' / f\"{basin_id}_em_earth_distributed.nc\"\n",
    "    src_files = glob.glob(str(met_folder / 'distributed' / f\"EM*.nc\"))\n",
    "    nc = xr.open_mfdataset(src_files, combine=\"by_coords\")\n",
    "    encoding = generate_encoding(nc, (\"time\",\"hru\"))\n",
    "    nc.to_netcdf(dst3, encoding=encoding)\n",
    "    nc.close()\n",
    "\n",
    "    # Check\n",
    "    expected_vars = ['tmean','time_bnds','prcp']\n",
    "    check_netcdf_compatibility(dst1, dst2, dst3, expected_vars, expected_length)\n",
    "\n",
    "    copied_files.append('forcing-emea-grid')\n",
    "    copied_files.append('forcing-emea-lump')\n",
    "    copied_files.append('forcing-emea-dist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a81d9f0-1b92-468a-9dc9-6da2c1de61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3c. ERA5\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "    # gridded\n",
    "    dst1 = dest_root / 'forcing' / category / 'era5' / 'era5-gridded' / f\"{basin_id}_era5_gridded.nc\"\n",
    "    src_files = glob.glob(str(met_folder / 'raw' / f\"ERA5*.nc\"))\n",
    "    src_files = [file for file in src_files if not 'invariant' in file]\n",
    "    nc = xr.open_mfdataset(src_files, combine=\"by_coords\")\n",
    "    expected_length = len(nc['time']) # used later to check\n",
    "    nc = fix_nc_encoding(nc, variables=[\n",
    "        'mper','msdwlwrf','msdwswrf','msnlwrf','msnswrf','mtpr','q','sp','t','u','v'], \n",
    "                    missing_values=-999.0)\n",
    "    encoding = generate_encoding(nc, (\"time\",\"latitude\",\"longitude\"))\n",
    "    nc.to_netcdf(dst1, encoding=encoding)\n",
    "    nc.close()\n",
    "    \n",
    "    # lumped\n",
    "    dst2 = dest_root / 'forcing' / category / 'era5' / 'era5-lumped' / f\"{basin_id}_era5_lumped.nc\"\n",
    "    src_files = glob.glob(str(met_folder / 'lumped' / f\"ERA5*.nc\"))\n",
    "    nc = xr.open_mfdataset(src_files, combine=\"by_coords\")\n",
    "    encoding = generate_encoding(nc, (\"time\",\"hru\"))\n",
    "    nc.to_netcdf(dst2, encoding=encoding)\n",
    "    nc.close()\n",
    "    \n",
    "    # distributed\n",
    "    dst3 = dest_root / 'forcing' / category / 'era5' / 'era5-distributed' / f\"{basin_id}_era5_distributed.nc\"\n",
    "    src_files = glob.glob(str(met_folder / 'distributed' / f\"ERA5*.nc\"))\n",
    "    nc = xr.open_mfdataset(src_files, combine=\"by_coords\")\n",
    "    encoding = generate_encoding(nc, (\"time\",\"hru\"))\n",
    "    nc.to_netcdf(dst3, encoding=encoding)\n",
    "    nc.close()\n",
    "    \n",
    "    # invariant file\n",
    "    dst_file = dest_root / 'forcing' / category / 'era5' / 'era5-invariants' / f\"{basin_id}_era5_invariants.nc\"\n",
    "    src_file = glob.glob(str(met_folder / 'raw' / f\"ERA5*_invariants.nc\"))\n",
    "    assert len(src_file) == 1\n",
    "    shutil.copy(src_file[0], dst_file)\n",
    "\n",
    "    # Check\n",
    "    expected_vars = ['e','mper','msdwlwrf','msdwswrf','msnlwrf','msnswrf','mtpr','q','rh',\n",
    "                     'sp','t','u','v','w','phi','time_bnds']\n",
    "    check_netcdf_compatibility(dst1, dst2, dst3, expected_vars, expected_length)\n",
    "    \n",
    "    copied_files.append('forcing-era5-grid')\n",
    "    copied_files.append('forcing-era5-lump')\n",
    "    copied_files.append('forcing-era5-dist')\n",
    "    copied_files.append('forcing-era5-inva')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "beeac548-e85b-48fe-92af-8e4ea29ee736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d. RDRS\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "    # gridded\n",
    "    dst1 = dest_root / 'forcing' / category / 'rdrs' / 'rdrs-gridded' / f\"{basin_id}_rdrs_gridded.nc\"\n",
    "    src_files = glob.glob(str(met_folder / 'raw' / f\"RDRS*.nc\"))\n",
    "    nc = xr.open_mfdataset(src_files, combine=\"by_coords\")\n",
    "    expected_length = len(nc['time']) # used later to check\n",
    "    encoding = generate_encoding(nc,(\"time\", \"rlat\", \"rlon\"))\n",
    "    nc.to_netcdf(dst1, encoding=encoding)\n",
    "    nc.close()\n",
    "    \n",
    "    # lumped\n",
    "    dst2 = dest_root / 'forcing' / category / 'rdrs' / 'rdrs-lumped' / f\"{basin_id}_rdrs_lumped.nc\"\n",
    "    src_files = glob.glob(str(met_folder / 'lumped' / f\"RDRS*.nc\"))\n",
    "    nc = xr.open_mfdataset(src_files, combine=\"by_coords\")\n",
    "    encoding = generate_encoding(nc,(\"time\", \"hru\"))\n",
    "    nc.to_netcdf(dst2, encoding=encoding)\n",
    "    nc.close()\n",
    "    \n",
    "    # distributed\n",
    "    dst3 = dest_root / 'forcing' / category / 'rdrs' / 'rdrs-distributed' / f\"{basin_id}_rdrs_distributed.nc\"\n",
    "    src_files = glob.glob(str(met_folder / 'distributed' / f\"RDRS*.nc\"))\n",
    "    nc = xr.open_mfdataset(src_files, combine=\"by_coords\")\n",
    "    encoding = generate_encoding(nc,(\"time\", \"hru\"))\n",
    "    nc.to_netcdf(dst3, encoding=encoding)\n",
    "    nc.close()\n",
    "\n",
    "    # Check\n",
    "    expected_vars = ['RDRS_v2.1_P_P0_SFC','RDRS_v2.1_P_TT_1.5m','RDRS_v2.1_P_HU_1.5m',\n",
    "                     'RDRS_v2.1_P_HR_1.5m','RDRS_v2.1_P_UUC_10m','RDRS_v2.1_P_VVC_10m',\n",
    "                     'RDRS_v2.1_P_UVC_10m','RDRS_v2.1_P_FI_SFC','RDRS_v2.1_P_FB_SFC',\n",
    "                     'RDRS_v2.1_P_GZ_SFC','RDRS_v2.1_A_PR0_SFC','e','pet','phi','time_bnds']\n",
    "    check_netcdf_compatibility(dst1, dst2, dst3, expected_vars, expected_length)\n",
    "    \n",
    "    copied_files.append('forcing-rdrs-grid')\n",
    "    copied_files.append('forcing-rdrs-lump')\n",
    "    copied_files.append('forcing-rdrs-dist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c27e347-5644-4783-91db-4a72e2971c80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcd50e28-abbc-433f-8f06-ff0dc142396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Attributes\n",
    "# Source:      /attributes/distributed/attributes_[basin_id].csv\n",
    "#\n",
    "# Destination: /camels-spat-upload/attributes/[category]/[basin_id]_attributes.csv\n",
    "\n",
    "file_src = f\"attributes_{basin_id}.csv\"\n",
    "file_dst = f\"{basin_id}_attributes.csv\"\n",
    "src = att_folder / 'distributed' / file_src\n",
    "dst = dest_root / 'attributes' / category / file_dst\n",
    "shutil.copy(src, dst)\n",
    "copied_files.append('attributes-dist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f4ae2e-0e80-418c-944e-e5d2f8453197",
   "metadata": {},
   "source": [
    "### Geospatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc987ccf-f0ff-4714-8eeb-f3193da0d20c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 5. Geospatial\n",
    "# Source:       /basin_data/[basin_id]/geospatial/forest_height/raw\n",
    "#                                                /glclu2019/raw\n",
    "#                                                /glhymps/raw\n",
    "#                                                /hydrolakes/raw\n",
    "#                                                /lai/raw\n",
    "#                                                /lgrip30/raw\n",
    "#                                                /merit/aspect\n",
    "#                                                /merit/raw\n",
    "#                                                /merit/slope\n",
    "#                                                /modis_land/raw\n",
    "#                                                /pelletier/raw\n",
    "#                                                /soilgrids/raw/bdod\n",
    "#                                                /soilgrids/raw/cfvo\n",
    "#                                                /soilgrids/raw/clay\n",
    "#                                                /soilgrids/raw/conductivity\n",
    "#                                                /soilgrids/raw/porosity\n",
    "#                                                /soilgrids/raw/sand\n",
    "#                                                /soilgrids/raw/silt\n",
    "#                                                /soilgrids/raw/soc\n",
    "#                                                /worldclim/raw/annual\n",
    "#                                                /worldclim/raw/aridity2\n",
    "#                                                /worldclim/raw/fracsnow2\n",
    "#                                                /worldclim/raw/pet\n",
    "#                                                /worldclim/raw/snow2\n",
    "#\n",
    "# Destination: /camels-spat-upload/attributes/[category]/forest_height/[basin_id]\n",
    "\n",
    "# Special treatment: we'll need to make sure we put the basin_id in every single filename.\n",
    "# This will avoid confusion for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8936b9e9-8559-4d53-ada2-5a21499ea2e7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Forest height\n",
    "\n",
    "# General\n",
    "dst_fold = dest_root / 'geospatial' / category / 'forest-height' / basin_id\n",
    "dst_fold.mkdir(exist_ok=True)\n",
    "\n",
    "# 2000\n",
    "file_src = \"forest_height_2000.tif\"\n",
    "file_dst = f\"{basin_id}_forest_height_2000.tif\"\n",
    "src = geo_folder / 'forest_height' / 'raw' / file_src\n",
    "dst = dst_fold / file_dst\n",
    "shutil.copy(src, dst)\n",
    "copied_files.append('geospatial-forest-height-2000')\n",
    "\n",
    "# 2020\n",
    "file_src = \"forest_height_2020.tif\"\n",
    "file_dst = f\"{basin_id}_forest_height_2020.tif\"\n",
    "src = geo_folder / 'forest_height' / 'raw' / file_src\n",
    "dst = dst_fold / file_dst\n",
    "shutil.copy(src, dst)\n",
    "copied_files.append('geospatial-forest-height-2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a10fb716-e06d-4f33-b45b-c01b2beff165",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# GLCLU2019\n",
    "\n",
    "# General\n",
    "dst_fold = dest_root / 'geospatial' / category / 'glclu2019' / basin_id\n",
    "dst_fold.mkdir(exist_ok=True)\n",
    "\n",
    "# Map\n",
    "file_src = \"glclu2019_map.tif\" # NOTE that we switch strata and map on purpose here, so that \n",
    "file_dst = f\"{basin_id}_glclu2019_strata.tif\" # the names match the legend in the excel file\n",
    "src = geo_folder / 'glclu2019' / 'raw' / file_src\n",
    "dst = dst_fold / file_dst\n",
    "shutil.copy(src, dst)\n",
    "copied_files.append('geospatial-glclu2019-strata')\n",
    "\n",
    "# Strata\n",
    "file_src = \"glclu2019_strata.tif\"\n",
    "file_dst = f\"{basin_id}_glclu2019_map.tif\"\n",
    "src = geo_folder / 'glclu2019' / 'raw' / file_src\n",
    "dst = dst_fold / file_dst\n",
    "shutil.copy(src, dst)\n",
    "copied_files.append('geospatial-glclu2019-map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8695d461-c98e-4b83-96ae-03be2b28d091",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# GLHYMPS\n",
    "\n",
    "# General\n",
    "dst_fold = dest_root / 'geospatial' / category / 'glhymps' / basin_id\n",
    "dst_fold.mkdir(exist_ok=True)\n",
    "\n",
    "# Find files\n",
    "src_files = glob.glob(str(geo_folder / 'glhymps' / 'raw' / \"glhymps.*\"))\n",
    "for src_file in src_files:\n",
    "    file = os.path.basename(src_file)\n",
    "    dst_file = dst_fold / f\"{basin_id}_{file}\"\n",
    "    shutil.copy(src_file, dst_file)\n",
    "    if os.path.basename(dst_file) == f\"{basin_id}_glhymps.shp\": copied_files.append('geospatial-glhymps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c9fc3592-8176-46d4-b076-d297e7178fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/software/custom/software/2020/avx2/MPI/gcc9/openmpi4/geo-stack/2022a/lib/python3.10/site-packages/geopandas/io/file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  pd.Int64Index,\n"
     ]
    }
   ],
   "source": [
    "# HydroLAKES\n",
    "\n",
    "# General\n",
    "dst_fold = dest_root / 'geospatial' / category / 'hydrolakes' / basin_id\n",
    "dst_fold.mkdir(exist_ok=True)\n",
    "\n",
    "# Find files\n",
    "src_files = glob.glob(str(geo_folder / 'hydrolakes' / 'raw' / \"HydroLAKES*\"))\n",
    "\n",
    "# Check if we have a .shp file: this suggests we have an actual lake shapefile\n",
    "if any('.shp' in src_file for src_file in src_files):\n",
    "    for src_file in src_files:\n",
    "        file = os.path.basename(src_file)\n",
    "        dst_file = dst_fold / f\"{basin_id}_{file}\"\n",
    "        shutil.copy(src_file, dst_file)\n",
    "        if os.path.basename(dst_file) == f\"{basin_id}_HydroLAKES_polys_v10_NorthAmerica.shp\": copied_files.append('geospatial-hydrolakes')\n",
    "# If we don't have a .shp file, confirm we have a .txt file: \n",
    "# This would ahve been generated if the hydrolakes data has no lake in this basin polygon\n",
    "# In this case, add an empty shapefile. This is likely easier for users (all basins have same file\n",
    "# just some are empty) than mixing file types (some basins have .shp, others have .txt)\n",
    "elif any('.txt' in src_file for src_file in src_files):\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        [{'no_lakes': np.nan, 'geometry': None}],\n",
    "        geometry='geometry',\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    dst_file = dst_fold / f\"{basin_id}_HydroLAKES_polys_v10_NorthAmerica.shp\"\n",
    "    gdf.to_file(dst_file)\n",
    "    copied_files.append('geospatial-hydrolakes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "797984d1-283a-4814-9d69-e91fab1669f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# LAI\n",
    "\n",
    "# General\n",
    "dst_fold = dest_root / 'geospatial' / category / 'lai' / basin_id\n",
    "dst_fold.mkdir(exist_ok=True)\n",
    "\n",
    "# Find files\n",
    "src_files = glob.glob(str(geo_folder / 'lai' / 'raw' / \"*_Lai_500m.tif\"))\n",
    "for src_file in src_files:\n",
    "    file = os.path.basename(src_file)\n",
    "    dst_file = dst_fold / f\"{basin_id}_{file}\"\n",
    "    shutil.copy(src_file, dst_file)\n",
    "    \n",
    "if len(src_files) > 0:\n",
    "    copied_files.append('geospatial-lai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b2625a8-539e-4e9f-b3d7-556ab3441d76",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# LGRIP30\n",
    "\n",
    "# General\n",
    "dst_fold = dest_root / 'geospatial' / category / 'lgrip30' / basin_id\n",
    "dst_fold.mkdir(exist_ok=True)\n",
    "\n",
    "# Main\n",
    "file_src = \"lgrip30_agriculture.tif\"\n",
    "file_dst = f\"{basin_id}_{file_src}\"\n",
    "src = geo_folder / 'lgrip30' / 'raw' / file_src\n",
    "dst = dst_fold / file_dst\n",
    "shutil.copy(src, dst)\n",
    "copied_files.append('geospatial-lgrip30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36876b9f-77a7-4861-83ec-4e80aad70458",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# MERIT\n",
    "\n",
    "# General\n",
    "dst_fold = dest_root / 'geospatial' / category / 'merit' / basin_id\n",
    "dst_fold.mkdir(exist_ok=True)\n",
    "\n",
    "# Aspect\n",
    "file_src = \"merit_hydro_aspect.tif\"\n",
    "file_dst = f\"{basin_id}_{file_src}\"\n",
    "src = geo_folder / 'merit' / 'aspect' / file_src\n",
    "dst = dst_fold / file_dst\n",
    "shutil.copy(src, dst)\n",
    "copied_files.append('geospatial-merit-aspect')\n",
    "\n",
    "# DEM\n",
    "file_src = \"merit_hydro_elv.tif\"\n",
    "file_dst = f\"{basin_id}_{file_src}\"\n",
    "src = geo_folder / 'merit' / 'raw' / file_src\n",
    "dst = dst_fold / file_dst\n",
    "shutil.copy(src, dst)\n",
    "copied_files.append('geospatial-merit-dem')\n",
    "\n",
    "# Slope\n",
    "file_src = \"merit_hydro_slope.tif\"\n",
    "file_dst = f\"{basin_id}_{file_src}\"\n",
    "src = geo_folder / 'merit' / 'slope' / file_src\n",
    "dst = dst_fold / file_dst\n",
    "shutil.copy(src, dst)\n",
    "copied_files.append('geospatial-merit-slope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbeb9792-0b7f-43bb-98cd-d603c9f82d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIS land\n",
    "\n",
    "# General\n",
    "dst_fold = dest_root / 'geospatial' / category / 'modis-land' / basin_id\n",
    "dst_fold.mkdir(exist_ok=True)\n",
    "\n",
    "# Mode file\n",
    "file_src = \"2001_2022_mode_MCD12Q1_LC_Type1.tif\"\n",
    "file_dst = f\"{basin_id}_{file_src}\"\n",
    "src = geo_folder / 'modis_land' / 'raw' / file_src\n",
    "dst = dst_fold / file_dst\n",
    "shutil.copy(src, dst)\n",
    "copied_files.append('geospatial-modis-land-mode')\n",
    "\n",
    "# Annual files\n",
    "for year in range(2001,2023):\n",
    "    file_src = f\"{year}0101_MCD12Q1_LC_Type1.tif\"\n",
    "    file_dst = f\"{basin_id}_{file_src}\"\n",
    "    src = geo_folder / 'modis_land' / 'raw' / file_src\n",
    "    dst = dst_fold / file_dst\n",
    "    shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "835fc7a3-8d46-4f98-b7a9-3118f80a47fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pelletier\n",
    "\n",
    "# General\n",
    "dst_fold = dest_root / 'geospatial' / category / 'pelletier' / basin_id\n",
    "dst_fold.mkdir(exist_ok=True)\n",
    "\n",
    "# Files\n",
    "src_files = ['average_soil_and_sedimentary-deposit_thickness.tif',\n",
    "             'hill-slope_valley-bottom.tif',\n",
    "             'land_cover_mask.tif',\n",
    "             'upland_hill-slope_regolith_thickness.tif',\n",
    "             'upland_hill-slope_soil_thickness.tif',\n",
    "             'upland_valley-bottom_and_lowland_sedimentary_deposit_thickness.tif']\n",
    "for file_src in src_files:\n",
    "    file_dst = f\"{basin_id}_{file_src}\"\n",
    "    src = geo_folder / 'pelletier' / 'raw' / file_src\n",
    "    dst = dst_fold / file_dst\n",
    "    shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ad8bad0-2b50-42ac-8e2d-3b672979a9e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# soilgrids\n",
    "\n",
    "# General\n",
    "dst_fold = dest_root / 'geospatial' / category / 'soilgrids' / basin_id\n",
    "dst_fold.mkdir(exist_ok=True)\n",
    "\n",
    "sub_folders = ['bdod','cfvo','clay','conductivity','porosity','sand','silt','soc']\n",
    "for sub_folder in sub_folders:\n",
    "    src_fold = geo_folder / 'soilgrids' / 'raw' / sub_folder\n",
    "    files_src = glob.glob(str(src_fold/'*.tif'))\n",
    "    for file_src in files_src:\n",
    "        file_name = os.path.basename(file_src)\n",
    "        file_dst = f\"{basin_id}_{file_name}\"\n",
    "        dst = dst_fold / file_dst\n",
    "        shutil.copy(file_src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1ce25f9-14d2-4f7a-995e-40bca933e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WorldClim\n",
    "\n",
    "# General\n",
    "dst_fold = dest_root / 'geospatial' / category / 'worldclim-derived' / basin_id\n",
    "dst_fold.mkdir(exist_ok=True)\n",
    "\n",
    "# annual files\n",
    "src_fold = geo_folder / 'worldclim' / 'raw' / 'annual'\n",
    "files_src = ['aridity.tif','fracsnow.tif','pet_sum.tif','prec_sum.tif',\n",
    "             'seasonality.tif','snow_sum.tif','t_avg.tif']\n",
    "files_dst = [f\"{basin_id}_wc2.1_30s_derived_annual_aridity.tif\",\n",
    "             f\"{basin_id}_wc2.1_30s_derived_annual_snow_fraction.tif\",\n",
    "             f\"{basin_id}_wc2.1_30s_derived_annual_pet_total.tif\",\n",
    "             f\"{basin_id}_wc2.1_30s_derived_annual_prec_total.tif\",\n",
    "             f\"{basin_id}_wc2.1_30s_derived_annual_seasonality.tif\",\n",
    "             f\"{basin_id}_wc2.1_30s_derived_annual_snow_total.tif\",\n",
    "             f\"{basin_id}_wc2.1_30s_derived_annual_mean_temp.tif\"]\n",
    "\n",
    "for file_src,file_dst in zip(files_src,files_dst):\n",
    "    src = src_fold / file_src\n",
    "    dst = dst_fold / file_dst\n",
    "    shutil.copy(src, dst)\n",
    "\n",
    "# Monthly files\n",
    "sub_folders = ['aridity2','fracsnow2','pet','snow2']\n",
    "new_names = ['aridity','snow_fraction','pet_total','snow_total']\n",
    "for sub_folder,new_name in zip(sub_folders,new_names):\n",
    "    src_fold = geo_folder / 'worldclim' / 'raw' / sub_folder\n",
    "    for mm in range(1,13):\n",
    "        file_src = f\"wc2.1_30s_{sub_folder}_{mm:02}.tif\"\n",
    "        file_dst = f\"{basin_id}_wc2.1_30s_derived_{new_name}_month_{mm:02}.tif\"\n",
    "        src = src_fold / file_src\n",
    "        dst = dst_fold / file_dst\n",
    "        shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e25ce-9170-40fc-8a0b-ad24f8bb0558",
   "metadata": {},
   "source": [
    "### Compare actual files with expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8afb22db-fdd5-4df7-97a4-4aec2d7f9267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we missed anything\n",
    "missing_elements = [item for item in expected_files if item not in copied_files]\n",
    "\n",
    "# Assertion with error message\n",
    "assert not missing_elements, f\"{basin_id}: {missing_elements} not successfully copied\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee88aa-dee8-4640-8743-09617a212adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels-spat-env-jupyter",
   "language": "python",
   "name": "camels-spat-env-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
