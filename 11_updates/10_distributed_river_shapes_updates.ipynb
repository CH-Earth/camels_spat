{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e281beec-b69a-45b3-995b-b75f68f8dcad",
   "metadata": {},
   "source": [
    "# Further updates to distributed river shapes\n",
    "Cyril identified minor issues with the `maxup` field in split polygon cases, and `uparea` in the most downstream polygon of every case.\n",
    "\n",
    "We already made changes to certain river shapefiles, and we should have every single one of them in the updates folder. We'll check this and if so proceed with processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4180e639-b5d1-4700-b579-5551e5fbd431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d9c6e5-105a-450d-87d6-338928519d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data location\n",
    "cs_main_folder = Path(\"/scratch/gwf/gwf_cmt/wknoben/camels-spat-upload\")\n",
    "cs_update_folder = Path(\"/scratch/gwf/gwf_cmt/wknoben/camels-spat-upload-updates/\")\n",
    "cs_update_folder2 = Path(\"/scratch/gwf/gwf_cmt/wknoben/camels-spat-upload-updates-2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133506f8-0504-424e-b8ae-014fdea2f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_meta = pd.read_csv(cs_update_folder2 / \"camels-spat-metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98446f-5c85-4170-9166-e26bb6e6700f",
   "metadata": {},
   "source": [
    "## Check if we have what we think we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc1886d9-26a7-4e55-aa3b-1a5d2e641880",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAN_01AK006_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_01AL004_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_01BU009_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_01CD005_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_01EE005_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_01FJ002_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_02BA005_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_02CG003_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_02GA043_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_02YA002_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_02YK008_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_02YL005_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_02ZG001_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_02ZL005_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_02ZM006_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_02ZM016_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_02ZN002_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_05EE006_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_05HG021_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_05LL027_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_07BG004_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_08DB013_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_08EE012_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_08FF006_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_08HA016_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_08HA068_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_08HB048_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_08LE108_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_08NH115_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_08NJ130_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_08NM240_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_08NM241_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_08NM242_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_08OA003_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "CAN_08OA005_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_01118300_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_01139800_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_01195100_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_01434025_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_01466500_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_01484100_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_01542810_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_01594950_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_01658500_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_02038850_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_02096846_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_02102908_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_02381600_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_02384540_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_02464146_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_03026500_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_03049800_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_03357350_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_03450000_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_04213075_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_06408700_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_06614800_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_06746095_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_06879650_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_09034900_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_09035800_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_09047700_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_09066200_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_09066300_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_09378170_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_09378630_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_10172800_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_10259000_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_10336645_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_10336740_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_10348850_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_11180500_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_11180960_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_11475560_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_12073500_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_12147600_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_12375900_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_12383500_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_14138800_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_14138870_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_14138900_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_14303200_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n",
      "USA_14306340_distributed_river.shp only found in old upload. Length = 1; COMID = [None]\n"
     ]
    }
   ],
   "source": [
    "dist_riv_shapes_in_updates = 0  # we expect to have a file for each basin here\n",
    "lump_bas_shapes_in_updates = 0  # we expect to have a file for each basin here\n",
    "\n",
    "for ix,row in cs_meta.iterrows():\n",
    "    scale = row.subset_category\n",
    "    cntry = row.Country\n",
    "    statn = row.Station_id\n",
    "    basin_id = f\"{cntry}_{statn}\"\n",
    "\n",
    "    # First, check if we have a river shapefile in the updates folder \n",
    "    # Note: we know updates2 at this point only contains observation netcdf files\n",
    "    path_middle_dist = f'shapefiles/{scale}/shapes-distributed'\n",
    "    path_middle_lump = f'shapefiles/{scale}/shapes-lumped'\n",
    "    \n",
    "    riv_path = cs_update_folder / path_middle_dist / basin_id / f\"{basin_id}_distributed_river.shp\"\n",
    "    bas_path = cs_update_folder / path_middle_lump / basin_id / f\"{basin_id}_lumped.shp\"\n",
    "\n",
    "    if riv_path.exists():\n",
    "        dist_riv_shapes_in_updates += 1\n",
    "    else:\n",
    "        #print(f\"{riv_path.name} missing in updates folder\")\n",
    "        riv_path = cs_main_folder / path_middle_dist / basin_id / f\"{basin_id}_distributed_river.shp\"\n",
    "        riv = gpd.read_file(riv_path)\n",
    "        print(f\"{riv_path.name} only found in old upload. Length = {len(riv)}; COMID = {riv['COMID'].values}\")\n",
    "    if bas_path.exists():\n",
    "        lump_bas_shapes_in_updates += 1\n",
    "    else:\n",
    "        print(f\"{riv_path.name} missing in updates folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f7088fc-605a-4df9-9c9d-79139a7fe466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1343 in updates folder\n",
      "1426 in updates folder\n"
     ]
    }
   ],
   "source": [
    "print(f\"{dist_riv_shapes_in_updates} in updates folder\")\n",
    "print(f\"{lump_bas_shapes_in_updates} in updates folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff4b00-2451-48dd-8354-a88171cdfe10",
   "metadata": {},
   "source": [
    "Prints above indicate that for the 83 basins missing in the updates folder we have no polygons anyway, so these can be left as is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e9a6c-efec-429c-bfc4-d8f3601c4fbc",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9ceb44d6-61ba-4e67-a6d1-425f15ca52d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_nextdownid_and_up(basin_id, gdf, comid, new_ndid):\n",
    "\n",
    "    '''Fixes connectivity in a number of specific cases where we have back-to-back nested polygons'''\n",
    "    print(f\"Applying manual update to {basin_id}\")\n",
    "\n",
    "    # First fix the NextDownID\n",
    "    mask = river['COMID'] == comid\n",
    "    river.at[river.index[mask][0], 'NextDownID'] = new_ndid\n",
    "    assert river.loc[mask, 'NextDownID'].iloc[0] == new_ndid, \"Update failed: NextDownID was not set correctly\"\n",
    "    print(f\" - NextDownID update to {new_ndid} complete\")\n",
    "\n",
    "    # Now check if this downstream COMID already has the correct up ID in one of its up columns\n",
    "    mask = river['COMID'] == new_ndid\n",
    "    assert mask.sum() == 1, f\"Expected to identify 1 polygon with COMID {new_ndid}, but found {mask.sum()}\"\n",
    "    if comid in river.loc[mask, ['up1','up2','up3','up4']].values:\n",
    "        print(f\" - Upstream COMID {comid} already found in up values {river.loc[mask, ['up1','up2','up3','up4']].values}\")\n",
    "    else:\n",
    "        #print(f\" - COMID {comid} not already present in any upstream values: {river.loc[mask, ['up1','up2','up3','up4']].values[0]}. Trying to identify old un-split COMID\")\n",
    "        up_mask = np.floor(comid) == river.loc[mask, ['up1','up2','up3','up4']].values[0]\n",
    "        if up_mask.sum() == 1:\n",
    "            \n",
    "            # identify the correct up column\n",
    "            up_name = f\"up{np.where(up_mask)[0][0]+1}\"\n",
    "            #print(f\" - Original COMID {np.floor(comid)} found in up values {river.loc[mask, ['up1','up2','up3','up4']].values[0]}. Replacing {up_name} with {comid}\")\n",
    "            assert (float(comid) - float(river.loc[mask, up_name].iloc[0]) > 0) & (float(comid) - float(river.loc[mask, up_name].iloc[0]) < 1), f\"COMID = {comid}; current up = {float(river.loc[mask, up_name].iloc[0])}\"\n",
    "\n",
    "            # get the index of this row so we can use .at[]\n",
    "            river.at[river.index[mask][0], up_name] = comid\n",
    "            assert river.loc[mask, up_name].iloc[0] == comid, f\"Update failed: {up_name} was not set correctly\"\n",
    "            print(f\" - {up_name} update to {comid} complete\")\n",
    "        else:\n",
    "            # COMID not found, or more than once\n",
    "            print(f\"Original COMID {np.floor(comid)} not uniquely identified in up values {river.loc[mask, ['up1','up2','up3','up4']].values[0]}. Check manually.\")\n",
    "\n",
    "    return river"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae8abce-2153-411b-bccb-4153e8568b54",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_nextdownid(basin_id, gdf, comid, new_ndid):\n",
    "    print(f\"Applying manual update to {basin_id}\")\n",
    "    river.loc[river['COMID'] == comid, 'NextDownID'] = new_ndid\n",
    "    assert river.loc[river['COMID'] == comid, 'NextDownID'].iloc[0] == new_ndid, \"Update failed: NextDownID was not set correctly\"\n",
    "    return river"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7765e1b6-c7aa-47fd-9aae-818226aadefa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_downstream_segment(gdf):\n",
    "    possibly_downstream = []\n",
    "    for ix, row in gdf.iterrows():\n",
    "        nextDownID = row['NextDownID']\n",
    "        if not nextDownID in gdf['COMID'].values:\n",
    "            possibly_downstream.append(row['COMID'])\n",
    "    return possibly_downstream        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "311b3027-5709-44d9-b976-1f1a4380ed04",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def add_missing_segment(basin_id, river, basin):\n",
    "    '''Adds a river segment to the river shapefile in cases where we had a small nested basin that didn't have its own river segment.\n",
    "       This happens in cases where the nested basin is either off the side of the main MERIT river network, or upstream of where the\n",
    "       MERIT network begins to be delineated.\n",
    "       \n",
    "       Based on a dry run of the main loop we found 12 cases where the basin shapefile contains exactly 1 entry more than the river,\n",
    "       and 1 case where the basin shapefile contains 7 extra entries. This function adds that missing row to the river shapefile.\n",
    "       \n",
    "       Summary based on visual inspection:\n",
    "       - CAN_01AL002:  nested gauge to the side of the main river stem; nested gauge is part of CAMELS-SPAT (CAN_01AL004)\n",
    "       - CAN_01BU002:  nested gauge upstream of where a MERIT river starts; nested gauge is part of CAMELS-SPAT (CAN_01BU009)\n",
    "       - CAN_02BF001:  ! nested gauges to upstream & sideways of where a MERIT river start; none are part of CAMELS-SPAT because no sub-daily streamflow was available\n",
    "                         (CAN_02BF005, CAN_02BF006, CAN_02BF007, CAN_02BF008, CAN_02BF009, CAN_02BF012, CAN_02BF0013)\n",
    "       - CAN_02GA010:  nested gauge to the side of the main river stem, nested gauge is part of CAMELS-SPAT (CAN_02GA043)\n",
    "       - CAN_05DA007:  ! nested gauge upstream of where a MERIT river starts; nested gauge is not part of CAMELS-SPAT because no sub-daily streamflow was available (CAN_05DA010)\n",
    "       - CAN_05DA009:  same case as CAN_05DA007, just for a bigger basin that includes CAN_05DA007\n",
    "       - CAN_05EE009:  nested gauge to the side of the main river stem, nested gauge is part of CAMELS-SPAT (CAN_05EE006)\n",
    "       - CAN_05LL015:  nested gauge upstream of where a MERIT river starts; nested gauge is part of CAMELS-SPAT (CAN_05LL027)\n",
    "       - CAN_08HA010:  nested gauge to the side of the main river stem, nested gauge is part of CAMELS-SPAT (CAN_08HA068)\n",
    "       - USA_01435000: nested gauge upstream of where a MERIT river starts; nested gauge is part of CAMELS-SPAT (USA_01434025)\n",
    "       - USA_01543000: nested gauge upstream of where a MERIT river starts; nested gauge is part of CAMELS-SPAT (USA_01542810)\n",
    "       - USA_01543500: same case as USA_01543000, just for a bigger basin that includes USA_01543000\n",
    "       - USA_14306500: nested gauge upstream of where a MERIT river starts; nested gauge is part of CAMELS-SPAT (USA_14306340)\n",
    "       '''\n",
    "\n",
    "    # Check we're looking at a known case\n",
    "    if not basin_id in ['CAN_01AL002',  'CAN_01BU002',  'CAN_02BF001', 'CAN_02GA010', 'CAN_05DA007',\n",
    "                        'CAN_05DA009',  'CAN_05EE009',  'CAN_05LL015', 'CAN_08HA010', 'USA_01435000',\n",
    "                        'USA_01543000', 'USA_01543500', 'USA_14306500']:\n",
    "        print(f\"add_missing_segment(): case {basin_id} not implemented. Early exit.\")\n",
    "        return\n",
    "\n",
    "    if basin_id == 'CAN_02BF001':  # case with 7 missing fields; simpler to just add the stuff manually\n",
    "\n",
    "        uparea = [                                                        # COMID:\n",
    "            basin[basin['COMID'] == 72044865.2]['unitarea'].values[0] +      # 72044865.2 (itself, and everything else upstream)\n",
    "                basin[basin['COMID'] == 72044865.3]['unitarea'].values[0] +      \n",
    "                basin[basin['COMID'] == 72044865.4]['unitarea'].values[0] +\n",
    "                basin[basin['COMID'] == 72044865.5]['unitarea'].values[0] +\n",
    "                basin[basin['COMID'] == 72044865.6]['unitarea'].values[0] +      \n",
    "                basin[basin['COMID'] == 72044865.7]['unitarea'].values[0] +  \n",
    "                basin[basin['COMID'] == 72044865.8]['unitarea'].values[0],\n",
    "            basin[basin['COMID'] == 72044865.3]['unitarea'].values[0] +      # 72044865.3 (itself + upstream .4, .5, .6, .7, .8)\n",
    "                basin[basin['COMID'] == 72044865.4]['unitarea'].values[0] +\n",
    "                basin[basin['COMID'] == 72044865.5]['unitarea'].values[0] +\n",
    "                basin[basin['COMID'] == 72044865.6]['unitarea'].values[0] +      \n",
    "                basin[basin['COMID'] == 72044865.7]['unitarea'].values[0] +  \n",
    "                basin[basin['COMID'] == 72044865.8]['unitarea'].values[0],\n",
    "            basin[basin['COMID'] == 72044865.4]['unitarea'].values[0],       # 72044865.4 (just itself, has no upstream)\n",
    "            basin[basin['COMID'] == 72044865.5]['unitarea'].values[0] +      # 72044865.5 (itself + upstream .6, .7, .8\n",
    "                basin[basin['COMID'] == 72044865.6]['unitarea'].values[0] +      \n",
    "                basin[basin['COMID'] == 72044865.7]['unitarea'].values[0] +  \n",
    "                basin[basin['COMID'] == 72044865.8]['unitarea'].values[0],\n",
    "            basin[basin['COMID'] == 72044865.6]['unitarea'].values[0] +      # 72044865.6 (itself + upstream .7, .8)\n",
    "                basin[basin['COMID'] == 72044865.7]['unitarea'].values[0] +  \n",
    "                basin[basin['COMID'] == 72044865.8]['unitarea'].values[0],   \n",
    "            basin[basin['COMID'] == 72044865.7]['unitarea'].values[0] +      # 72044865.7 (itself + upstream .8)\n",
    "                basin[basin['COMID'] == 72044865.7]['unitarea'].values[0],   \n",
    "            basin[basin['COMID'] == 72044865.8]['unitarea'].values[0],       # 72044865.8 (just itself, has no upstream)\n",
    "        ]\n",
    "    \n",
    "        new_segments = {'COMID'     : [72044865.2, 72044865.3, 72044865.4, 72044865.5, 72044865.6, 72044865.7, 72044865.8],\n",
    "                        'NextDownID': [72044865.1, 72044865.2, 72044865.3, 72044865.3, 72044865.5, 72044865.6, 72044865.7],\n",
    "                        'up1'       : [72044865.3, 72044865.4, 0,          72044865.6, 72044865.7, 72044865.8, 0         ],\n",
    "                        'up2'       : [0,          72044865.5, 0,          0,          0,          0,          0         ],\n",
    "                        'up3'       : [0,          0,          0,          0,          0,          0,          0         ],\n",
    "                        'up4'       : [0,          0,          0,          0,          0,          0,          0         ],\n",
    "                        'new_len_km': [np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan    ],\n",
    "                        'slope'     : [np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan    ],\n",
    "                        'uparea'    : uparea,\n",
    "                        'lengthdir' : [np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan    ],\n",
    "                        'sinuosity' : [np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan    ],\n",
    "                        'order'     : [np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan    ],\n",
    "                        'strmDrop_t': [np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan    ],\n",
    "                        'slope_taud': [np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan    ],\n",
    "                        'maxup'     : [np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan    ],\n",
    "                        'geometry'  : [np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan,     np.nan    ]\n",
    "              }\n",
    "\n",
    "        river = pd.concat([river, pd.DataFrame(new_segments)], ignore_index=True)\n",
    "\n",
    "    else:  # case with 1 missing field\n",
    "    \n",
    "        # Find the comid that's in the basin but not the river\n",
    "        b_ids = basin['COMID']\n",
    "        r_ids = river['COMID']\n",
    "        missing = b_ids[~b_ids.isin(r_ids)]\n",
    "        assert len(missing) == 1, f\"unexpected different number of missing basins in river file (expected 1, found {len(missing)}\"\n",
    "    \n",
    "        # Find the downstream COMID (i.e., the one that's pointing to this as its upstream)\n",
    "        mask = river[['up1', 'up2', 'up3', 'up4']].isin([missing.iloc[0]]).any(axis=1)\n",
    "        assert mask.sum() == 1, f\"expected to identify 1 COMID that has our current one as upstream, but found {len(mask)}\"\n",
    "    \n",
    "        # Check we have a basin area\n",
    "        uparea = basin[basin['COMID'] == missing.iloc[0]]['unitarea']\n",
    "        assert len(uparea) == 1, f\"expected to identify 1 COMID to use for upstream area, but found {len(uparea)}\"\n",
    "    \n",
    "        # Update the geodataframe\n",
    "        new_segment = {'COMID'     : missing.iloc[0],\n",
    "                       'NextDownID': river[mask]['COMID'].iloc[0],\n",
    "                       'up1'       : 0.0,\n",
    "                       'up2'       : 0.0,\n",
    "                       'up3'       : 0.0,\n",
    "                       'up4'       : 0.0,\n",
    "                       'new_len_km': np.nan,\n",
    "                       'slope'     : np.nan,\n",
    "                       'uparea'    : uparea.iloc[0],\n",
    "                       'lengthdir' : np.nan,\n",
    "                       'sinuosity' : np.nan,\n",
    "                       'order'     : np.nan,\n",
    "                       'strmDrop_t': np.nan,\n",
    "                       'slope_taud': np.nan,\n",
    "                       'maxup'     : np.nan,\n",
    "                       'geometry'  : np.nan\n",
    "                      }\n",
    "        river = pd.concat([river, pd.DataFrame([new_segment])], ignore_index=True)\n",
    "    \n",
    "        # Check we have the same values in either geodataframe now\n",
    "        b_ids = basin['COMID']\n",
    "        r_ids = river['COMID']\n",
    "        missing = b_ids[~b_ids.isin(r_ids)]\n",
    "        assert len(missing) == 0, f\"still missing a basin: {missing}\"\n",
    "\n",
    "    return river"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "9fb44d48-4734-4dde-bc9b-455674aba034",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Create the reverse lookup graph: COMID -> list of upstream COMIDs\n",
    "def build_upstream_graph(river):\n",
    "    graph = defaultdict(list)\n",
    "    upstream_cols = ['up1', 'up2', 'up3', 'up4']\n",
    "    \n",
    "    for _, row in river.iterrows():\n",
    "        current = row['COMID']\n",
    "        upstreams = pd.to_numeric(row[upstream_cols]).dropna()\n",
    "        for up in upstreams:\n",
    "            graph[current].append(up)\n",
    "    return graph\n",
    "\n",
    "# Step 2: Traverse upstream recursively (DFS) to collect all upstream COMIDs\n",
    "def find_all_upstreams(graph, start, visited=None):\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    for upstream in graph.get(start, []):\n",
    "        if upstream not in visited:\n",
    "            visited.add(upstream)\n",
    "            find_all_upstreams(graph, upstream, visited)\n",
    "    return visited\n",
    "\n",
    "# Step 3: Build the full lookup table for all COMIDs\n",
    "def build_upstream_lookup(river):\n",
    "    graph = build_upstream_graph(river)\n",
    "    lookup = dict()\n",
    "    for comid in river['COMID']:\n",
    "        lookup[comid] = list(find_all_upstreams(graph, comid))\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "18f96de1-7627-410a-950a-c69a294b9c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a temporary plotting dir\n",
    "plot_dir = Path('./img')\n",
    "plot_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "4bf3212b-7fd4-4cd9-a1b4-c95243ce35a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying manual update to CAN_02ED003\n",
      " - NextDownID update to 72051937.2 complete\n",
      " - COMID 72052405.1 not already present in any upstream values: [72052405. 72053278.        0.        0.]. Trying to identify old un-split COMID\n",
      " - Original COMID 72052405.0 found in up values [72052405. 72053278.        0.        0.]. Replacing up1 with 72052405.1\n",
      " - up1 update to 72052405.1 complete\n",
      "Applying manual update to CAN_02ED101\n",
      " - NextDownID update to 72051937.2 complete\n",
      " - COMID 72052405.1 not already present in any upstream values: [72052405. 72053278.        0.        0.]. Trying to identify old un-split COMID\n",
      " - Original COMID 72052405.0 found in up values [72052405. 72053278.        0.        0.]. Replacing up1 with 72052405.1\n",
      " - up1 update to 72052405.1 complete\n",
      "Applying manual update to CAN_05DA007\n",
      " - NextDownID update to 71029794.2 complete\n",
      " - COMID 71032880.1 not already present in any upstream values: [71029920. 71032880.        0.        0.]. Trying to identify old un-split COMID\n",
      " - Original COMID 71032880.0 found in up values [71029920. 71032880.        0.        0.]. Replacing up2 with 71032880.1\n",
      " - up2 update to 71032880.1 complete\n",
      "Applying manual update to CAN_05DA009\n",
      " - NextDownID update to 71029794.2 complete\n",
      " - COMID 71032880.1 not already present in any upstream values: [71029920. 71032880.        0.        0.]. Trying to identify old un-split COMID\n",
      " - Original COMID 71032880.0 found in up values [71029920. 71032880.        0.        0.]. Replacing up2 with 71032880.1\n",
      " - up2 update to 71032880.1 complete\n",
      "Applying manual update to CAN_08EE003\n",
      " - NextDownID update to 78004211.2 complete\n",
      " - COMID 78004244.1 not already present in any upstream values: [78004216. 78004244.        0.        0.]. Trying to identify old un-split COMID\n",
      " - Original COMID 78004244.0 found in up values [78004216. 78004244.        0.        0.]. Replacing up2 with 78004244.1\n",
      " - up2 update to 78004244.1 complete\n",
      "Applying manual update to CAN_08EE004\n",
      " - NextDownID update to 78004211.2 complete\n",
      " - COMID 78004244.1 not already present in any upstream values: [78004216. 78004244.        0.        0.]. Trying to identify old un-split COMID\n",
      " - Original COMID 78004244.0 found in up values [78004216. 78004244.        0.        0.]. Replacing up2 with 78004244.1\n",
      " - up2 update to 78004244.1 complete\n",
      "Applying manual update to CAN_08LD001\n",
      " - NextDownID update to 78010204.2 complete\n",
      " - COMID 78010211.1 not already present in any upstream values: [78010211. 78010355.        0.        0.]. Trying to identify old un-split COMID\n",
      " - Original COMID 78010211.0 found in up values [78010211. 78010355.        0.        0.]. Replacing up1 with 78010211.1\n",
      " - up1 update to 78010211.1 complete\n"
     ]
    }
   ],
   "source": [
    "uparea_diffs = 0\n",
    "uparea_updates = []\n",
    "any_maxup_updated = 0\n",
    "\n",
    "for ix,row in cs_meta.iterrows():\n",
    "    scale = row.subset_category\n",
    "    cntry = row.Country\n",
    "    statn = row.Station_id\n",
    "    basin_id = f\"{cntry}_{statn}\"\n",
    "\n",
    "    # Paths\n",
    "    path_middle_dist = f'shapefiles/{scale}/shapes-distributed/{basin_id}'\n",
    "    path_middle_lump = f'shapefiles/{scale}/shapes-lumped/{basin_id}'\n",
    "    \n",
    "    riv_path = cs_update_folder / path_middle_dist / f\"{basin_id}_distributed_river.shp\"\n",
    "    bas_path = cs_main_folder   / path_middle_dist / f\"{basin_id}_distributed_basin.shp\"\n",
    "    lum_path = cs_update_folder / path_middle_lump / f\"{basin_id}_lumped.shp\"\n",
    "\n",
    "    if riv_path.exists():  # i.e., if we have a file in the updates folder, and thus an actual polygon for this place\n",
    "        \n",
    "        # Load the shapefiles\n",
    "        river = gpd.read_file(riv_path)\n",
    "        basin = gpd.read_file(bas_path)\n",
    "        lumped= gpd.read_file(lum_path)\n",
    "\n",
    "        # --- CONNECTIVITY FIXES ---\n",
    "        # For 7 specific cases, apply a manual fix to niche cases where two split polygons are directly upstream/downstream\n",
    "        #  from one another, and the general code did not handle this correctly\n",
    "        if basin_id == 'CAN_02ED003':  # Split polygon 72052405.1 currently has NextDownID 72051937.0 but this should be 72051937.2\n",
    "            river = update_nextdownid_and_up(basin_id, river, 72052405.1, 72051937.2)  \n",
    "        elif basin_id == 'CAN_02ED101':  # Exact case of CAN_02ED003, but smaller basin (i.e., the other one includes this one)\n",
    "            river = update_nextdownid_and_up(basin_id, river, 72052405.1, 72051937.2)\n",
    "        elif basin_id == 'CAN_05DA007':  # Split polygon 71032880.1 currently has NextDownID 71029794.0 but this should be 71029794.2\n",
    "            river = update_nextdownid_and_up(basin_id, river, 71032880.1, 71029794.2)\n",
    "        elif basin_id == 'CAN_05DA009':  # Exact case of CAN_05DA007, but larger basin (i.e., this one includes the other)\n",
    "            river = update_nextdownid_and_up(basin_id, river, 71032880.1, 71029794.2)\n",
    "        elif basin_id == 'CAN_08EE003':  # Split polygon 78004244.1 currently has NextDownID 78004211.0 but this should be 78004211.2\n",
    "            river = update_nextdownid_and_up(basin_id, river, 78004244.1, 78004211.2)\n",
    "        elif basin_id == 'CAN_08EE004':  # Exact case of CAN_05DA007, but larger basin (i.e., this one includes the other)\n",
    "            river = update_nextdownid_and_up(basin_id, river, 78004244.1, 78004211.2)\n",
    "        elif basin_id == 'CAN_08LD001':  # Split polygon 78010211.1 currently has NextDownID 78010204.0 but this should be 78010204.2\n",
    "            river = update_nextdownid_and_up(basin_id, river, 78010211.1, 78010204.2)\n",
    "\n",
    "        # For 13 basins, apply a fix to account for (a) missing stream segment(s)\n",
    "        if len(river) != len(basin):\n",
    "            #print(f\"{scale}/{basin_id}: number of subbasins ({len(basin)}) does not match number of stream segments ({len(river)})\")  # used this to identify the cases we check for in the function\n",
    "            river = add_missing_segment(basin_id, river, basin)\n",
    "\n",
    "        # --- UPAREA ---\n",
    "        # Identify a new best guess at upstream areas and compare to what we have. Flag cases where differences are large and create a plot for those\n",
    "        # Build the network table\n",
    "        upstream_lookup = build_upstream_lookup(river)\n",
    "        upstream_df = pd.DataFrame([\n",
    "            {'COMID': k, 'all_upstream': v}\n",
    "            for k, v in upstream_lookup.items()\n",
    "        ])\n",
    "        \n",
    "        # Re-calculate upstream areas for each COMID\n",
    "        comids = []\n",
    "        uparea = []\n",
    "        for ix, row in upstream_df.iterrows():\n",
    "            comid = row['COMID']\n",
    "            comids.append(comid)\n",
    "            sum_ids = row['all_upstream'] + [comid]\n",
    "            uparea.append(basin[basin['COMID'].isin(sum_ids)]['unitarea'].sum())\n",
    "        check = pd.DataFrame(data={'COMID': comids,\n",
    "                                   'uparea': uparea})\n",
    "        \n",
    "        # Combine with original river dataframe\n",
    "        merged_df = river.merge(check, on='COMID', suffixes=('_org','_new'))\n",
    "        merged_df['uparea_org'] = pd.to_numeric(merged_df['uparea_org'])\n",
    "        merged_df['area_diff'] = np.where(abs(merged_df['uparea_org'] - merged_df['uparea_new']) > 1e-3,  # if any area differences larger than 0.001 km^2\n",
    "                                          merged_df['uparea_org'] - merged_df['uparea_new'],  # track the actual area difference\n",
    "                                          np.nan)  # otherwise put nan\n",
    "        \n",
    "        # Check if we have any large-ish discrepencies. If so, increment counter and plot\n",
    "        if any(merged_df['area_diff'].notna()):\n",
    "            uparea_diffs += 1\n",
    "\n",
    "            # Create a plot - NOTE: I check these during a dry run and it all seems to make sense\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")  # shapely deprecation warning\n",
    "                fig, ax = plt.subplots(1,3, figsize=(20,5))\n",
    "            \n",
    "                merged_df.plot(ax=ax[0],column='uparea_org',legend=True)\n",
    "                ax[0].set_title('old')\n",
    "                \n",
    "                merged_df.plot(ax=ax[1], color='lightgrey')\n",
    "                merged_df.plot(ax=ax[1],\n",
    "                               column='area_diff',\n",
    "                               legend=True,\n",
    "                            )\n",
    "                ax[1].set_title(f'{basin_id}: upstream area diff [km^2]')\n",
    "            \n",
    "                merged_df.plot(ax=ax[2],column='uparea_new',legend=True)\n",
    "                ax[2].set_title('new')\n",
    "                \n",
    "                plt.savefig(plot_dir/f\"{basin_id}_uparea_diffs.png\", dpi=100)\n",
    "                plt.close()\n",
    "\n",
    "            # --- Update the river geodataframe\n",
    "            # Make a selection of what to update\n",
    "            update_threshold = 1e-3 # km^2\n",
    "            update_this = merged_df[abs(merged_df['area_diff']) > update_threshold][['COMID','uparea_new', 'area_diff']].copy().reset_index(drop=True)\n",
    "            update_this['basin'] = basin_id\n",
    "            uparea_updates.append(update_this)  # save to csv later so we have a record of changes\n",
    "            \n",
    "            # Set indices and update\n",
    "            river_tmp = river.copy()\n",
    "            river_tmp = river_tmp.set_index('COMID')\n",
    "            update_this = update_this.set_index('COMID')\n",
    "            river_tmp['uparea'].update(update_this['uparea_new'])\n",
    "            \n",
    "            # Check that we did in fact update something\n",
    "            assert len(river_tmp) == len(river), f\"length of river dataframe changed during update (was {len(river)}; now {len(river_tmp)})\"\n",
    "            assert any(river_tmp['uparea'].values != river['uparea'].values), \"no changed values found\"\n",
    "            \n",
    "            # Re-assign the updated shapefile to 'river'\n",
    "            river = river_tmp.reset_index().copy()\n",
    "\n",
    "        # --- MAXUP ---\n",
    "        has_up = (river[['up1','up2','up3','up4']] > 0.0).sum(axis=1)  # check if this row has any upstream segments defined\n",
    "        up_sum = np.where(has_up, has_up, 0)  # count how many up segments each row has, and put in 0 if it hasn't got any (should account for NaN)\n",
    "        if any(river['maxup'] != up_sum):\n",
    "            any_maxup_updated += 1\n",
    "            river['maxup'] = np.where(river['maxup'] != up_sum, up_sum, river['maxup'])  # replace existing maxup with new where different\n",
    "\n",
    "        # --- save\n",
    "        # we'll save the files regardless of updates, and just upload the whole thing\n",
    "        des_fold = cs_update_folder2 / path_middle_dist\n",
    "        des_fold.mkdir(exist_ok=True, parents=True)\n",
    "        des_path = des_fold / f\"{basin_id}_distributed_river.shp\"\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")  # geopandas FutureWarning\n",
    "            river.to_file(des_path)\n",
    "\n",
    "# save the uparea updates to CSV\n",
    "uparea_updates_df = pd.concat(uparea_updates)\n",
    "uparea_updates_df = uparea_updates_df[['basin','COMID','uparea_new','area_diff']]\n",
    "uparea_updates_df.to_csv('river_updates_upareas.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "8b3f3064-2e70-48a7-9531-30dd5a595007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1039, 389)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uparea_diffs, any_maxup_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace1d162-fc04-42cc-beb4-2b6418d97e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels-spat-env-jupyter",
   "language": "python",
   "name": "camels-spat-env-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
