{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f77245-0165-4478-9061-e9815c6bbe29",
   "metadata": {},
   "source": [
    "# Move updated flow files\n",
    "Apparently the current camels-spat resource contains older netcdf files with flow data, that have timesteps in UTC and also use older hourly averaging code. The new files lived on my laptop and we're now moving these into the update folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9eb025b0-0ab1-43df-89ec-d829fec164dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import xarray as xr\n",
    "\n",
    "import netCDF4 as nc4\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(str(Path().absolute().parent))\n",
    "import python_cs_functions as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e48b7948-b6b9-48cb-a7f8-38c7a7cd456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data location\n",
    "cs_main_folder = Path(\"/scratch/gwf/gwf_cmt/wknoben/camels-spat-upload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794d2530-eb69-4259-8994-d9ff1537ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination location\n",
    "cs_update_folder = Path(\"/scratch/gwf/gwf_cmt/wknoben/camels-spat-upload-updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7617ec69-30a1-46e3-bd6e-6d2c99b94bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer folder\n",
    "cs_transfer_folder = Path(\"/scratch/gwf/gwf_cmt/wknoben/TEMP_flow_transfers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b5642ec-c6db-46ae-8c63-7e193af2930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer folder\n",
    "cs_working_folder = Path(\"/scratch/gwf/gwf_cmt/wknoben/camels-spat-wip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db6ffa00-36c1-4840-afde-9e027d106b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder structure\n",
    "obs_path_part1 = \"observations\"\n",
    "obs_path_parts2 = [\"headwater\", \"macro-scale\", \"meso-scale\"]\n",
    "obs_path_parts3 = [\"obs-hourly\",\"obs-daily\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a15e51-92aa-4deb-99bd-d98badb01018",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe077c92-c491-45c9-bd33-6aea80d2cb9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def confirm_netcdf_basics(ds, country, basin, tz_expected):\n",
    "\n",
    "    assert ds.country == country, \"attribute mismatch: country\"\n",
    "    assert ds.station == basin, \"attribute mismatch: basin\"\n",
    "\n",
    "    expected_vars = ['q_obs_data_quality', 'q_obs', 'time_bnds']\n",
    "    assert set(list(ds.data_vars.keys())).issuperset(expected_vars), \"key variable missing\"\n",
    "    \n",
    "    expected_dims = ['time', 'nbnds']\n",
    "    assert set(list(ds.dims.keys())) == set(expected_dims), \"dimensions mismatch\"\n",
    "\n",
    "    tz_actual = ds['time_bnds'].time_zone\n",
    "    if tz_expected == 'NST':\n",
    "        if tz_actual != 'NST':\n",
    "            print(f\"Basin {basin}: expected timezone {tz_expected} but found {tz_actual}\")\n",
    "    else:\n",
    "        assert tz_actual == tz_expected, f\"time zone expected as {tz_expected} but is {tz_actual}\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c44b04b5-5ff9-4757-9066-c63ff9160812",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compare_netcdf_sizes_and_vars(ds1, ds2):\n",
    "    assert set(ds1.data_vars.keys()) == set(ds2.data_vars.keys()), \"old/new mismatch: variables\"\n",
    "    if abs(len(ds1['time']) - len(ds2['time'])) > 1: # allow a 1 timestep difference\n",
    "        assert len(ds1['time']) == len(ds2['time']), \"length mismatch: time\"\n",
    "    if abs(len(ds1['q_obs']) - len(ds2['q_obs'])) > 1:\n",
    "        assert len(ds1['q_obs']) == len(ds2['q_obs']), \"length mismatch: qobs\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe5b7830-3c52-4eed-a4a1-74b31cfb577a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compare_daily_netcdfs(ds1,ds2,basin):\n",
    "    assert np.allclose(ds1['q_obs'].values, ds2['q_obs'].values,equal_nan=True)\n",
    "    if not ds1['q_obs'].identical(ds2['q_obs']):\n",
    "        print(f\"Basin {basin}: flows allclose() but not identical()\")\n",
    "\n",
    "    # check that time periods are roughly the same\n",
    "    common_times = np.intersect1d(ds1['time'].values, ds2['time'].values)\n",
    "    if abs(len(common_times) / len(ds1['time']) - 1) > 0.01:\n",
    "        print(f\"Basin {basin}: old and new hourly timesteps don't overlap for more than 1% of timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ca123f5-a238-4bc7-b6ad-1288701946ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compare_hourly_netcdfs(ds1,ds2,basin):\n",
    "    \n",
    "    # we know these won't be the same so check if statistics are roughly similar and warn if not\n",
    "    mean_flow_diff = (ds1['q_obs'].mean() / ds2['q_obs'].mean() - 1).values\n",
    "    if abs(mean_flow_diff) > 0.01:\n",
    "        print(f\"Basin {basin}: old and new hourly mean flow more than 1% different ({mean_flow_diff:4f})\")\n",
    "\n",
    "    std_flow_diff = (ds1['q_obs'].std() / ds2['q_obs'].std() - 1).values\n",
    "    if abs(std_flow_diff) > 0.01:\n",
    "        print(f\"Basin {basin}: old and new hourly standard deviation of flow more than 1% different ({std_flow_diff:4f})\")\n",
    "\n",
    "    corr_flow_diff = pd.Series(ds1['q_obs']).corr(pd.Series(ds2['q_obs']))-1\n",
    "    if abs(corr_flow_diff) > 0.01:\n",
    "        print(f\"Basin {basin}: old and new hourly correlation of flow more than 1% different ({corr_flow_diff:4f})\")\n",
    "    \n",
    "    # check that time periods are roughly the same\n",
    "    common_times = np.intersect1d(ds1['time'].values, ds2['time'].values)\n",
    "    if abs(len(common_times) / len(ds1['time']) - 1) > 0.01:\n",
    "        print(f\"Basin {basin}: old and new hourly timesteps don't overlap for more than 1% of timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d9c8cd-ab3f-405a-83c2-6541762b8bb7",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c32e5451-1799-410f-b56e-8d6e62e01044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the meta-data, so we known which basins we have\n",
    "cs_meta = pd.read_csv(cs_main_folder / \"camels-spat-metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a9a6004-b4fc-411b-8873-4614c29fa8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all new files\n",
    "new_files = list(cs_transfer_folder.glob(\"*.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d9a04b2-597d-4f1e-8cf5-7bed08ef9f11",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basin 02038850: old and new hourly correlation of flow more than 1% different (-0.013663)\n",
      "Basin 10259200: old and new hourly correlation of flow more than 1% different (-0.012054)\n",
      "Basin 06879650: old and new hourly correlation of flow more than 1% different (-0.126134)\n",
      "Basin 09508300: old and new hourly correlation of flow more than 1% different (-0.010310)\n",
      "Basin 03368000: old and new hourly correlation of flow more than 1% different (-0.016509)\n",
      "Basin 11180960: old and new hourly correlation of flow more than 1% different (-0.015299)\n",
      "Basin 07083000: old and new hourly mean flow more than 1% different (-0.014278)\n",
      "Basin 02055100: old and new hourly correlation of flow more than 1% different (-0.016752)\n",
      "Basin 11180500: old and new hourly correlation of flow more than 1% different (-0.013549)\n",
      "Basin 03291780: old and new hourly correlation of flow more than 1% different (-0.016070)\n",
      "Basin 03237280: old and new hourly correlation of flow more than 1% different (-0.022489)\n",
      "Basin 08103900: old and new hourly correlation of flow more than 1% different (-0.081152)\n",
      "Basin 01516500: old and new hourly correlation of flow more than 1% different (-0.012156)\n",
      "Basin 02ZH002: expected timezone NST but found UTC\n",
      "Basin 01586610: old and new hourly correlation of flow more than 1% different (-0.012083)\n",
      "Basin 07362587: old and new hourly correlation of flow more than 1% different (-0.013886)\n",
      "Basin 02381600: old and new hourly correlation of flow more than 1% different (-0.015141)\n",
      "Basin 02464146: old and new hourly correlation of flow more than 1% different (-0.034490)\n",
      "Basin 01658500: old and new hourly correlation of flow more than 1% different (-0.013107)\n",
      "Basin 07195800: old and new hourly correlation of flow more than 1% different (-0.011902)\n",
      "Basin 08158810: old and new hourly correlation of flow more than 1% different (-0.090308)\n",
      "Basin 11299600: old and new hourly correlation of flow more than 1% different (-0.013687)\n",
      "Basin 05HG021: old and new hourly correlation of flow more than 1% different (-0.018683)\n",
      "Basin 09513780: old and new hourly standard deviation of flow more than 1% different (0.016760)\n",
      "Basin 09513780: old and new hourly correlation of flow more than 1% different (-0.022503)\n",
      "Basin 10172700: old and new hourly correlation of flow more than 1% different (-0.012113)\n",
      "Basin 04213075: old and new hourly correlation of flow more than 1% different (-0.010503)\n",
      "Basin 03049800: old and new hourly correlation of flow more than 1% different (-0.023268)\n",
      "Basin 01591400: old and new hourly correlation of flow more than 1% different (-0.011543)\n",
      "Basin 03357350: old and new hourly correlation of flow more than 1% different (-0.015802)\n",
      "Basin 02ZH002: expected timezone NST but found UTC\n",
      "Basin 08171300: old and new hourly correlation of flow more than 1% different (-0.012603)\n",
      "Basin 05NF002: old and new hourly mean flow more than 1% different (0.032847)\n",
      "Basin 07226500: old and new hourly correlation of flow more than 1% different (-0.013269)\n",
      "Basin 08190500: old and new hourly correlation of flow more than 1% different (-0.010819)\n",
      "Basin 08190000: old and new hourly correlation of flow more than 1% different (-0.011349)\n",
      "Basin 07014500: old and new hourly standard deviation of flow more than 1% different (0.011685)\n",
      "Basin 01664000: old and new hourly standard deviation of flow more than 1% different (0.011587)\n",
      "Basin 08070200: old and new hourly standard deviation of flow more than 1% different (-0.018839)\n",
      "Basin 07JF002: old and new hourly standard deviation of flow more than 1% different (-0.425155)\n",
      "Basin 07JF002: old and new hourly correlation of flow more than 1% different (-0.775590)\n",
      "Basin 06453600: old and new hourly mean flow more than 1% different (-0.014855)\n",
      "Basin 06453600: old and new hourly standard deviation of flow more than 1% different (-0.027858)\n",
      "Basin 09484600: old and new hourly correlation of flow more than 1% different (-0.066938)\n",
      "Basin 07071500: old and new hourly standard deviation of flow more than 1% different (-0.028689)\n",
      "Basin 08195000: old and new hourly correlation of flow more than 1% different (-0.016446)\n",
      "Basin 03455500: old and new hourly correlation of flow more than 1% different (-0.010131)\n",
      "Basin 06332515: old and new hourly mean flow more than 1% different (-0.016359)\n",
      "Basin 06332515: old and new hourly standard deviation of flow more than 1% different (-0.079972)\n",
      "Basin 08202700: old and new hourly correlation of flow more than 1% different (-0.017519)\n",
      "Basin 02246000: old and new hourly standard deviation of flow more than 1% different (-0.020076)\n",
      "Basin 14020000: old and new hourly standard deviation of flow more than 1% different (0.018537)\n",
      "Basin 08150800: old and new hourly correlation of flow more than 1% different (-0.012448)\n",
      "Basin 09306242: old and new hourly mean flow more than 1% different (-0.011314)\n",
      "Basin 08200000: old and new hourly correlation of flow more than 1% different (-0.039909)\n",
      "Basin 07340300: old and new hourly correlation of flow more than 1% different (-0.011430)\n",
      "Basin 08165300: old and new hourly correlation of flow more than 1% different (-0.016386)\n",
      "Basin 07335700: old and new hourly correlation of flow more than 1% different (-0.013271)\n",
      "Basin 08104900: old and new hourly correlation of flow more than 1% different (-0.015649)\n",
      "Basin 02ZB001: old and new hourly correlation of flow more than 1% different (-0.028111)\n",
      "Basin 09480000: old and new hourly correlation of flow more than 1% different (-0.037215)\n",
      "Basin 09510200: old and new hourly mean flow more than 1% different (-0.011438)\n",
      "Basin 09510200: old and new hourly standard deviation of flow more than 1% different (-0.015847)\n",
      "Basin 13011500: old and new hourly mean flow more than 1% different (0.010974)\n",
      "Basin 08196000: old and new hourly correlation of flow more than 1% different (-0.026120)\n",
      "Basin 11253310: old and new hourly correlation of flow more than 1% different (-0.013151)\n",
      "Basin 05MH008: old and new hourly mean flow more than 1% different (0.025940)\n",
      "Basin 05MH008: old and new hourly standard deviation of flow more than 1% different (0.010681)\n",
      "Basin 08155200: old and new hourly correlation of flow more than 1% different (-0.016193)\n",
      "Basin 05NF008: old and new hourly mean flow more than 1% different (-0.060628)\n",
      "Basin 05NF008: old and new hourly standard deviation of flow more than 1% different (-0.019169)\n",
      "Basin 02128000: old and new hourly standard deviation of flow more than 1% different (-0.011568)\n",
      "Basin 07196900: old and new hourly correlation of flow more than 1% different (-0.025436)\n",
      "Basin 07CA008: old and new hourly correlation of flow more than 1% different (-0.010999)\n",
      "Basin 05CK005: old and new hourly standard deviation of flow more than 1% different (0.082193)\n",
      "Basin 05CK005: old and new hourly correlation of flow more than 1% different (-0.231795)\n",
      "Basin 09512280: old and new hourly correlation of flow more than 1% different (-0.016241)\n",
      "Basin 09484000: old and new hourly correlation of flow more than 1% different (-0.019599)\n",
      "Basin 09497800: old and new hourly correlation of flow more than 1% different (-0.012548)\n",
      "Basin 05MG003: old and new hourly mean flow more than 1% different (0.044489)\n",
      "Basin 05MG003: old and new hourly standard deviation of flow more than 1% different (0.016248)\n",
      "Basin 08198500: old and new hourly correlation of flow more than 1% different (-0.023298)\n"
     ]
    }
   ],
   "source": [
    "# Loop over the upload data to check which basins we have\n",
    "countries = []\n",
    "basins_updated = []\n",
    "files_moved = []\n",
    "for obs_path_part2 in obs_path_parts2:\n",
    "    for obs_path_part3 in obs_path_parts3:\n",
    "        \n",
    "        # Find the current files\n",
    "        obs_middle = f\"{obs_path_part1}/{obs_path_part2}/{obs_path_part3}\"\n",
    "        obs_files = list((cs_main_folder / obs_middle).glob(\"*.nc\"))\n",
    "        \n",
    "        # Loop over files\n",
    "        for obs_file in obs_files:\n",
    "\n",
    "            # Tracking\n",
    "            file_name = obs_file.name\n",
    "            country = file_name.split(\"_\")[0]\n",
    "            countries.append(country)\n",
    "            basin = file_name.split(\"_\")[1]\n",
    "            basins_updated.append(basin)\n",
    "\n",
    "            # Confirm we have a new file\n",
    "            new_file = []\n",
    "            new_file = [p for p in new_files if file_name in str(p)]\n",
    "            assert len(new_file) == 1, f\"No/extra file found for {basin}. new_file: {new_file}\"\n",
    "            new_file = new_file[0]\n",
    "            \n",
    "            # Open both files\n",
    "            old = xr.open_dataset(obs_file)\n",
    "            new = xr.open_dataset(new_file)\n",
    "\n",
    "            # Find the expected timezone\n",
    "            tz = cs_meta[cs_meta['Station_id'] == basin]['dv_flow_obs_timezone'].iloc[0]\n",
    "            \n",
    "            # Compare the new file, depending on what case we're dealing with\n",
    "            confirm_netcdf_basics(old, country, basin, 'UTC') # key attributes, time & nbnds dims\n",
    "            confirm_netcdf_basics(new, country, basin, tz)\n",
    "            compare_netcdf_sizes_and_vars(old,new) # time and qobs same length, same vars\n",
    "            if 'hourly' in obs_path_part3:\n",
    "                compare_hourly_netcdfs(old,new,basin)\n",
    "            elif 'daily' in obs_path_part3:\n",
    "                compare_daily_netcdfs(old,new,basin)\n",
    "\n",
    "            # Move the new files into the upload folder\n",
    "            des_folder = cs_update_folder / obs_path_part1 / obs_path_part2 / obs_path_part3\n",
    "            des_folder.mkdir(exist_ok=True, parents=True)\n",
    "            shutil.copy(new_file, des_folder/file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdcfdbb-c3a2-454a-85f1-e20419e8f346",
   "metadata": {},
   "source": [
    "## Checks\n",
    "Updated daily values are 100% identical, just with different timesteps. Updated hourly averages are mostly within 1% for a couple of important statistics. There is one gauge (02ZH002) where, somehow, our new data isn't in the expected timezone (NST). We'll need to keep this into account later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09be04c5-241e-4e8e-96d3-742147be8fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 1426.0 out of expected 1426 basins.\n"
     ]
    }
   ],
   "source": [
    "# Check we moved what was expected\n",
    "print(f\"Updated {len(basins_updated)/2} out of expected {len(cs_meta)} basins.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae25e191-2ff4-437a-9c28-145b6b5b32b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df = pd.DataFrame(data={'country': countries,\n",
    "                              'station': basins_updated,\n",
    "                              'scale': scales,\n",
    "                              'resolution': times,\n",
    "                              'file':files_moved})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "475ebda9-c9cc-488f-a110-a2d88f616688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking: observations/headwater/obs-hourly\n",
      "Missing in new:\n",
      "set()\n",
      "Missing in old:\n",
      "set()\n",
      "\n",
      "Checking: observations/headwater/obs-daily\n",
      "Missing in new:\n",
      "set()\n",
      "Missing in old:\n",
      "set()\n",
      "\n",
      "Checking: observations/macro-scale/obs-hourly\n",
      "Missing in new:\n",
      "set()\n",
      "Missing in old:\n",
      "set()\n",
      "\n",
      "Checking: observations/macro-scale/obs-daily\n",
      "Missing in new:\n",
      "set()\n",
      "Missing in old:\n",
      "set()\n",
      "\n",
      "Checking: observations/meso-scale/obs-hourly\n",
      "Missing in new:\n",
      "set()\n",
      "Missing in old:\n",
      "set()\n",
      "\n",
      "Checking: observations/meso-scale/obs-daily\n",
      "Missing in new:\n",
      "set()\n",
      "Missing in old:\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "# It's pretty much a given, but confirm that we have everything we need\n",
    "for obs_path_part2 in obs_path_parts2:\n",
    "    for obs_path_part3 in obs_path_parts3:\n",
    "        \n",
    "        # Find all the files\n",
    "        obs_middle = f\"{obs_path_part1}/{obs_path_part2}/{obs_path_part3}\"\n",
    "        old_files = list((cs_main_folder / obs_middle).glob(\"*.nc\"))\n",
    "        new_files = list((cs_update_folder / obs_middle).glob(\"*.nc\"))\n",
    "\n",
    "        # extract file names from paths\n",
    "        old_names = {p.name for p in old_files} # {}: set\n",
    "        new_names = {p.name for p in new_files}\n",
    "        \n",
    "        # Make sure these match\n",
    "        only_in_old = old_names - new_names\n",
    "        only_in_new = new_names - old_names\n",
    "\n",
    "        # report\n",
    "        print(f\"\\nChecking: {obs_middle}\")\n",
    "        print(f\"Missing in new:\")\n",
    "        print(only_in_old)\n",
    "        print(f\"Missing in old:\")\n",
    "        print(only_in_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a4b8f-a05d-4b68-9f1a-efabcaf65e8a",
   "metadata": {},
   "source": [
    "## Update 02ZH002\n",
    "Here we'll update the new files for basin `02ZH002` so that everything is accounted for before the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d2e8f28-2870-41bb-b76f-0d65980a8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "basin = '02ZH002'\n",
    "site = f\"CAN_{basin}\"\n",
    "row = cs_meta[cs_meta['Station_id'] == basin].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2d094f0-602e-42bf-9266-18e6d3da5a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_tmp = cs_working_folder / f\"tmp_{basin}\"\n",
    "cs_tmp.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2fa5960-bc4f-4595-928e-da6ecc05f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually move the daily and hourly CSV files into working folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf1407-7ae7-4c20-8654-3231b271c312",
   "metadata": {},
   "source": [
    "#### Daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2b7cb254-cc37-43e4-8fd2-59fcc057ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get paths\n",
    "csv_path = cs_tmp / 'CAN_02ZH002_daily_raw_flow_observations.csv'\n",
    "nc_path = cs_tmp / 'CAN_02ZH002_daily_flow_observations.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "eb6d4b1a-81b4-4754-ba84-aeb430bec969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the csv\n",
    "csv = cs.prep_daily_country_csv_for_netcdf(csv_path, row.Country, row.dv_flow_obs_timezone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9eebc733-5e56-474e-9666-15211cb8cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Add the time bounds for daily averages in LST\n",
    "csv['time_bnds_l'] = csv.index\n",
    "csv['time_bnds_r'] = csv.index + pd.Timedelta(hours=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "83bb9b03-72df-486a-8136-08114546bcb3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def daily_flow_csv_to_netcdf(csv, nc_path, country, station):\n",
    "\n",
    "    # 1. Define standard values\n",
    "    # -------------------------\n",
    "    \n",
    "    # Auxiliary\n",
    "    global_att_countries = ['USA', 'CAN', 'MEX']\n",
    "    global_att_i = global_att_countries.index(country)\n",
    "    global_att_now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Global attributes\n",
    "    global_att_ttl = 'CAMELS-spat streamflow data'\n",
    "    global_att_con = 'CF-1.10'\n",
    "    global_att_src = 'Streamflow derived from observed water levels'\n",
    "    global_att_ins = ['United States Geological Survey',\n",
    "                      'Water Survey of Canada']\n",
    "    global_att_ref = [('U.S. Geological Survey, 2016, National Water Information System data available ' +\n",
    "                       'on the World Wide Web (USGS Water Data for the Nation), accessed 2023-03-23, at '+\n",
    "                       'URL [http://waterdata.usgs.gov/nwis/]'),\n",
    "                      ('Original data extracted from the Environment and Climate Change Canada Real-time ' +\n",
    "                       'Hydrometric Data web site (https://wateroffice.ec.gc.ca/mainmenu/real_time_data_index_e.html) ' + \n",
    "                       'on 2023-04-05')]\n",
    "    global_att_his = (f'{global_att_now} | File prepared using CAMELS-spat scripts. See:' + \n",
    "                       'https://github.com/CH-Earth/camels-spat')\n",
    "    global_att_com = (f'{global_att_ins[global_att_i]} calculates daily average flow values' +\n",
    "                      ' in the station\\'s standard time (i.e., not UTC). See: variable time_bnds.')\n",
    "\n",
    "    # Data variables\n",
    "    q_obs_unit = 'm3 s-1'\n",
    "    q_obs_long = 'observed streamflow values'\n",
    "    q_obs_anc = [column for column in csv.columns if '_is_' in column] # Get names of all ancillary variables in .csv \n",
    "    q_obs_anc.append('q_obs_data_quality') # add the 'q_obs_data_quality' variable that's not captured by the above\n",
    "    q_obs_anc = ' '.join([f\"'{anc}'\" for anc in q_obs_anc]) # convert full list into single string\n",
    "\n",
    "    # Time settings\n",
    "    time_unit = 'minutes since 1950-01-01 00:00:00'\n",
    "    time_cal = 'proleptic_gregorian'\n",
    "    \n",
    "    # Time settings - ensure the data only specifies a single time zone that was used for calculating averages\n",
    "    #                 Communications with USGS and WSC state that this should be the case\n",
    "    assert len(csv['q_obs_tz_cd'].unique()) == 1, \"Multiple timezones specified in csv; there should be only one\"\n",
    "    time_original_tz = csv['q_obs_tz_cd'].unique()[0]\n",
    "\n",
    "    # 2. Create a basic data set to build from\n",
    "    ds = csv.to_xarray()\n",
    "\n",
    "    # 3. Global attributes\n",
    "    ds.attrs['title'] = global_att_ttl\n",
    "    ds.attrs['conventions'] = global_att_con\n",
    "    ds.attrs['source'] = global_att_src\n",
    "    ds.attrs['country'] = country\n",
    "    ds.attrs['station'] = station\n",
    "    ds.attrs['institution'] = global_att_ins[global_att_i]\n",
    "    ds.attrs['references'] = global_att_ref[global_att_i]\n",
    "    ds.attrs['history'] = global_att_his\n",
    "    ds.attrs['comment'] = global_att_com\n",
    "\n",
    "    # 4a. Time attributes (coordinate already exists)\n",
    "    # NOTE: attributes 'units' and 'calendar' are automatically specified when writing to netcdf\n",
    "    #       This can be checked by saving to netcdf, and then loading as follows: xr.open_dataset(nc_path, decode_times=False)\n",
    "    ds.time.attrs['standard_name'] = 'time'\n",
    "    ds.time.attrs['bounds'] = 'time_bnds'\n",
    "    ds.time.encoding['units'] = time_unit\n",
    "    ds.time.encoding['calendar'] = time_cal\n",
    "        \n",
    "    # 4b. Time bounds variable\n",
    "    ds = ds.assign_coords(nbnds=[1,2])\n",
    "    ds = ds.assign(time_bnds=(['nbnds','time'],\n",
    "                              [csv['time_bnds_l'], csv['time_bnds_r']]))\n",
    "    ds.nbnds.attrs['standard_name'] = 'bounds for timestep intervals'\n",
    "    ds.time_bnds.attrs['long_name'] = 'start and end points of each time interval'\n",
    "    #ds.time_bnds.attrs['time_zone'] = 'UTC'\n",
    "    #ds.time_bnds.attrs['station_standard_time'] = time_original_tz\n",
    "    ds.time_bnds.attrs['time_zone'] = time_original_tz\n",
    "\n",
    "    # 5. Observed streamflow\n",
    "    ds.q_obs.attrs['units'] = q_obs_unit\n",
    "    ds.q_obs.attrs['long_name'] = q_obs_long\n",
    "    ds.q_obs.attrs['cell_methods'] = 'time:mean' # indicating that values are average values over the timestep\n",
    "    ds.q_obs.attrs['ancillary_variables'] = q_obs_anc\n",
    "\n",
    "    # 6. Data quality flags\n",
    "    flags = [str(s) for s in csv['q_obs_data_quality'].unique()]\n",
    "    flags.sort()\n",
    "    while ' ' in flags: flags.remove(' ')  # Sometimes we have empty spaces with no specific meaning in the data quality column: take those out\n",
    "    meanings = cs.return_data_quality_flag_meaning(flags,country)\n",
    "    ds.q_obs_data_quality.attrs['standard_name'] = 'quality_flag'\n",
    "    ds.q_obs_data_quality.attrs['long_name'] = 'data quality flag'\n",
    "    ds.q_obs_data_quality.attrs['flag_values'] = ' '.join([f\"'{flag}'\" for flag in flags])\n",
    "    ds.q_obs_data_quality.attrs['flag_meanings'] = ' '.join([f\"'{meaning}'\" for meaning in meanings])\n",
    "\n",
    "    # 7. Other status variables\n",
    "    for variable in ds.variables:\n",
    "        if '_is_' in variable:\n",
    "            ds[variable].attrs['standard_name'] = 'quality_flag'\n",
    "            ds[variable].attrs['long_name'] = 'flag indicating if main variable is affected by process in variable name'\n",
    "            ds[variable].attrs['flag_values'] = \"'0' '1'\"\n",
    "            ds[variable].attrs['flag_meanings'] = \"'no' 'yes'\"\n",
    "\n",
    "    # 8. Remove the timezone variables we added to get the time_bnds\n",
    "    ds = ds.drop_vars(['q_obs_tz_cd', 'time_bnds_l', 'time_bnds_r'])\n",
    "\n",
    "    # 9. Save to file\n",
    "    ds.to_netcdf(nc_path)\n",
    "    ds.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a0a02671-9ede-4aa4-84b1-4e466604a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Convert to netcdf and save\n",
    "daily_flow_csv_to_netcdf(csv, nc_path, row.Country, basin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2964b1f2-706c-49b1-803c-2f8fd698ba7f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compare_datasets(ds1, ds2, rtol=1e-5, atol=1e-8):\n",
    "    differences = []\n",
    "\n",
    "    # Dimensions\n",
    "    if ds1.dims != ds2.dims:\n",
    "        differences.append(\"❌ Dimension mismatch:\")\n",
    "        for dim in set(ds1.dims.keys()) | set(ds2.dims.keys()):\n",
    "            d1 = ds1.dims.get(dim)\n",
    "            d2 = ds2.dims.get(dim)\n",
    "            if d1 != d2:\n",
    "                differences.append(f\"  - Dimension '{dim}': {d1} vs {d2}\")\n",
    "\n",
    "    # Coordinates\n",
    "    if set(ds1.coords) != set(ds2.coords):\n",
    "        differences.append(\"❌ Coordinate name mismatch:\")\n",
    "        only1 = set(ds1.coords) - set(ds2.coords)\n",
    "        only2 = set(ds2.coords) - set(ds1.coords)\n",
    "        if only1:\n",
    "            differences.append(f\"  - Only in ds1: {only1}\")\n",
    "        if only2:\n",
    "            differences.append(f\"  - Only in ds2: {only2}\")\n",
    "\n",
    "    else:\n",
    "        for coord in ds1.coords:\n",
    "            a = ds1[coord].values\n",
    "            b = ds2[coord].values\n",
    "\n",
    "            # Check and warn about dtype mismatch\n",
    "            if a.dtype != b.dtype:\n",
    "                differences.append(\n",
    "                    f\"⚠️ Coordinate '{coord}' has differing dtypes: \"\n",
    "                    f\"ds1={a.dtype}, ds2={b.dtype}\"\n",
    "                )\n",
    "\n",
    "            # Choose appropriate comparison method\n",
    "            if a.dtype.kind in \"fiu\" and b.dtype.kind in \"fiu\":  # numeric\n",
    "                if not np.allclose(a, b, rtol=rtol, atol=atol, equal_nan=True):\n",
    "                    differences.append(f\"❌ Coordinate '{coord}' values differ (numeric)\")\n",
    "            else:\n",
    "                if not np.array_equal(a, b):\n",
    "                    differences.append(f\"❌ Coordinate '{coord}' values differ (non-numeric)\")\n",
    "\n",
    "\n",
    "    # Data variables\n",
    "    if set(ds1.data_vars) != set(ds2.data_vars):\n",
    "        differences.append(\"❌ Variable name mismatch:\")\n",
    "        only1 = set(ds1.data_vars) - set(ds2.data_vars)\n",
    "        only2 = set(ds2.data_vars) - set(ds1.data_vars)\n",
    "        if only1:\n",
    "            differences.append(f\"  - Only in ds1: {only1}\")\n",
    "        if only2:\n",
    "            differences.append(f\"  - Only in ds2: {only2}\")\n",
    "\n",
    "    else:\n",
    "        for var in ds1.data_vars:\n",
    "            a = ds1[var].values\n",
    "            b = ds2[var].values\n",
    "\n",
    "            # Check and warn about dtype mismatch\n",
    "            if a.dtype != b.dtype:\n",
    "                differences.append(\n",
    "                    f\"⚠️ Variable '{var}' has differing dtypes: ds1={a.dtype}, ds2={b.dtype}\"\n",
    "                )\n",
    "\n",
    "            # Choose comparison based on dtype\n",
    "            if a.dtype.kind in \"fiu\" and b.dtype.kind in \"fiu\":  # numeric\n",
    "                if not np.allclose(a, b, rtol=rtol, atol=atol, equal_nan=True):\n",
    "                    mismatch = ~np.isclose(a, b, rtol=rtol, atol=atol, equal_nan=True)\n",
    "                    n_total = np.size(a)\n",
    "                    n_diff = np.count_nonzero(mismatch)\n",
    "                    differences.append(f\"❌ Variable '{var}': {n_diff}/{n_total} entries differ\")\n",
    "            else:\n",
    "                try:\n",
    "                    equal = np.array_equal(a, b, equal_nan=True)\n",
    "                except TypeError:\n",
    "                    # Fallback: if dtype is float, we can safely substitute NaNs with a sentinel\n",
    "                    if np.issubdtype(a.dtype, np.floating):\n",
    "                        equal = np.array_equal(\n",
    "                            np.where(np.isnan(a), 'NaN', a.astype(\"object\")),\n",
    "                            np.where(np.isnan(b), 'NaN', b.astype(\"object\"))\n",
    "                        )\n",
    "                    else:\n",
    "                        # Non-numeric, non-NaN-tolerant types (e.g. strings, datetimes)\n",
    "                        equal = np.array_equal(a, b)\n",
    "\n",
    "            # Check variable attributes\n",
    "            attrs1 = ds1[var].attrs\n",
    "            attrs2 = ds2[var].attrs\n",
    "\n",
    "            if attrs1 != attrs2:\n",
    "                differences.append(f\"⚠️ Variable '{var}' attributes differ:\")\n",
    "                keys = set(attrs1) | set(attrs2)\n",
    "                for key in keys:\n",
    "                    v1 = attrs1.get(key)\n",
    "                    v2 = attrs2.get(key)\n",
    "                    if v1 != v2:\n",
    "                        differences.append(f\"    - Attr '{key}': ds1={v1!r}, ds2={v2!r}\")\n",
    "\n",
    "\n",
    "    # Global attributes\n",
    "    if ds1.attrs != ds2.attrs:\n",
    "        differences.append(\"❌ Global attributes differ\")\n",
    "        keys = set(ds1.attrs.keys()) | set(ds2.attrs.keys())\n",
    "        for k in keys:\n",
    "            v1 = ds1.attrs.get(k)\n",
    "            v2 = ds2.attrs.get(k)\n",
    "            if v1 != v2:\n",
    "                differences.append(f\"  - Attr '{k}': {v1} vs {v2}\")\n",
    "\n",
    "    # Summary\n",
    "    if not differences:\n",
    "        print(\"✅ Datasets are identical across dimensions, coordinates, variables, and attributes.\")\n",
    "    else:\n",
    "        print(\"🔍 Differences found between datasets:\")\n",
    "        for diff in differences:\n",
    "            print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7379950f-057f-44d8-9fcd-79b2db342b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Differences found between datasets:\n",
      "⚠️ Coordinate 'nbnds' has differing dtypes: ds1=int32, ds2=int64\n",
      "⚠️ Variable 'q_obs_is_ice_affected' has differing dtypes: ds1=int32, ds2=int64\n",
      "⚠️ Variable 'q_obs_is_partial_day' has differing dtypes: ds1=int32, ds2=int64\n",
      "⚠️ Variable 'q_obs_is_dry_day' has differing dtypes: ds1=int32, ds2=int64\n",
      "⚠️ Variable 'q_obs_is_estimated_value' has differing dtypes: ds1=int32, ds2=int64\n",
      "⚠️ Variable 'time_bnds' attributes differ:\n",
      "    - Attr 'time_zone': ds1='UTC', ds2='NST'\n",
      "    - Attr 'station_standard_time': ds1='NST', ds2=None\n",
      "❌ Global attributes differ\n",
      "  - Attr 'history': 2023-08-02 11:54:37 | File prepared using CAMELS-spat scripts. See:https://github.com/CH-Earth/camels-spat vs 2025-05-31 10:29:15 | File prepared using CAMELS-spat scripts. See:https://github.com/CH-Earth/camels-spat\n"
     ]
    }
   ],
   "source": [
    "# Compare to the existing one\n",
    "old_file = cs_transfer_folder / 'CAN_02ZH002_daily_flow_observations.nc'\n",
    "old_ds = xr.open_dataset(old_file)\n",
    "new_ds = xr.open_dataset(nc_path)\n",
    "compare_datasets(old_ds,new_ds)\n",
    "old_ds.close()\n",
    "new_ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0ef5a87c-26d3-477c-9027-1f7e09373b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray &#x27;time&#x27; (time: 18719)&gt;\n",
       "array([&#x27;1961-01-01T00:00:00.000000000&#x27;, &#x27;1961-01-02T00:00:00.000000000&#x27;,\n",
       "       &#x27;1961-01-03T00:00:00.000000000&#x27;, ..., &#x27;2019-12-29T00:00:00.000000000&#x27;,\n",
       "       &#x27;2019-12-30T00:00:00.000000000&#x27;, &#x27;2019-12-31T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 1961-01-01 1961-01-02 ... 2019-12-31\n",
       "Attributes:\n",
       "    standard_name:  time\n",
       "    bounds:         time_bnds</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'>'time'</div><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 18719</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-9bff8612-1b1b-40aa-aef4-764ef248e2ef' class='xr-array-in' type='checkbox' checked><label for='section-9bff8612-1b1b-40aa-aef4-764ef248e2ef' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>1961-01-01 1961-01-02 1961-01-03 ... 2019-12-29 2019-12-30 2019-12-31</span></div><div class='xr-array-data'><pre>array([&#x27;1961-01-01T00:00:00.000000000&#x27;, &#x27;1961-01-02T00:00:00.000000000&#x27;,\n",
       "       &#x27;1961-01-03T00:00:00.000000000&#x27;, ..., &#x27;2019-12-29T00:00:00.000000000&#x27;,\n",
       "       &#x27;2019-12-30T00:00:00.000000000&#x27;, &#x27;2019-12-31T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></div></li><li class='xr-section-item'><input id='section-561791f3-fb04-49fe-a1ce-d6689e049798' class='xr-section-summary-in' type='checkbox'  checked><label for='section-561791f3-fb04-49fe-a1ce-d6689e049798' class='xr-section-summary' >Coordinates: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>1961-01-01 ... 2019-12-31</div><input id='attrs-3762cf86-cca5-4e4b-b432-3bdda4b09393' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-3762cf86-cca5-4e4b-b432-3bdda4b09393' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-daffe690-909b-4800-9419-fd79db25eae2' class='xr-var-data-in' type='checkbox'><label for='data-daffe690-909b-4800-9419-fd79db25eae2' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>time</dd><dt><span>bounds :</span></dt><dd>time_bnds</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;1961-01-01T00:00:00.000000000&#x27;, &#x27;1961-01-02T00:00:00.000000000&#x27;,\n",
       "       &#x27;1961-01-03T00:00:00.000000000&#x27;, ..., &#x27;2019-12-29T00:00:00.000000000&#x27;,\n",
       "       &#x27;2019-12-30T00:00:00.000000000&#x27;, &#x27;2019-12-31T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-0d973daf-cc06-4e83-9d37-ba825e0f72b9' class='xr-section-summary-in' type='checkbox'  checked><label for='section-0d973daf-cc06-4e83-9d37-ba825e0f72b9' class='xr-section-summary' >Attributes: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>time</dd><dt><span>bounds :</span></dt><dd>time_bnds</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.DataArray 'time' (time: 18719)>\n",
       "array(['1961-01-01T00:00:00.000000000', '1961-01-02T00:00:00.000000000',\n",
       "       '1961-01-03T00:00:00.000000000', ..., '2019-12-29T00:00:00.000000000',\n",
       "       '2019-12-30T00:00:00.000000000', '2019-12-31T00:00:00.000000000'],\n",
       "      dtype='datetime64[ns]')\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 1961-01-01 1961-01-02 ... 2019-12-31\n",
       "Attributes:\n",
       "    standard_name:  time\n",
       "    bounds:         time_bnds"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll accept the dtype differences because that shouldn't really affect anything.\n",
    "# The time_bnds differences is what we wanted to change.\n",
    "# The history attribute is a logical consequence of our changes.\n",
    "#\n",
    "# Not sure why the actual timesteps are the same, but they appear correct in the new file.\n",
    "new_ds['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e64c391b-b18d-4d92-a136-59227e256fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving:\n",
      "/scratch/gwf/gwf_cmt/wknoben/camels-spat-wip/tmp_02ZH002/CAN_02ZH002_daily_flow_observations.nc \n",
      "to:\n",
      "/scratch/gwf/gwf_cmt/wknoben/camels-spat-upload-updates/observations/headwater/obs-daily/CAN_02ZH002_daily_flow_observations.nc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/scratch/gwf/gwf_cmt/wknoben/camels-spat-upload-updates/observations/headwater/obs-daily/CAN_02ZH002_daily_flow_observations.nc')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the new file into the correct update folder\n",
    "scale = check_df[(check_df['station'] == basin) & (check_df['resolution'] == 'obs-daily')]['scale']\n",
    "src = nc_path\n",
    "des = cs_update_folder / obs_path_part1 / scale.iloc[0] / 'obs-daily' / src.name\n",
    "print(f\"Moving:\\n{src} \\nto:\\n{des}\")\n",
    "shutil.copy(src,des)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d34a751-0438-451c-b899-55821762fea1",
   "metadata": {},
   "source": [
    "#### Hourly\n",
    "We don't need to re-do these, because we'll do them again for the longer time period anyway in the next section. We just need to make sure we do process a new version of this basin's file in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7618b0-223a-48b4-8e6d-a229eb60aacf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camels-spat-env-jupyter",
   "language": "python",
   "name": "camels-spat-env-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
